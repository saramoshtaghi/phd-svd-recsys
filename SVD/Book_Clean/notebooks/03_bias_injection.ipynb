{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Injection Experiments\n",
    "\n",
    "This notebook creates synthetic biased users with strong preferences for Adventure or Mystery genres\n",
    "and trains SVD models to study bias propagation effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Original dataset: 5,976,479 ratings\n",
      "🗺️  Adventure books: 3525\n",
      "🔍 Mystery books: 4084\n",
      "🎯 Baseline RMSE: 0.8310\n"
     ]
    }
   ],
   "source": [
    "# Load processed data\n",
    "ratings = pd.read_csv('../data/processed/clean_ratings.csv')\n",
    "books = pd.read_csv('../data/processed/clean_books.csv')\n",
    "adventure_book_ids = pd.read_csv('../data/processed/adventure_book_ids.csv')['book_id'].tolist()\n",
    "mystery_book_ids = pd.read_csv('../data/processed/mystery_book_ids.csv')['book_id'].tolist()\n",
    "\n",
    "# Load baseline results for comparison\n",
    "baseline_results = pd.read_csv('../results/baseline/baseline_results.csv').iloc[0]\n",
    "with open('../results/baseline/baseline_testset.pkl', 'rb') as f:\n",
    "    baseline_testset = pickle.load(f)\n",
    "\n",
    "print(f\"📚 Original dataset: {len(ratings):,} ratings\")\n",
    "print(f\"🗺️  Adventure books: {len(adventure_book_ids)}\")\n",
    "print(f\"🔍 Mystery books: {len(mystery_book_ids)}\")\n",
    "print(f\"🎯 Baseline RMSE: {baseline_results['overall_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Synthetic Biased Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Will test bias levels: [50, 100, 200, 500, 1000, 2000]\n",
      "📋 Will test genres: ['adventure', 'mystery']\n"
     ]
    }
   ],
   "source": [
    "def create_biased_users(genre_book_ids, num_users, genre_name, min_ratings=20, max_ratings=50):\n",
    "    \"\"\"\n",
    "    Create synthetic users with strong bias toward a specific genre\n",
    "    \n",
    "    Args:\n",
    "        genre_book_ids: List of book IDs for the target genre\n",
    "        num_users: Number of biased users to create\n",
    "        genre_name: Name of the genre (for logging)\n",
    "        min_ratings: Minimum number of ratings per user\n",
    "        max_ratings: Maximum number of ratings per user\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with synthetic ratings\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Creating {num_users} {genre_name}-biased users...\")\n",
    "    \n",
    "    # Get next available user ID\n",
    "    max_user_id = ratings['user_id'].max()\n",
    "    \n",
    "    synthetic_ratings = []\n",
    "    \n",
    "    for i in range(num_users):\n",
    "        user_id = max_user_id + i + 1\n",
    "        num_ratings = np.random.randint(min_ratings, max_ratings + 1)\n",
    "        \n",
    "        # Sample books for this user\n",
    "        user_books = np.random.choice(genre_book_ids, \n",
    "                                     min(num_ratings, len(genre_book_ids)), \n",
    "                                     replace=False)\n",
    "        \n",
    "        for book_id in user_books:\n",
    "            # Biased users give high ratings (4-5) with high probability\n",
    "            rating = np.random.choice([3, 4, 5], p=[0.1, 0.4, 0.5])\n",
    "            \n",
    "            synthetic_ratings.append({\n",
    "                'user_id': user_id,\n",
    "                'book_id': book_id,\n",
    "                'rating': rating\n",
    "            })\n",
    "    \n",
    "    synthetic_df = pd.DataFrame(synthetic_ratings)\n",
    "    print(f\"✅ Created {len(synthetic_df)} synthetic ratings for {num_users} users\")\n",
    "    return synthetic_df\n",
    "\n",
    "# Define experiment parameters\n",
    "bias_levels = [50, 100, 200, 500, 1000, 2000]  # Number of biased users to inject\n",
    "genres = {\n",
    "    'adventure': adventure_book_ids,\n",
    "    'mystery': mystery_book_ids\n",
    "}\n",
    "\n",
    "print(f\"📋 Will test bias levels: {bias_levels}\")\n",
    "print(f\"📋 Will test genres: {list(genres.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Bias Injection Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model with bias injection\n",
    "def evaluate_biased_model(original_ratings, synthetic_ratings, testset, genre_name, num_biased_users):\n",
    "    \"\"\"\n",
    "    Train model with bias injection and evaluate\n",
    "    \"\"\"\n",
    "    # Combine original training data with synthetic biased users\n",
    "    combined_ratings = pd.concat([original_ratings, synthetic_ratings], ignore_index=True)\n",
    "    \n",
    "    # Create Surprise dataset\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data = Dataset.load_from_df(combined_ratings[['user_id', 'book_id', 'rating']], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    \n",
    "    # Train SVD model\n",
    "    svd_model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\n",
    "    svd_model.fit(trainset)\n",
    "    \n",
    "    # Evaluate on the same test set as baseline\n",
    "    predictions = svd_model.test(testset)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    rmse = accuracy.rmse(predictions, verbose=False)\n",
    "    mae = accuracy.mae(predictions, verbose=False)\n",
    "    \n",
    "    # Calculate genre-specific metrics\n",
    "    adventure_predictions = [p for p in predictions if p.iid in adventure_book_ids]\n",
    "    mystery_predictions = [p for p in predictions if p.iid in mystery_book_ids]\n",
    "    other_predictions = [p for p in predictions if p.iid not in adventure_book_ids + mystery_book_ids]\n",
    "    \n",
    "    def calc_metrics(pred_list):\n",
    "        if not pred_list:\n",
    "            return None, None\n",
    "        errors = [abs(p.est - p.r_ui) for p in pred_list]\n",
    "        squared_errors = [(p.est - p.r_ui) ** 2 for p in pred_list]\n",
    "        return np.mean(errors), np.sqrt(np.mean(squared_errors))\n",
    "    \n",
    "    adv_mae, adv_rmse = calc_metrics(adventure_predictions)\n",
    "    mys_mae, mys_rmse = calc_metrics(mystery_predictions)\n",
    "    oth_mae, oth_rmse = calc_metrics(other_predictions)\n",
    "    \n",
    "    # Generate sample recommendations to check bias propagation\n",
    "    sample_recs = generate_sample_recommendations(svd_model, trainset, n_users=50, n_recs=10)\n",
    "    genre_dist = analyze_recommendation_bias(sample_recs, adventure_book_ids, mystery_book_ids)\n",
    "    \n",
    "    return {\n",
    "        'genre': genre_name,\n",
    "        'num_biased_users': num_biased_users,\n",
    "        'total_ratings': len(combined_ratings),\n",
    "        'synthetic_ratings': len(synthetic_ratings),\n",
    "        'overall_rmse': rmse,\n",
    "        'overall_mae': mae,\n",
    "        'adventure_mae': adv_mae,\n",
    "        'adventure_rmse': adv_rmse,\n",
    "        'mystery_mae': mys_mae,\n",
    "        'mystery_rmse': mys_rmse,\n",
    "        'other_mae': oth_mae,\n",
    "        'other_rmse': oth_rmse,\n",
    "        'rec_adventure_pct': genre_dist['adventure_pct'],\n",
    "        'rec_mystery_pct': genre_dist['mystery_pct'],\n",
    "        'rec_other_pct': genre_dist['other_pct']\n",
    "    }\n",
    "\n",
    "def generate_sample_recommendations(model, trainset, n_users=50, n_recs=10):\n",
    "    \"\"\"Generate sample recommendations for bias analysis\"\"\"\n",
    "    all_book_ids = set([iid for (uid, iid, rating) in trainset.all_ratings()])\n",
    "    all_user_ids = set([uid for (uid, iid, rating) in trainset.all_ratings()])\n",
    "    \n",
    "    # Sample original users only (exclude synthetic users)\n",
    "    original_users = [uid for uid in all_user_ids if uid <= ratings['user_id'].max()]\n",
    "    sample_user_ids = np.random.choice(original_users, min(n_users, len(original_users)), replace=False)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    for uid in sample_user_ids:\n",
    "        user_rated_books = set([iid for (u, iid, rating) in trainset.all_ratings() if u == uid])\n",
    "        candidates = list(all_book_ids - user_rated_books)\n",
    "        \n",
    "        if len(candidates) < n_recs:\n",
    "            continue\n",
    "            \n",
    "        user_predictions = [(iid, model.predict(uid, iid).est) for iid in candidates]\n",
    "        user_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for rank, (book_id, rating) in enumerate(user_predictions[:n_recs], 1):\n",
    "            recommendations.append((uid, book_id, rating, rank))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def analyze_recommendation_bias(recommendations, adventure_ids, mystery_ids):\n",
    "    \"\"\"Analyze genre distribution in recommendations\"\"\"\n",
    "    adventure_count = sum(1 for _, book_id, _, _ in recommendations if book_id in adventure_ids)\n",
    "    mystery_count = sum(1 for _, book_id, _, _ in recommendations if book_id in mystery_ids)\n",
    "    other_count = len(recommendations) - adventure_count - mystery_count\n",
    "    total = len(recommendations)\n",
    "    \n",
    "    return {\n",
    "        'adventure_pct': (adventure_count / total * 100) if total > 0 else 0,\n",
    "        'mystery_pct': (mystery_count / total * 100) if total > 0 else 0,\n",
    "        'other_pct': (other_count / total * 100) if total > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Original training data: 4,781,183 ratings\n",
      "🧪 Test data: 1,195,296 ratings\n",
      "\n",
      "🔬 Experiment 1/12: adventure bias with 50 users\n",
      "🔄 Creating 50 adventure-biased users...\n",
      "✅ Created 1828 synthetic ratings for 50 users\n",
      "   📊 RMSE: 1.1289 (Δ: +0.2978)\n",
      "   🗺️  Adventure recs: 42.4% (Δ: -6.4%)\n",
      "   🔍 Mystery recs: 33.8% (Δ: +25.6%)\n",
      "\n",
      "🔬 Experiment 2/12: adventure bias with 100 users\n",
      "🔄 Creating 100 adventure-biased users...\n",
      "✅ Created 3520 synthetic ratings for 100 users\n",
      "   📊 RMSE: 1.1290 (Δ: +0.2980)\n",
      "   🗺️  Adventure recs: 38.6% (Δ: -10.2%)\n",
      "   🔍 Mystery recs: 34.0% (Δ: +25.8%)\n",
      "\n",
      "🔬 Experiment 3/12: adventure bias with 200 users\n",
      "🔄 Creating 200 adventure-biased users...\n",
      "✅ Created 7042 synthetic ratings for 200 users\n",
      "   📊 RMSE: 1.1290 (Δ: +0.2980)\n",
      "   🗺️  Adventure recs: 40.0% (Δ: -8.8%)\n",
      "   🔍 Mystery recs: 36.0% (Δ: +27.8%)\n",
      "\n",
      "🔬 Experiment 4/12: adventure bias with 500 users\n",
      "🔄 Creating 500 adventure-biased users...\n",
      "✅ Created 17538 synthetic ratings for 500 users\n",
      "   📊 RMSE: 1.1284 (Δ: +0.2974)\n",
      "   🗺️  Adventure recs: 35.2% (Δ: -13.6%)\n",
      "   🔍 Mystery recs: 36.0% (Δ: +27.8%)\n",
      "\n",
      "🔬 Experiment 5/12: adventure bias with 1000 users\n",
      "🔄 Creating 1000 adventure-biased users...\n",
      "✅ Created 35226 synthetic ratings for 1000 users\n",
      "   📊 RMSE: 1.1275 (Δ: +0.2965)\n",
      "   🗺️  Adventure recs: 35.2% (Δ: -13.6%)\n",
      "   🔍 Mystery recs: 30.0% (Δ: +21.8%)\n",
      "\n",
      "🔬 Experiment 6/12: adventure bias with 2000 users\n",
      "🔄 Creating 2000 adventure-biased users...\n",
      "✅ Created 70512 synthetic ratings for 2000 users\n",
      "   📊 RMSE: 1.1262 (Δ: +0.2951)\n",
      "   🗺️  Adventure recs: 31.0% (Δ: -17.8%)\n",
      "   🔍 Mystery recs: 33.0% (Δ: +24.8%)\n",
      "\n",
      "🔬 Experiment 7/12: mystery bias with 50 users\n",
      "🔄 Creating 50 mystery-biased users...\n",
      "✅ Created 1799 synthetic ratings for 50 users\n",
      "   📊 RMSE: 1.1289 (Δ: +0.2979)\n",
      "   🗺️  Adventure recs: 45.2% (Δ: -3.6%)\n",
      "   🔍 Mystery recs: 36.2% (Δ: +28.0%)\n",
      "\n",
      "🔬 Experiment 8/12: mystery bias with 100 users\n",
      "🔄 Creating 100 mystery-biased users...\n",
      "✅ Created 3379 synthetic ratings for 100 users\n"
     ]
    }
   ],
   "source": [
    "# Prepare original training data (exclude test set)\n",
    "# We need to recreate the same train-test split as baseline\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings[['user_id', 'book_id', 'rating']], reader)\n",
    "trainset, _ = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert trainset back to DataFrame for easier manipulation\n",
    "original_train_data = []\n",
    "for uid, iid, rating in trainset.all_ratings():\n",
    "    original_train_data.append({'user_id': uid, 'book_id': iid, 'rating': rating})\n",
    "original_train_df = pd.DataFrame(original_train_data)\n",
    "\n",
    "print(f\"📚 Original training data: {len(original_train_df):,} ratings\")\n",
    "print(f\"🧪 Test data: {len(baseline_testset):,} ratings\")\n",
    "\n",
    "# Run all experiments\n",
    "all_results = []\n",
    "\n",
    "# Add baseline results for comparison\n",
    "baseline_row = {\n",
    "    'genre': 'baseline',\n",
    "    'num_biased_users': 0,\n",
    "    'total_ratings': len(original_train_df),\n",
    "    'synthetic_ratings': 0,\n",
    "    'overall_rmse': baseline_results['overall_rmse'],\n",
    "    'overall_mae': baseline_results['overall_mae'],\n",
    "    'adventure_mae': baseline_results['adventure_mae'],\n",
    "    'adventure_rmse': baseline_results['adventure_rmse'],\n",
    "    'mystery_mae': baseline_results['mystery_mae'],\n",
    "    'mystery_rmse': baseline_results['mystery_rmse'],\n",
    "    'other_mae': baseline_results['other_mae'],\n",
    "    'other_rmse': baseline_results['other_rmse'],\n",
    "    'rec_adventure_pct': baseline_results['adventure_rec_percentage'],\n",
    "    'rec_mystery_pct': baseline_results['mystery_rec_percentage'],\n",
    "    'rec_other_pct': baseline_results['other_rec_percentage']\n",
    "}\n",
    "all_results.append(baseline_row)\n",
    "\n",
    "# Run bias injection experiments\n",
    "total_experiments = len(genres) * len(bias_levels)\n",
    "experiment_count = 0\n",
    "\n",
    "for genre_name, genre_book_ids in genres.items():\n",
    "    for num_biased_users in bias_levels:\n",
    "        experiment_count += 1\n",
    "        print(f\"\\n🔬 Experiment {experiment_count}/{total_experiments}: {genre_name} bias with {num_biased_users} users\")\n",
    "        \n",
    "        # Create synthetic biased users\n",
    "        synthetic_ratings = create_biased_users(genre_book_ids, num_biased_users, genre_name)\n",
    "        \n",
    "        # Train and evaluate model\n",
    "        results = evaluate_biased_model(original_train_df, synthetic_ratings, \n",
    "                                      baseline_testset, genre_name, num_biased_users)\n",
    "        \n",
    "        all_results.append(results)\n",
    "        \n",
    "        print(f\"   📊 RMSE: {results['overall_rmse']:.4f} (Δ: {results['overall_rmse'] - baseline_results['overall_rmse']:+.4f})\")\n",
    "        print(f\"   🗺️  Adventure recs: {results['rec_adventure_pct']:.1f}% (Δ: {results['rec_adventure_pct'] - baseline_results['adventure_rec_percentage']:+.1f}%)\")\n",
    "        print(f\"   🔍 Mystery recs: {results['rec_mystery_pct']:.1f}% (Δ: {results['rec_mystery_pct'] - baseline_results['mystery_rec_percentage']:+.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv('../results/biased/bias_injection_results.csv', index=False)\n",
    "\n",
    "print(\"✅ Results saved!\")\n",
    "print(f\"📁 Results saved to: ../results/biased/bias_injection_results.csv\")\n",
    "print(f\"📊 Total experiments: {len(results_df)}\")\n",
    "print(\"\\n🔄 Next step: Run 04_bias_analysis.ipynb to analyze results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
