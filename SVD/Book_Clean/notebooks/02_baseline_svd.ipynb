{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline SVD Model - Original Data Only\n",
    "\n",
    "This notebook establishes the baseline performance using SVD on the original dataset without any bias injection.\n",
    "This serves as our control group for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "from surprise import accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dataset loaded:\n",
      "   📚 Books: 10,000.0\n",
      "   👥 Users: 53,424.0\n",
      "   ⭐ Ratings: 5,976,479.0\n",
      "   🗺️  Adventure books: 3525.0\n",
      "   🔍 Mystery books: 4084.0\n"
     ]
    }
   ],
   "source": [
    "# Load processed data\n",
    "ratings = pd.read_csv('../data/processed/clean_ratings.csv')\n",
    "books = pd.read_csv('../data/processed/clean_books.csv')\n",
    "adventure_book_ids = pd.read_csv('../data/processed/adventure_book_ids.csv')['book_id'].tolist()\n",
    "mystery_book_ids = pd.read_csv('../data/processed/mystery_book_ids.csv')['book_id'].tolist()\n",
    "summary = pd.read_csv('../data/processed/dataset_summary.csv').iloc[0]\n",
    "\n",
    "print(f\"📊 Dataset loaded:\")\n",
    "print(f\"   📚 Books: {summary['total_books']:,}\")\n",
    "print(f\"   👥 Users: {summary['total_users']:,}\")\n",
    "print(f\"   ⭐ Ratings: {summary['total_ratings']:,}\")\n",
    "print(f\"   🗺️  Adventure books: {summary['adventure_books']}\")\n",
    "print(f\"   🔍 Mystery books: {summary['mystery_books']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Surprise Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Training set size: 4,781,183\n",
      "🧪 Test set size: 1,195,296\n"
     ]
    }
   ],
   "source": [
    "# Create Surprise dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings[['user_id', 'book_id', 'rating']], reader)\n",
    "\n",
    "# Split data: 80% train, 20% test\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"📚 Training set size: {trainset.n_ratings:,}\")\n",
    "print(f\"🧪 Test set size: {len(testset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Baseline SVD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training baseline SVD model...\n",
      "✅ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train SVD model\n",
    "print(\"🚀 Training baseline SVD model...\")\n",
    "svd_model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\n",
    "svd_model.fit(trainset)\n",
    "print(\"✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BASELINE MODEL PERFORMANCE:\n",
      "   🎯 RMSE: 0.8310\n",
      "   🎯 MAE: 0.6415\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "predictions = svd_model.test(testset)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "mae = accuracy.mae(predictions, verbose=False)\n",
    "\n",
    "print(f\"📊 BASELINE MODEL PERFORMANCE:\")\n",
    "print(f\"   🎯 RMSE: {rmse:.4f}\")\n",
    "print(f\"   🎯 MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Genre-Specific Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 GENRE-SPECIFIC PERFORMANCE:\n",
      "   🗺️  Adventure (498324 predictions):\n",
      "      MAE: 0.6159, RMSE: 0.8025\n",
      "   🔍 Mystery (487473 predictions):\n",
      "      MAE: 0.6145, RMSE: 0.8014\n",
      "   📚 Other (464050 predictions):\n",
      "      MAE: 0.6742, RMSE: 0.8666\n"
     ]
    }
   ],
   "source": [
    "# Separate predictions by genre\n",
    "adventure_predictions = [p for p in predictions if p.iid in adventure_book_ids]\n",
    "mystery_predictions = [p for p in predictions if p.iid in mystery_book_ids]\n",
    "other_predictions = [p for p in predictions if p.iid not in adventure_book_ids + mystery_book_ids]\n",
    "\n",
    "# Calculate genre-specific metrics\n",
    "def calculate_metrics(pred_list):\n",
    "    if not pred_list:\n",
    "        return None, None\n",
    "    errors = [abs(p.est - p.r_ui) for p in pred_list]\n",
    "    squared_errors = [(p.est - p.r_ui) ** 2 for p in pred_list]\n",
    "    mae = np.mean(errors)\n",
    "    rmse = np.sqrt(np.mean(squared_errors))\n",
    "    return mae, rmse\n",
    "\n",
    "adv_mae, adv_rmse = calculate_metrics(adventure_predictions)\n",
    "mys_mae, mys_rmse = calculate_metrics(mystery_predictions)\n",
    "oth_mae, oth_rmse = calculate_metrics(other_predictions)\n",
    "\n",
    "print(f\"📊 GENRE-SPECIFIC PERFORMANCE:\")\n",
    "print(f\"   🗺️  Adventure ({len(adventure_predictions)} predictions):\")\n",
    "print(f\"      MAE: {adv_mae:.4f}, RMSE: {adv_rmse:.4f}\")\n",
    "print(f\"   🔍 Mystery ({len(mystery_predictions)} predictions):\")\n",
    "print(f\"      MAE: {mys_mae:.4f}, RMSE: {mys_rmse:.4f}\")\n",
    "print(f\"   📚 Other ({len(other_predictions)} predictions):\")\n",
    "print(f\"      MAE: {oth_mae:.4f}, RMSE: {oth_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Recommendation Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Generating sample recommendations...\n",
      "✅ Generated recommendations for 50 users\n"
     ]
    }
   ],
   "source": [
    "# Generate top-N recommendations for a sample of users\n",
    "def get_top_n_recommendations(model, trainset, n=10, sample_users=100):\n",
    "    \"\"\"\n",
    "    Generate top-N recommendations for a sample of users\n",
    "    \"\"\"\n",
    "    # Get all book IDs\n",
    "    all_book_ids = set([iid for (uid, iid, rating) in trainset.all_ratings()])\n",
    "    \n",
    "    # Sample random users\n",
    "    all_user_ids = set([uid for (uid, iid, rating) in trainset.all_ratings()])\n",
    "    sample_user_ids = np.random.choice(list(all_user_ids), \n",
    "                                       min(sample_users, len(all_user_ids)), \n",
    "                                       replace=False)\n",
    "    \n",
    "    recommendations = defaultdict(list)\n",
    "    \n",
    "    for uid in sample_user_ids:\n",
    "        # Get books user has already rated\n",
    "        user_rated_books = set([iid for (u, iid, rating) in trainset.all_ratings() if u == uid])\n",
    "        \n",
    "        # Get recommendations for unrated books\n",
    "        candidates = all_book_ids - user_rated_books\n",
    "        \n",
    "        user_recommendations = []\n",
    "        for iid in candidates:\n",
    "            pred = model.predict(uid, iid)\n",
    "            user_recommendations.append((iid, pred.est))\n",
    "        \n",
    "        # Sort by predicted rating and take top N\n",
    "        user_recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "        recommendations[uid] = user_recommendations[:n]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "print(\"🔄 Generating sample recommendations...\")\n",
    "baseline_recommendations = get_top_n_recommendations(svd_model, trainset, n=10, sample_users=50)\n",
    "print(f\"✅ Generated recommendations for {len(baseline_recommendations)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Genre Distribution in Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BASELINE RECOMMENDATION GENRE DISTRIBUTION:\n",
      "   🗺️  Adventure: 244 (48.8%)\n",
      "   🔍 Mystery: 41 (8.2%)\n",
      "   📚 Other: 215 (43.0%)\n",
      "   📊 Total recommendations: 500\n"
     ]
    }
   ],
   "source": [
    "# Analyze genre distribution in recommendations\n",
    "def analyze_genre_distribution(recommendations, adventure_ids, mystery_ids):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of genres in recommendations\n",
    "    \"\"\"\n",
    "    adventure_count = 0\n",
    "    mystery_count = 0\n",
    "    other_count = 0\n",
    "    total_recommendations = 0\n",
    "    \n",
    "    for user_recs in recommendations.values():\n",
    "        for book_id, rating in user_recs:\n",
    "            total_recommendations += 1\n",
    "            if book_id in adventure_ids:\n",
    "                adventure_count += 1\n",
    "            elif book_id in mystery_ids:\n",
    "                mystery_count += 1\n",
    "            else:\n",
    "                other_count += 1\n",
    "    \n",
    "    return {\n",
    "        'adventure': (adventure_count, adventure_count / total_recommendations * 100),\n",
    "        'mystery': (mystery_count, mystery_count / total_recommendations * 100),\n",
    "        'other': (other_count, other_count / total_recommendations * 100),\n",
    "        'total': total_recommendations\n",
    "    }\n",
    "\n",
    "baseline_genre_dist = analyze_genre_distribution(baseline_recommendations, adventure_book_ids, mystery_book_ids)\n",
    "\n",
    "print(f\"📊 BASELINE RECOMMENDATION GENRE DISTRIBUTION:\")\n",
    "print(f\"   🗺️  Adventure: {baseline_genre_dist['adventure'][0]} ({baseline_genre_dist['adventure'][1]:.1f}%)\")\n",
    "print(f\"   🔍 Mystery: {baseline_genre_dist['mystery'][0]} ({baseline_genre_dist['mystery'][1]:.1f}%)\")\n",
    "print(f\"   📚 Other: {baseline_genre_dist['other'][0]} ({baseline_genre_dist['other'][1]:.1f}%)\")\n",
    "print(f\"   📊 Total recommendations: {baseline_genre_dist['total']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Baseline results saved!\n",
      "📁 Results saved to: ../results/baseline\n",
      "\n",
      "🔄 Next step: Run 03_bias_injection.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Save model and results\n",
    "results_path = '../results/baseline'\n",
    "\n",
    "# Save model\n",
    "with open(f'{results_path}/baseline_svd_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svd_model, f)\n",
    "\n",
    "# Save test set for consistent evaluation\n",
    "with open(f'{results_path}/baseline_testset.pkl', 'wb') as f:\n",
    "    pickle.dump(testset, f)\n",
    "\n",
    "# Save baseline metrics\n",
    "baseline_results = {\n",
    "    'overall_rmse': rmse,\n",
    "    'overall_mae': mae,\n",
    "    'adventure_mae': adv_mae,\n",
    "    'adventure_rmse': adv_rmse,\n",
    "    'mystery_mae': mys_mae,\n",
    "    'mystery_rmse': mys_rmse,\n",
    "    'other_mae': oth_mae,\n",
    "    'other_rmse': oth_rmse,\n",
    "    'adventure_rec_count': baseline_genre_dist['adventure'][0],\n",
    "    'adventure_rec_percentage': baseline_genre_dist['adventure'][1],\n",
    "    'mystery_rec_count': baseline_genre_dist['mystery'][0],\n",
    "    'mystery_rec_percentage': baseline_genre_dist['mystery'][1],\n",
    "    'other_rec_count': baseline_genre_dist['other'][0],\n",
    "    'other_rec_percentage': baseline_genre_dist['other'][1],\n",
    "    'total_recommendations': baseline_genre_dist['total']\n",
    "}\n",
    "\n",
    "pd.DataFrame([baseline_results]).to_csv(f'{results_path}/baseline_results.csv', index=False)\n",
    "\n",
    "# Save sample recommendations\n",
    "rec_df = []\n",
    "for user_id, recs in baseline_recommendations.items():\n",
    "    for rank, (book_id, predicted_rating) in enumerate(recs, 1):\n",
    "        rec_df.append({\n",
    "            'user_id': user_id,\n",
    "            'book_id': book_id,\n",
    "            'rank': rank,\n",
    "            'predicted_rating': predicted_rating\n",
    "        })\n",
    "\n",
    "pd.DataFrame(rec_df).to_csv(f'{results_path}/baseline_recommendations.csv', index=False)\n",
    "\n",
    "print(\"✅ Baseline results saved!\")\n",
    "print(f\"📁 Results saved to: {results_path}\")\n",
    "print(\"\\n🔄 Next step: Run 03_bias_injection.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
