{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_heavy_bias_allrated.py\n",
    "# For each genre G1 (primary),\n",
    "# create synthetic users who rate *all* books: 5 if primary == G1, else 0.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "BASE_DIR   = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book\")\n",
    "INPUT_CSV  = BASE_DIR / \"data/df_final_with_genres.csv\"   # must have: user_id, book_id, rating, genres\n",
    "OUT_DIR    = BASE_DIR / \"result/rec/top_re/0928/data/improved_synthetic_heavy_allrated\"\n",
    "SUMMARY_TXT= BASE_DIR / \"result/rec/top_re/0928/data/improved_synthetic_heavy_allrated_summary.txt\"\n",
    "SUMMARY_CSV= BASE_DIR / \"result/rec/top_re/0928/data/improved_synthetic_heavy_allrated_summary.csv\"\n",
    "\n",
    "GENRE_COL  = \"genres\"\n",
    "USER_COL   = \"user_id\"\n",
    "BOOK_COL   = \"book_id\"\n",
    "RATING_COL = \"rating\"\n",
    "\n",
    "RUNS = [25, 400]   # number of synthetic users to inject per genre\n",
    "TARGET_RATING = 5\n",
    "OTHER_RATING  = 0\n",
    "# =========================\n",
    "\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def primary_genre(cell: str) -> str:\n",
    "    if not isinstance(cell, str) or not cell.strip():\n",
    "        return \"\"\n",
    "    return cell.split(\",\")[0].strip()\n",
    "\n",
    "def main():\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---------- Load ----------\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    # numeric hygiene\n",
    "    df[USER_COL]   = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]   = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "    df[GENRE_COL]  = df[GENRE_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "\n",
    "    # book->full genres (preserve original)\n",
    "    book_to_genres = dict(df[[BOOK_COL, GENRE_COL]].drop_duplicates().values)\n",
    "\n",
    "    # compute primary genre per book and build book->primary map (2-column dict)\n",
    "    work = df[[BOOK_COL, GENRE_COL]].copy()\n",
    "    work[\"_primary\"] = work[GENRE_COL].apply(primary_genre)\n",
    "    book_to_primary = (\n",
    "        work[[BOOK_COL, \"_primary\"]]\n",
    "        .drop_duplicates(subset=[BOOK_COL])\n",
    "        .set_index(BOOK_COL)[\"_primary\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    target_genres = sorted(\n",
    "        set(g for g in work[\"_primary\"] if g),\n",
    "        key=lambda x: x.lower()\n",
    "    )\n",
    "\n",
    "    BLOCK = 1_000_000\n",
    "    rows_summary = []\n",
    "\n",
    "    with open(SUMMARY_TXT, \"w\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=== BASELINE ===\\n\")\n",
    "        log.write(f\"üë§ Unique users: {baseline_users:,}\\n\")\n",
    "        log.write(f\"üßæ Rows: {baseline_rows:,}\\n\")\n",
    "        log.write(f\"üî¢ Synthetic user_id base start: {base_start_uid}\\n\")\n",
    "        log.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    grand_added = 0\n",
    "    made_any = False\n",
    "\n",
    "    all_books = sorted(df[BOOK_COL].unique().tolist())\n",
    "    n_all_books = len(all_books)\n",
    "\n",
    "    for gi, g in enumerate(target_genres):\n",
    "        safe_g = sanitize_fn(g)\n",
    "        with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"üé≠ {g} | total books rated by each synthetic user = {n_all_books}\\n\")\n",
    "\n",
    "        for r_i, run in enumerate(RUNS):\n",
    "            start_uid = base_start_uid + gi * (len(RUNS) * BLOCK) + r_i * BLOCK\n",
    "            new_uids = list(range(start_uid, start_uid + run))\n",
    "\n",
    "            synth = {USER_COL: [], BOOK_COL: [], RATING_COL: [], GENRE_COL: []}\n",
    "            append_u = synth[USER_COL].append\n",
    "            append_b = synth[BOOK_COL].append\n",
    "            append_r = synth[RATING_COL].append\n",
    "            append_g = synth[GENRE_COL].append\n",
    "\n",
    "            for uid in new_uids:\n",
    "                for b in all_books:\n",
    "                    pg = book_to_primary.get(b, \"\")\n",
    "                    rating = TARGET_RATING if pg == g else OTHER_RATING\n",
    "                    append_u(uid); append_b(b); append_r(rating); append_g(book_to_genres.get(b, \"\"))\n",
    "\n",
    "            synth_df = pd.DataFrame(synth, columns=[USER_COL, BOOK_COL, RATING_COL, GENRE_COL])\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            expected = run * n_all_books\n",
    "            assert len(synth_df) == expected, f\"Row count mismatch for {g}, run={run}\"\n",
    "\n",
    "            out_path = OUT_DIR / f\"allrated_{safe_g}_{run}.csv\"\n",
    "            combined.to_csv(out_path, index=False)\n",
    "\n",
    "            with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"  run={run:>4} ‚Üí records_added={expected:>9,} | \"\n",
    "                          f\"new_rows={len(combined):,} | new_users={combined[USER_COL].nunique():,}\\n\")\n",
    "\n",
    "            rows_summary.append({\n",
    "                \"genre\": g,\n",
    "                \"safe_genre\": safe_g,\n",
    "                \"all_books\": n_all_books,\n",
    "                \"run_users\": run,\n",
    "                \"records_added\": expected,\n",
    "                \"new_total_rows\": len(combined),\n",
    "                \"new_total_users\": combined[USER_COL].nunique(),\n",
    "                \"output_csv\": str(out_path)\n",
    "            })\n",
    "\n",
    "            grand_added += expected\n",
    "            made_any = True\n",
    "\n",
    "        with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(\"\\n\")\n",
    "\n",
    "    if rows_summary:\n",
    "        pd.DataFrame(rows_summary).to_csv(SUMMARY_CSV, index=False)\n",
    "\n",
    "    with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"================================================================================\\n\")\n",
    "        log.write(f\"Grand total injected rows: {grand_added:,}\\n\")\n",
    "        log.write(f\"Outputs folder: {OUT_DIR}\\n\")\n",
    "        log.write(f\"Per-run summary CSV: {SUMMARY_CSV}\\n\")\n",
    "\n",
    "    print(\"\\n‚úÖ Done.\" if made_any else \"‚ö†Ô∏è No datasets were produced.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
