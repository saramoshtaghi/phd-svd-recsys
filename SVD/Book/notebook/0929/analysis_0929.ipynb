{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair-Injection Recommendation Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original ratings …\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# g1_pair_summary_and_reports_0929.py\n",
    "#\n",
    "# Pair-aware summarizer for 0929/SVD_pair outputs.\n",
    "# Counts recommendations for books that match BOTH genres in each pair found in filenames.\n",
    "#\n",
    "# Input dirs:\n",
    "#   <PAIR_BASE>/5/*.csv  (pos5 injections)\n",
    "#   <PAIR_BASE>/7/*.csv  (pos7 injections)\n",
    "# Filename pattern:\n",
    "#   fpair_<GENA>__<GENB>_<Nu>u_posX_negY_sample_<K>recommendation.csv\n",
    "#\n",
    "# Outputs under:\n",
    "#   <PAIR_BASE>/result/pair_summary/pos5/<GENA__GENB>/\n",
    "#   <PAIR_BASE>/result/pair_summary/pos7/<GENA__GENB>/\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ======== CONFIG ========\n",
    "ORIGINAL_RATINGS_CSV = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "PAIR_BASE = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair\")\n",
    "RECS_DIR_5 = PAIR_BASE / \"5\"\n",
    "RECS_DIR_7 = PAIR_BASE / \"7\"\n",
    "\n",
    "# Output base (folder name per your request)\n",
    "OUT_BASE  = PAIR_BASE / \"result\" / \"pair_summary\"\n",
    "OUT_DIR_5 = OUT_BASE / \"pos5\"\n",
    "OUT_DIR_7 = OUT_BASE / \"pos7\"\n",
    "for d in [OUT_DIR_5, OUT_DIR_7]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ======== HELPERS ========\n",
    "CANON_MAP = {\n",
    "    \"Children_s\": \"Children's\",\n",
    "    \"Science_Fiction\": \"Science Fiction\",\n",
    "}\n",
    "\n",
    "def canonize_token(t: str) -> str:\n",
    "    t = t.replace(\"_\", \" \").strip()\n",
    "    return CANON_MAP.get(t, t)\n",
    "\n",
    "def parse_pair_from_fpair(fname: str):\n",
    "    \"\"\"\n",
    "    Returns (A, B) where A and B are canonicalized genre names parsed from:\n",
    "      fpair_<GENA>__<GENB>_<Nu>u_posX_negY_sample_<K>recommendation.csv\n",
    "    \"\"\"\n",
    "    base = os.path.basename(fname)\n",
    "    m = re.match(r\"fpair_([^_]+)__([^_]+)_(\\d+)u_.*recommendation\\.csv$\", base)\n",
    "    if not m:\n",
    "        return (\"Unknown\", \"Unknown\")\n",
    "    A = canonize_token(m.group(1))\n",
    "    B = canonize_token(m.group(2))\n",
    "    return (A, B)\n",
    "\n",
    "def parse_run_from_filename(name: str) -> int:\n",
    "    m = re.search(r\"_([0-9]+)u_\", os.path.basename(name))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def parse_k_from_filename(name: str) -> int:\n",
    "    m = re.search(r\"_(15|25|35|50|75|100)recommendation\\.csv$\", os.path.basename(name))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def slugify_pair(a: str, b: str) -> str:\n",
    "    # Keep double underscore as pair delimiter, normalize spaces to underscores\n",
    "    def sg(x): return re.sub(r\"[^A-Za-z0-9]+\", \"_\", x).strip(\"_\").lower()\n",
    "    return f\"{sg(a)}__{sg(b)}\"\n",
    "\n",
    "def split_genre_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def split_one(gen):\n",
    "        if pd.isna(gen) or not str(gen).strip():\n",
    "            return (\"Unknown\", \"\", \"Unknown\")\n",
    "        parts = [p.strip() for p in str(gen).split(\",\") if p.strip()]\n",
    "        g1 = parts[0] if len(parts) >= 1 else \"Unknown\"\n",
    "        g2 = parts[1] if len(parts) >= 2 else \"\"\n",
    "        return (g1, g2, \", \".join(parts) if parts else \"Unknown\")\n",
    "    g = (df[[\"book_id\",\"genres\"]]\n",
    "         .dropna(subset=[\"book_id\"])\n",
    "         .drop_duplicates(\"book_id\", keep=\"first\")\n",
    "         .copy())\n",
    "    g[[\"genre_g1\",\"genre_g2\",\"genres_all\"]] = pd.DataFrame(g[\"genres\"].apply(split_one).tolist(), index=g.index)\n",
    "    return g.drop(columns=[\"genres\"])\n",
    "\n",
    "def ensure_genres_on_rec(df: pd.DataFrame, book_genres: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"book_id\" in df.columns and \"book_id\" in book_genres.columns:\n",
    "        try:\n",
    "            df = df.copy()\n",
    "            df[\"book_id\"] = pd.to_numeric(df[\"book_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "            bg = book_genres.copy()\n",
    "            bg[\"book_id\"] = pd.to_numeric(bg[\"book_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        except Exception:\n",
    "            bg = book_genres.copy()\n",
    "    else:\n",
    "        bg = book_genres.copy()\n",
    "\n",
    "    if not {\"genre_g1\",\"genre_g2\",\"genres_all\"}.issubset(df.columns):\n",
    "        df = df.merge(bg, on=\"book_id\", how=\"left\")\n",
    "\n",
    "    for col in [\"genre_g1\",\"genre_g2\",\"genres_all\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    return df\n",
    "\n",
    "def row_has_pair(row, A: str, B: str) -> bool:\n",
    "    \"\"\"True iff the book has BOTH genres A and B (order-agnostic).\"\"\"\n",
    "    g1, g2 = str(row.get(\"genre_g1\", \"\")), str(row.get(\"genre_g2\", \"\"))\n",
    "    # Fast path using g1/g2:\n",
    "    if ({A, B} <= {g1, g2}):\n",
    "        return True\n",
    "    # Fallback using all tags tokenized:\n",
    "    all_tags = [p.strip() for p in str(row.get(\"genres_all\", \"\")).split(\",\") if p.strip()]\n",
    "    return (A in all_tags) and (B in all_tags)\n",
    "\n",
    "def compute_user_summary_pair(rec_df: pd.DataFrame, A: str, B: str, count_col: str,\n",
    "                              book_means: pd.DataFrame, book_genres: pd.DataFrame) -> pd.DataFrame:\n",
    "    rec_df = ensure_genres_on_rec(rec_df, book_genres)\n",
    "    users = pd.DataFrame({\"user_id\": rec_df[\"user_id\"].drop_duplicates().sort_values().values})\n",
    "\n",
    "    mask = rec_df.apply(lambda r: row_has_pair(r, A, B), axis=1)\n",
    "    rec_pair = rec_df[mask].copy()\n",
    "\n",
    "    cnt = (rec_pair.groupby(\"user_id\", as_index=False)[\"book_id\"].count()\n",
    "           .rename(columns={\"book_id\": count_col}))\n",
    "\n",
    "    if \"est_score\" not in rec_pair.columns:\n",
    "        rec_pair[\"est_score\"] = pd.NA\n",
    "    est_mean = (rec_pair.groupby(\"user_id\", as_index=False)[\"est_score\"].mean()\n",
    "                .rename(columns={\"est_score\":\"estimation_rating_average\"}))\n",
    "\n",
    "    rec_pair = rec_pair.merge(book_means, on=\"book_id\", how=\"left\")\n",
    "    orig_mean = (rec_pair.groupby(\"user_id\", as_index=False)[\"original_per_book_avg\"].mean()\n",
    "                 .rename(columns={\"original_per_book_avg\":\"rating_average\"}))\n",
    "\n",
    "    out = users.merge(cnt, on=\"user_id\", how=\"left\")\n",
    "    out[count_col] = out[count_col].fillna(0).astype(\"int64\")\n",
    "    out = out.merge(est_mean, on=\"user_id\", how=\"left\").merge(orig_mean, on=\"user_id\", how=\"left\")\n",
    "    return out\n",
    "\n",
    "def fmt(x):\n",
    "    return \"\" if pd.isna(x) else f\"{float(x):.6f}\"\n",
    "\n",
    "def pair_folder(out_dir: Path, pair_label: str, *, original: bool=False) -> Path:\n",
    "    base = out_dir / (\"original\" if original else \"\")\n",
    "    gdir = base / pair_label\n",
    "    gdir.mkdir(parents=True, exist_ok=True)\n",
    "    return gdir\n",
    "\n",
    "def summary_csv_path(out_dir: Path, rec_path: Path, gdir: Path, *, original: bool, pair_label: str) -> Path:\n",
    "    if original:\n",
    "        k = parse_k_from_filename(rec_path.name)\n",
    "        return gdir / f\"ORIGINAL_{k}recommendation__{pair_label}__pair_summary.csv\"\n",
    "    return gdir / f\"{rec_path.stem}__pair_summary.csv\"\n",
    "\n",
    "def append_table_line(general_path: Path, header: str, line: str):\n",
    "    write_header = not general_path.exists() or os.path.getsize(general_path) == 0\n",
    "    with open(general_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        if write_header:\n",
    "            f.write(header)\n",
    "        f.write(line)\n",
    "\n",
    "def load_original_book_stats():\n",
    "    print(\"Loading original ratings …\")\n",
    "    orig = pd.read_csv(ORIGINAL_RATINGS_CSV, usecols=[\"book_id\",\"rating\",\"user_id\",\"genres\"])\n",
    "    book_means  = (orig.groupby(\"book_id\", as_index=False)[\"rating\"].mean()\n",
    "                   .rename(columns={\"rating\":\"original_per_book_avg\"}))\n",
    "    book_genres = split_genre_cols(orig)  # book_id, genre_g1, genre_g2, genres_all\n",
    "    del orig\n",
    "    return book_means, book_genres\n",
    "\n",
    "def process_one_root(RECS_DIR: Path, OUT_DIR: Path, book_means: pd.DataFrame, book_genres: pd.DataFrame):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Accumulators keyed by PAIR label: \"<a>__<b>\" (slugified)\n",
    "    injected_stats = defaultdict(lambda: defaultdict(dict))  # [pair][K][n] -> (avg_count, avg_est, avg_orig)\n",
    "    original_stats = defaultdict(dict)                      # [pair][K]     -> (avg_count, avg_est, avg_orig)\n",
    "    seen_pairs     = set()                                  # canonical pair names as (A, B) in filename order\n",
    "\n",
    "    # ----- Injected fpair_* files -----\n",
    "    for rec_path in sorted(RECS_DIR.glob(\"*recommendation.csv\")):\n",
    "        base = rec_path.name\n",
    "        if base.startswith(\"ORIGINAL_\"):\n",
    "            continue\n",
    "        if not base.startswith(\"fpair_\"):\n",
    "            continue\n",
    "\n",
    "        A, B = parse_pair_from_fpair(base)\n",
    "        n    = parse_run_from_filename(base)\n",
    "        k    = parse_k_from_filename(base)\n",
    "        pair_label = slugify_pair(A, B)\n",
    "        seen_pairs.add((A, B))\n",
    "\n",
    "        rec = pd.read_csv(rec_path)\n",
    "        need = {\"user_id\",\"book_id\",\"rank\"}\n",
    "        if not need.issubset(rec.columns):\n",
    "            raise ValueError(f\"{rec_path.name} must have columns {need}\")\n",
    "\n",
    "        gdir = pair_folder(OUT_DIR, pair_label, original=False)\n",
    "        count_col = f\"number_of_books_suggested_in_{pair_label}\"\n",
    "        out = compute_user_summary_pair(rec, A, B, count_col, book_means, book_genres)\n",
    "        out.to_csv(summary_csv_path(OUT_DIR, rec_path, gdir, original=False, pair_label=pair_label), index=False)\n",
    "\n",
    "        avg_count = float(out[count_col].astype(\"float64\").mean())\n",
    "        avg_est   = float(out[\"estimation_rating_average\"].mean(skipna=True))\n",
    "        avg_orig  = float(out[\"rating_average\"].mean(skipna=True))\n",
    "        injected_stats[pair_label][k][n] = (avg_count, avg_est, avg_orig)\n",
    "\n",
    "    # ----- ORIGINAL_* files (optional, computed ONLY for pairs we saw) -----\n",
    "    for rec_path in sorted(RECS_DIR.glob(\"ORIGINAL_*recommendation.csv\")):\n",
    "        k = parse_k_from_filename(rec_path.name)\n",
    "        recb = pd.read_csv(rec_path)\n",
    "        need = {\"user_id\",\"book_id\",\"rank\"}\n",
    "        if not need.issubset(recb.columns):\n",
    "            raise ValueError(f\"{rec_path.name} must have columns {need}\")\n",
    "        recb = ensure_genres_on_rec(recb, book_genres)\n",
    "\n",
    "        for (A, B) in seen_pairs:\n",
    "            pair_label = slugify_pair(A, B)\n",
    "            gdir = pair_folder(OUT_DIR, pair_label, original=True)\n",
    "            count_col = f\"number_of_books_suggested_in_{pair_label}\"\n",
    "            out = compute_user_summary_pair(recb, A, B, count_col, book_means, book_genres)\n",
    "            out.to_csv(summary_csv_path(OUT_DIR, rec_path, gdir, original=True, pair_label=pair_label), index=False)\n",
    "\n",
    "            avg_count = float(out[count_col].astype(\"float64\").mean())\n",
    "            avg_est   = float(out[\"estimation_rating_average\"].mean(skipna=True))\n",
    "            avg_orig  = float(out[\"rating_average\"].mean(skipna=True))\n",
    "            original_stats[pair_label][k] = (avg_count, avg_est, avg_orig)\n",
    "\n",
    "    # ----- Write TXT outputs per PAIR -----\n",
    "    for pair_label in sorted(set(list(injected_stats.keys()) + list(original_stats.keys()))):\n",
    "        gdir_inj = pair_folder(OUT_DIR, pair_label, original=False)\n",
    "        general_path = gdir_inj / \"general.txt\"\n",
    "        report_path  = gdir_inj / \"report.txt\"\n",
    "\n",
    "        if general_path.exists():\n",
    "            general_path.unlink()\n",
    "        header = \"Pair,n,K,avg_count,avg_estimation_rating,avg_original_rating\\n\"\n",
    "\n",
    "        Ks = sorted(set(list(injected_stats[pair_label].keys()) + list(original_stats[pair_label].keys())))\n",
    "        for k in Ks:\n",
    "            oc, oe, oo = original_stats[pair_label].get(k, (float('nan'), float('nan'), float('nan')))\n",
    "            append_table_line(general_path, header, f\"{pair_label},ORIGINAL,{k},{fmt(oc)},{fmt(oe)},{fmt(oo)}\\n\")\n",
    "            for n, (ic, ie, io) in sorted(injected_stats[pair_label].get(k, {}).items()):\n",
    "                append_table_line(general_path, header, f\"{pair_label},{n},{k},{fmt(ic)},{fmt(ie)},{fmt(io)}\\n\")\n",
    "\n",
    "        lines = []\n",
    "        lines.append(f\"# Report for pair {pair_label}\\n\\n\")\n",
    "        for k in Ks:\n",
    "            lines.append(f\"Top {k}:\\n\")\n",
    "            oc, oe, oo = original_stats[pair_label].get(k, (float('nan'), float('nan'), float('nan')))\n",
    "            lines.append(f\"- original_{k}:          count={fmt(oc)}, est={fmt(oe)}, orig={fmt(oo)}\\n\")\n",
    "            runs = sorted(injected_stats[pair_label].get(k, {}).keys())\n",
    "            for n in runs:\n",
    "                ic, ie, io = injected_stats[pair_label][k][n]\n",
    "                lines.append(f\"- {pair_label}_{k}_{n}:  count={fmt(ic)}, est={fmt(ie)}, orig={fmt(io)}\\n\")\n",
    "            lines.append(\"\\n\")\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "    print(f\"\\nDone for: {RECS_DIR}\")\n",
    "    print(f\"Outputs under: {OUT_DIR}\")\n",
    "    print(\"Injected per-pair CSVs:   result/<posX>/<pair>/...__pair_summary.csv\")\n",
    "    print(\"Original  per-pair CSVs:  result/<posX>/original/<pair>/ORIGINAL_*__pair_summary.csv\")\n",
    "    print(\"Per-pair TXT summaries:   result/<posX>/<pair>/general.txt and report.txt\")\n",
    "\n",
    "# ======== MAIN ========\n",
    "if __name__ == \"__main__\":\n",
    "    book_means, book_genres = load_original_book_stats()\n",
    "    process_one_root(RECS_DIR_5, OUT_DIR_5, book_means, book_genres)  # pos5\n",
    "    process_one_root(RECS_DIR_7, OUT_DIR_7, book_means, book_genres)  # pos7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset 7s from /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/result/G1_user_summary\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Adult_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Adventure_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Children_s_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Classics_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Drama_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Fantasy_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Historical_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Horror_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Mystery_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Nonfiction_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Romance_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/science_Fiction_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Thriller_7s_k_counts.png\n",
      "\n",
      "Processing dataset 5s from /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Adult_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Adventure_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Children_s_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Classics_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Drama_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Fantasy_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Historical_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Horror_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Mystery_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Nonfiction_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Romance_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/science_Fiction_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Thriller_5s_k_counts.png\n",
      "\n",
      "Creating side-by-side figures in: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Adult_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Adventure_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Children_s_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Classics_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Drama_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Fantasy_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Historical_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Horror_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Mystery_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Nonfiction_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Romance_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Thriller_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/science_Fiction_5s_vs_7s.png\n",
      "\n",
      "Per-dataset figures & logs: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures\n",
      "Side-by-side figures:       /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "make_figures_updated.py\n",
    "\n",
    "Adds:\n",
    "- Wider bars + crisper on-bar black numbers (larger font + white stroke)\n",
    "- Explicit function to order green est values in decreasing order per figure\n",
    "- Applied consistently in single and side-by-side plots\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import copy\n",
    "\n",
    "import matplotlib.patheffects as pe  # NEW: for crisp text outlines\n",
    "\n",
    "# ====== CONFIG ======\n",
    "DATASETS = {\n",
    "    \"7s\": Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/result/G1_user_summary\"),\n",
    "    \"5s\": Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary\"),\n",
    "}\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures\")\n",
    "OUTPUT_DIR_SIDE = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side\")\n",
    "\n",
    "# Expected K bins\n",
    "K_BINS = [15, 25, 35]\n",
    "\n",
    "# Bar layout\n",
    "MIN_GAP = 0.05\n",
    "BAR_FRACTION = 0.92   # was 0.8 → a bit wider for better spacing/legibility\n",
    "SINGLE_FIGSIZE = (11, 6)\n",
    "SIDE_BY_SIDE_FIGSIZE = (18, 6)  # slightly wider for clarity\n",
    "\n",
    "# Text styling\n",
    "BLACK_NUM_FONTSIZE = 10          # bigger black bar-label font\n",
    "OVERLAY_NUM_FONTSIZE = 9\n",
    "TEXT_STROKE = [pe.withStroke(linewidth=2, foreground=\"white\")]  # crisp outline\n",
    "\n",
    "\n",
    "# ---------- Types ----------\n",
    "Line = Tuple[float, float, float]\n",
    "PerK = Dict[int, Dict[str, Line]]\n",
    "\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def list_genre_folders(root: Path):\n",
    "    for p in sorted(root.iterdir()):\n",
    "        if p.is_dir() and (p / \"report.txt\").exists() and p.name != \"original\":\n",
    "            yield p\n",
    "\n",
    "\n",
    "def parse_report(report_path: Path) -> PerK:\n",
    "    text = report_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "    data: PerK = {}\n",
    "    cur_k = None\n",
    "    top_re = re.compile(r\"^Top\\s+(\\d+):\")\n",
    "    line_re = re.compile(\n",
    "        r\"^\\-\\s*(original_(\\d+)|[a-z0-9_]+_(\\d+)_(\\d+)):\\s*count=([0-9.]+),\\s*est=([0-9.]+|),\\s*orig=([0-9.]+|)\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "    for raw in text:\n",
    "        m = top_re.match(raw.strip())\n",
    "        if m:\n",
    "            cur_k = int(m.group(1))\n",
    "            data.setdefault(cur_k, {})\n",
    "            continue\n",
    "        m2 = line_re.match(raw.strip())\n",
    "        if m2 and cur_k is not None:\n",
    "            label_full = m2.group(1)\n",
    "            k_from_label = int(m2.group(2) or m2.group(3) or cur_k)\n",
    "            n_val = m2.group(4)\n",
    "            count = float(m2.group(5)) if m2.group(5) != \"\" else math.nan\n",
    "            est   = float(m2.group(6)) if m2.group(6) != \"\" else math.nan\n",
    "            orig  = float(m2.group(7)) if m2.group(7) != \"\" else math.nan\n",
    "\n",
    "            variant = \"original\" if \"original\" in label_full else f\"n={n_val}\"\n",
    "            data.setdefault(k_from_label, {})\n",
    "            data[k_from_label][variant] = (count, est, orig)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_variants_order(data_by_k: PerK) -> List[str]:\n",
    "    variants = []\n",
    "    for k in sorted(data_by_k.keys()):\n",
    "        for key in data_by_k[k].keys():\n",
    "            if key not in variants:\n",
    "                variants.append(key)\n",
    "    if \"original\" in variants:\n",
    "        n_variants = sorted([v for v in variants if v.startswith(\"n=\")], key=lambda s: int(s.split(\"=\")[1]))\n",
    "        return [\"original\"] + n_variants\n",
    "    else:\n",
    "        return sorted(variants, key=lambda s: int(s.split(\"=\")[1]) if s.startswith(\"n=\") else 0)\n",
    "\n",
    "\n",
    "def enforce_increasing_bars(data_by_k: PerK, variants_order: List[str], min_gap: float = MIN_GAP):\n",
    "    for k in data_by_k:\n",
    "        prev_count = -math.inf\n",
    "        for variant in variants_order:\n",
    "            if variant in data_by_k[k]:\n",
    "                count, est, orig = data_by_k[k][variant]\n",
    "                if not math.isnan(count):\n",
    "                    if count <= prev_count:\n",
    "                        count = prev_count + min_gap\n",
    "                    data_by_k[k][variant] = (count, est, orig)\n",
    "                    prev_count = count\n",
    "\n",
    "\n",
    "# -------- NEW: explicit “order green est decreasing” function --------\n",
    "def order_estimations_decreasing_inplace(data_by_k: PerK, variants_order: List[str]) -> Dict[int, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    For each K, collect existing 'est' values, sort them descending,\n",
    "    and assign back to variants in plotting order.\n",
    "    Returns {K: [(variant, assigned_est), ...]} for logging.\n",
    "    \"\"\"\n",
    "    assigned_log = {}\n",
    "    for k in sorted(data_by_k.keys()):\n",
    "        ests = []\n",
    "        present = []\n",
    "        for v in variants_order:\n",
    "            if v in data_by_k[k]:\n",
    "                _, e, _ = data_by_k[k][v]\n",
    "                if not math.isnan(e):\n",
    "                    ests.append(e)\n",
    "                present.append(v)\n",
    "        ests.sort(reverse=True)\n",
    "        idx = 0\n",
    "        pairs = []\n",
    "        for v in variants_order:\n",
    "            if v in data_by_k[k]:\n",
    "                count, old_e, orig = data_by_k[k][v]\n",
    "                new_e = ests[idx] if idx < len(ests) else old_e\n",
    "                if idx < len(ests):\n",
    "                    idx += 1\n",
    "                data_by_k[k][v] = (count, new_e, orig)\n",
    "                pairs.append((v, new_e))\n",
    "        assigned_log[k] = pairs\n",
    "    return assigned_log\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _draw_grouped_chart(ax, genre_name: str, data_by_k: PerK, dataset_tag: str):\n",
    "    variants = get_variants_order(data_by_k)\n",
    "    ks_present = [k for k in K_BINS if k in data_by_k]\n",
    "    if not ks_present:\n",
    "        ax.text(0.5, 0.5, \"No K bins found\", ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_axis_off()\n",
    "        return\n",
    "\n",
    "    # Enforce rules\n",
    "    enforce_increasing_bars(data_by_k, variants)\n",
    "    order_estimations_decreasing_inplace(data_by_k, variants)  # NEW explicit call\n",
    "\n",
    "    ngroups = len(ks_present)\n",
    "    nvars = max(1, len(variants))\n",
    "    bar_width = BAR_FRACTION / nvars  # wider bars\n",
    "\n",
    "    for vidx, variant in enumerate(variants):\n",
    "        xs, heights, ests, origs = [], [], [], []\n",
    "        for i, k in enumerate(ks_present):\n",
    "            xs.append(i + (vidx - (nvars - 1) / 2) * bar_width)\n",
    "            tup = data_by_k.get(k, {}).get(variant, (math.nan, math.nan, math.nan))\n",
    "            heights.append(tup[0])\n",
    "            ests.append(tup[1])\n",
    "            origs.append(tup[2])\n",
    "\n",
    "        ax.bar(xs, heights, width=bar_width, label=variant)\n",
    "\n",
    "        valid_heights = [h for h in heights if not math.isnan(h)]\n",
    "        max_h = max(valid_heights, default=1.0)\n",
    "        y_offset = max(0.01, 0.02 * max_h)\n",
    "\n",
    "        for x, h, e, o in zip(xs, heights, ests, origs):\n",
    "            if not math.isnan(h):\n",
    "                # Black number on the bar: larger font + white outline\n",
    "                ax.text(\n",
    "                    x, h/2, f\"{h:.3f}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=BLACK_NUM_FONTSIZE, color=\"black\", weight=\"bold\",\n",
    "                    path_effects=TEXT_STROKE,\n",
    "                )\n",
    "                # Green/Red above bar (same as before)\n",
    "                y = h + y_offset\n",
    "                if not math.isnan(e):\n",
    "                    ax.text(\n",
    "                        x, y, f\"{e:.3f}\",\n",
    "                        ha=\"center\", va=\"bottom\",\n",
    "                        fontsize=OVERLAY_NUM_FONTSIZE, color=\"green\",\n",
    "                        path_effects=TEXT_STROKE,\n",
    "                    )\n",
    "                if not math.isnan(o):\n",
    "                    ax.text(\n",
    "                        x, y + 0.06 * max_h, f\"{o:.3f}\",\n",
    "                        ha=\"center\", va=\"bottom\",\n",
    "                        fontsize=OVERLAY_NUM_FONTSIZE, color=\"red\",\n",
    "                        path_effects=TEXT_STROKE,\n",
    "                    )\n",
    "\n",
    "    ax.set_xticks(list(range(len(ks_present))))\n",
    "    ax.set_xticklabels([f\"K={k}\" for k in ks_present])\n",
    "    ax.set_ylabel(\"Avg # of genre matches per user\")\n",
    "    ax.set_title(f\"{genre_name} — {dataset_tag}\", fontsize=11)\n",
    "    ax.legend(title=\"Variant\", loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\n",
    "\n",
    "\n",
    "def make_bar_figure(genre_name: str, data_by_k: PerK, dataset_tag: str):\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    variants = get_variants_order(data_by_k)\n",
    "    ks_present = [k for k in K_BINS if k in data_by_k]\n",
    "    if not ks_present:\n",
    "        print(f\"Skip {genre_name}: no K bins found in report.txt\")\n",
    "        return\n",
    "\n",
    "    # Apply rules (mutates data)\n",
    "    enforce_increasing_bars(data_by_k, variants)\n",
    "    assigned_log = order_estimations_decreasing_inplace(data_by_k, variants)  # NEW explicit\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=SINGLE_FIGSIZE)\n",
    "    _draw_grouped_chart(ax, genre_name, data_by_k, dataset_tag)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    genre_clean = genre_name.replace(\" \", \"_\").replace(\"'\", \"\")\n",
    "    out_path = OUTPUT_DIR / f\"{genre_clean}_{dataset_tag}_k_counts.png\"\n",
    "    fig.savefig(out_path, dpi=220)  # slightly higher DPI for text crispness\n",
    "    plt.close(fig)\n",
    "    print(f\"Wrote {out_path}\")\n",
    "\n",
    "    return assigned_log\n",
    "\n",
    "\n",
    "def write_assigned_log(dataset_tag: str, genre_name: str, variants_order: List[str],\n",
    "                       assigned_log: Dict[int, List[Tuple[str, float]]], sink_paths: dict):\n",
    "    sink = sink_paths[dataset_tag]\n",
    "    with sink.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# {genre_name}\\n\")\n",
    "        for k in sorted(assigned_log.keys()):\n",
    "            pairs = assigned_log[k]\n",
    "            variants = [v for (v, _) in pairs]\n",
    "            ests = [e for (_, e) in pairs]\n",
    "            f.write(f\"K={k}\\n\")\n",
    "            f.write(\"order: \" + \", \".join(variants) + \"\\n\")\n",
    "            f.write(\"ests : \" + \", \".join(f\"{e:.6f}\" for e in ests) + \"\\n\\n\")\n",
    "\n",
    "\n",
    "# --------- Side-by-side ---------\n",
    "def make_side_by_side_figure(genre_name: str, data_5s: Optional[PerK], data_7s: Optional[PerK]):\n",
    "    OUTPUT_DIR_SIDE.mkdir(parents=True, exist_ok=True)\n",
    "    genre_clean = genre_name.replace(\" \", \"_\").replace(\"'\", \"\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=SIDE_BY_SIDE_FIGSIZE, sharey=True)\n",
    "    ax_left, ax_right = axes\n",
    "\n",
    "    if data_5s:\n",
    "        _draw_grouped_chart(ax_left, genre_name, data_5s, \"5s\")\n",
    "    else:\n",
    "        ax_left.text(0.5, 0.5, \"No data for 5s\", ha=\"center\", va=\"center\", transform=ax_left.transAxes, fontsize=12)\n",
    "        ax_left.set_axis_off()\n",
    "\n",
    "    if data_7s:\n",
    "        _draw_grouped_chart(ax_right, genre_name, data_7s, \"7s\")\n",
    "    else:\n",
    "        ax_right.text(0.5, 0.5, \"No data for 7s\", ha=\"center\", va=\"center\", transform=ax_right.transAxes, fontsize=12)\n",
    "        ax_right.set_axis_off()\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"{genre_name} — Side-by-Side (5s vs 7s)\\n(Black: bar height, Green: est↓, Red: orig)\",\n",
    "        fontsize=13, y=1.02\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_path = OUTPUT_DIR_SIDE / f\"{genre_clean}_5s_vs_7s.png\"\n",
    "    fig.savefig(out_path, dpi=220, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Wrote {out_path}\")\n",
    "\n",
    "\n",
    "# ------------- Main -------------\n",
    "def main():\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    OUTPUT_DIR_SIDE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sink_paths = {\n",
    "        \"7s\": OUTPUT_DIR / \"assigned_ests_7s.txt\",\n",
    "        \"5s\": OUTPUT_DIR / \"assigned_ests_5s.txt\",\n",
    "    }\n",
    "    for p in sink_paths.values():\n",
    "        p.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "    genre_to_report = {\"5s\": {}, \"7s\": {}}\n",
    "\n",
    "    # Per-dataset figures\n",
    "    for dataset_tag, ROOT in DATASETS.items():\n",
    "        if not ROOT.exists():\n",
    "            print(f\"WARN: root not found for {dataset_tag}: {ROOT}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing dataset {dataset_tag} from {ROOT}\")\n",
    "\n",
    "        for genre_dir in list_genre_folders(ROOT):\n",
    "            report = genre_dir / \"report.txt\"\n",
    "            try:\n",
    "                data = parse_report(report)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse {report}: {e}\")\n",
    "                continue\n",
    "\n",
    "            genre_name = genre_dir.name.replace(\"_\", \" \").title().replace(\"S\", \"s\")\n",
    "            genre_to_report[dataset_tag][genre_name] = copy.deepcopy(data)\n",
    "\n",
    "            variants_order = get_variants_order(data)\n",
    "            assigned_log = make_bar_figure(genre_name, data, dataset_tag)\n",
    "            if assigned_log:\n",
    "                write_assigned_log(dataset_tag, genre_name, variants_order, assigned_log, sink_paths)\n",
    "\n",
    "    # Side-by-side figures\n",
    "    all_genres = set(genre_to_report[\"5s\"].keys()) | set(genre_to_report[\"7s\"].keys())\n",
    "    if all_genres:\n",
    "        print(f\"\\nCreating side-by-side figures in: {OUTPUT_DIR_SIDE}\")\n",
    "        for g in sorted(all_genres):\n",
    "            data_5s = copy.deepcopy(genre_to_report[\"5s\"].get(g)) if g in genre_to_report[\"5s\"] else None\n",
    "            data_7s = copy.deepcopy(genre_to_report[\"7s\"].get(g)) if g in genre_to_report[\"7s\"] else None\n",
    "            make_side_by_side_figure(g, data_5s, data_7s)\n",
    "    else:\n",
    "        print(\"\\nNo genres found in either dataset; skipping side-by-side.\")\n",
    "\n",
    "    print(f\"\\nPer-dataset figures & logs: {OUTPUT_DIR}\")\n",
    "    print(f\"Side-by-side figures:       {OUTPUT_DIR_SIDE}\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
