{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair/result/pair_summary/pos5/adult__classics/selected_pairs_summary.csv\n",
      "[OK] Saved: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair/result/pair_summary/pos5/adult__fantasy/selected_pairs_summary.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# selected_pairs_pos5_summary.py\n",
    "#\n",
    "# Restricts to pos5 and these pairs only:\n",
    "#   1) Adult + Classics\n",
    "#   2) Adult + Fantasy\n",
    "#   3) Mystery + Romance\n",
    "#\n",
    "# For K in {15, 25, 35}, compute:\n",
    "#   - ORIGINAL from: <PAIR_BASE>/ORIGINAL_{K}recommendation.csv\n",
    "#   - Injected (n in {25, 50, 100, 200}) from: <PAIR_BASE>/5/fpair_*__*_{n}u_pos5_*_sample_{K}recommendation.csv\n",
    "#\n",
    "# Output:\n",
    "#   <PAIR_BASE>/result/pair_summary/pos5/<pair_slug>/selected_pairs_summary.csv\n",
    "#   <PAIR_BASE>/result/pair_summary/pos5/ALL_selected_pairs_summary.csv\n",
    "#\n",
    "# Columns:\n",
    "#   pair, K, n, avg_count, users_counted, source\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "PAIR_BASE = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair\")\n",
    "RECS_DIR_5 = PAIR_BASE / \"5\"\n",
    "\n",
    "# Output base (reuse pair_summary/pos5 tree)\n",
    "OUT_BASE  = PAIR_BASE / \"result\" / \"pair_summary\" / \"pos5\"\n",
    "OUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pairs (canonical names)\n",
    "PAIRS = [\n",
    "    (\"Adult\", \"Classics\"),\n",
    "    (\"Adult\", \"Fantasy\"),\n",
    "    (\"Mystery\", \"Romance\"),\n",
    "]\n",
    "\n",
    "K_VALUES = [15, 25, 35]\n",
    "N_VALUES = [25, 50, 100, 200]\n",
    "\n",
    "# ================== HELPERS ==================\n",
    "CANON_MAP = {\n",
    "    \"Children_s\": \"Children's\",\n",
    "    \"Science_Fiction\": \"Science Fiction\",\n",
    "}\n",
    "\n",
    "def canonize_token(t: str) -> str:\n",
    "    t = t.replace(\"_\", \" \").strip()\n",
    "    return CANON_MAP.get(t, t)\n",
    "\n",
    "def slugify_pair(a: str, b: str) -> str:\n",
    "    def sg(x): return re.sub(r\"[^A-Za-z0-9]+\", \"_\", x).strip(\"_\").lower()\n",
    "    return f\"{sg(a)}__{sg(b)}\"\n",
    "\n",
    "def row_has_pair(row, A: str, B: str) -> bool:\n",
    "    g1, g2 = str(row.get(\"genre_g1\", \"\")), str(row.get(\"genre_g2\", \"\"))\n",
    "    if ({A, B} <= {g1, g2}):\n",
    "        return True\n",
    "    all_tags = [p.strip() for p in str(row.get(\"genres_all\", \"\")).split(\",\") if p.strip()]\n",
    "    return (A in all_tags) and (B in all_tags)\n",
    "\n",
    "def ensure_genres_on_rec(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Expect columns genre_g1, genre_g2, genres_all already present in the recommendation CSVs.\n",
    "    # If not, you can merge from a book metadata table here. For now, assume present.\n",
    "    for col in [\"genre_g1\", \"genre_g2\", \"genres_all\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    return df\n",
    "\n",
    "def count_avg_for_pair(rec_df: pd.DataFrame, A: str, B: str) -> tuple[float, int]:\n",
    "    rec_df = ensure_genres_on_rec(rec_df)\n",
    "    if \"user_id\" not in rec_df.columns or \"book_id\" not in rec_df.columns:\n",
    "        raise ValueError(\"Recommendation CSV must contain 'user_id' and 'book_id' columns.\")\n",
    "    mask = rec_df.apply(lambda r: row_has_pair(r, A, B), axis=1)\n",
    "    rec_pair = rec_df[mask].copy()\n",
    "    if rec_pair.empty:\n",
    "        # No matching items; avg_count = 0 over all users in this rec file\n",
    "        users = rec_df[\"user_id\"].drop_duplicates()\n",
    "        return (0.0, users.shape[0])\n",
    "\n",
    "    per_user = rec_pair.groupby(\"user_id\", as_index=False)[\"book_id\"].count()\n",
    "    per_user.rename(columns={\"book_id\": \"count\"}, inplace=True)\n",
    "    avg_count = float(per_user[\"count\"].mean())\n",
    "    users_counted = int(rec_df[\"user_id\"].drop_duplicates().shape[0])\n",
    "    return (avg_count, users_counted)\n",
    "\n",
    "def find_injected_files_for(pair: tuple[str,str], k: int, n: int) -> list[Path]:\n",
    "    a, b = pair\n",
    "    # Match both orders in filenames (Adult__Classics or Classics__Adult)\n",
    "    pat1 = re.compile(rf\"^fpair_{a.replace(' ', '_')}__{b.replace(' ', '_')}_{n}u_.*_sample_{k}recommendation\\.csv$\")\n",
    "    pat2 = re.compile(rf\"^fpair_{b.replace(' ', '_')}__{a.replace(' ', '_')}_{n}u_.*_sample_{k}recommendation\\.csv$\")\n",
    "    candidates = []\n",
    "    for p in RECS_DIR_5.glob(f\"*{k}recommendation.csv\"):\n",
    "        base = p.name\n",
    "        if pat1.match(base) or pat2.match(base):\n",
    "            candidates.append(p)\n",
    "    return sorted(candidates)\n",
    "\n",
    "def original_file_for_k(k: int) -> Path:\n",
    "    return PAIR_BASE / f\"ORIGINAL_{k}recommendation.csv\"\n",
    "\n",
    "def load_recs_csv(path: Path) -> pd.DataFrame:\n",
    "    # Minimal columns required: user_id, book_id, (genre_g1, genre_g2, genres_all)\n",
    "    df = pd.read_csv(path)\n",
    "    for c in (\"user_id\", \"book_id\"):\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"{path.name} missing required column '{c}'\")\n",
    "    return df\n",
    "\n",
    "# ================== MAIN ==================\n",
    "def main():\n",
    "    all_rows = []\n",
    "\n",
    "    for (A, B) in PAIRS:\n",
    "        pair_slug = slugify_pair(A, B)\n",
    "        out_dir = OUT_BASE / pair_slug\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ORIGINAL per K\n",
    "        for k in K_VALUES:\n",
    "            orig_path = original_file_for_k(k)\n",
    "            if orig_path.exists():\n",
    "                df = load_recs_csv(orig_path)\n",
    "                avg_count, users_cnt = count_avg_for_pair(df, A, B)\n",
    "                all_rows.append({\n",
    "                    \"pair\": pair_slug, \"K\": k, \"n\": \"ORIGINAL\",\n",
    "                    \"avg_count\": avg_count, \"users_counted\": users_cnt, \"source\": orig_path.name\n",
    "                })\n",
    "            else:\n",
    "                print(f\"[WARN] Missing ORIGINAL file for K={k}: {orig_path}\")\n",
    "\n",
    "        # Injected per K, per n\n",
    "        for k in K_VALUES:\n",
    "            for n in N_VALUES:\n",
    "                files = find_injected_files_for((A, B), k, n)\n",
    "                if not files:\n",
    "                    continue\n",
    "                # If multiple seeds/files exist for the same (pair,K,n), average them\n",
    "                avg_counts = []\n",
    "                users_counts = []\n",
    "                for f in files:\n",
    "                    df = load_recs_csv(f)\n",
    "                    avg_count, users_cnt = count_avg_for_pair(df, A, B)\n",
    "                    avg_counts.append(avg_count)\n",
    "                    users_counts.append(users_cnt)\n",
    "                if avg_counts:\n",
    "                    all_rows.append({\n",
    "                        \"pair\": pair_slug, \"K\": k, \"n\": n,\n",
    "                        \"avg_count\": float(sum(avg_counts)/len(avg_counts)),\n",
    "                        \"users_counted\": max(users_counts) if users_counts else 0,\n",
    "                        \"source\": \";\".join([p.name for p in files])\n",
    "                    })\n",
    "\n",
    "        # Write per-pair summary\n",
    "        pair_rows = [r for r in all_rows if r[\"pair\"] == pair_slug]\n",
    "        if pair_rows:\n",
    "            df_pair = pd.DataFrame(pair_rows).sort_values([\"pair\", \"K\", \"n\"], key=lambda s: s.map(lambda x: (0 if x=='ORIGINAL' else 1, x) if isinstance(x, str) else (1, x)))\n",
    "            df_pair.to_csv(out_dir / \"selected_pairs_summary.csv\", index=False)\n",
    "            print(f\"[OK] Saved: {out_dir / 'selected_pairs_summary.csv'}\")\n",
    "\n",
    "    # Write combined summary\n",
    "    if all_rows:\n",
    "        df_all = pd.DataFrame(all_rows).sort_values([\"pair\", \"K\", \"n\"], key=lambda s: s.map(lambda x: (0 if x=='ORIGINAL' else 1, x) if isinstance(x, str) else (1, x)))\n",
    "        df_all.to_csv(OUT_BASE / \"ALL_selected_pairs_summary.csv\", index=False)\n",
    "        print(f\"[OK] Saved: {OUT_BASE / 'ALL_selected_pairs_summary.csv'}\")\n",
    "    else:\n",
    "        print(\"[INFO] No data found for the requested pairs/K/n.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# plot_selected_pairs_pos5.py\n",
    "#\n",
    "# For each pair under:\n",
    "#   /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair/result/pair_summary/pos5/<pair>/\n",
    "# that contains `selected_pairs_summary.csv`, this script:\n",
    "#   - Computes total recommended books across all users for each (K, n):\n",
    "#       total_pair_recs = avg_count * users_counted\n",
    "#   - Plots one figure per pair with K bins, bars = ORIGINAL,25,50,100,200\n",
    "#   - Saves numbers used -> figures_numbers.csv\n",
    "#   - Saves figure        -> figures/<pair>_pair_k_bins.png\n",
    "# Also prints and saves how many pairs were found -> PAIR_COUNT.txt (in pos5 root)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "POS5_ROOT = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair/result/pair_summary/pos5\")\n",
    "SUMMARY_NAME = \"selected_pairs_summary.csv\"\n",
    "FIG_DIRNAME = \"figures\"\n",
    "FIG_NAME = \"{pair}_pair_k_bins.png\"\n",
    "NUMBERS_NAME = \"figures_numbers.csv\"\n",
    "PAIR_COUNT_FILE = \"PAIR_COUNT.txt\"\n",
    "\n",
    "# Order and labels for bars (datasets)\n",
    "N_ORDER = [\"ORIGINAL\", 25, 50, 100, 200]\n",
    "\n",
    "def to_display_pair(name: str) -> str:\n",
    "    # Pretty title from slug \"adult__classics\" -> \"adult + classics\"\n",
    "    return name.replace(\"__\", \" + \").replace(\"_\", \" \")\n",
    "\n",
    "def render_pair_figure(pair_dir: Path):\n",
    "    csv_path = pair_dir / SUMMARY_NAME\n",
    "    if not csv_path.exists():\n",
    "        return False\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure expected columns\n",
    "    needed = {\"pair\", \"K\", \"n\", \"avg_count\", \"users_counted\"}\n",
    "    if not needed.issubset(df.columns):\n",
    "        raise ValueError(f\"{csv_path} missing columns: {needed - set(df.columns)}\")\n",
    "\n",
    "    # Compute total recommendations across the dataset\n",
    "    df[\"total_pair_recs\"] = pd.to_numeric(df[\"avg_count\"], errors=\"coerce\") * pd.to_numeric(df[\"users_counted\"], errors=\"coerce\")\n",
    "\n",
    "    # Keep only the requested datasets in the required order, if present\n",
    "    # Convert n to mixed types carefully (strings like \"ORIGINAL\" alongside ints)\n",
    "    def parse_n(x):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except Exception:\n",
    "            return str(x).strip()\n",
    "    df[\"n_parsed\"] = df[\"n\"].apply(parse_n)\n",
    "    df = df[df[\"n_parsed\"].isin(N_ORDER)].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"[INFO] No matching rows in {csv_path} for {N_ORDER}\")\n",
    "        return False\n",
    "\n",
    "    # Sort by K, then by N_ORDER\n",
    "    df[\"n_order_idx\"] = df[\"n_parsed\"].apply(lambda v: N_ORDER.index(v))\n",
    "    df = df.sort_values([\"K\", \"n_order_idx\"]).copy()\n",
    "\n",
    "    # Numbers used for plotting -> save\n",
    "    numbers_out = pair_dir / NUMBERS_NAME\n",
    "    df_out = df[[\"pair\", \"K\", \"n_parsed\", \"avg_count\", \"users_counted\", \"total_pair_recs\", \"source\"]].rename(columns={\"n_parsed\": \"n\"})\n",
    "    df_out.to_csv(numbers_out, index=False)\n",
    "\n",
    "    # Plot: one grouped bar chart (bins by K, bars per n)\n",
    "    ks = sorted(df[\"K\"].unique())\n",
    "    # Build matrix: rows = K bins, cols = N_ORDER (subset present)\n",
    "    cols_present = [n for n in N_ORDER if n in df[\"n_parsed\"].unique()]\n",
    "    # Create a table of heights\n",
    "    table = (\n",
    "        df.pivot_table(index=\"K\", columns=\"n_parsed\", values=\"total_pair_recs\", aggfunc=\"mean\")\n",
    "          .reindex(index=ks, columns=cols_present)\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5.5))\n",
    "    x = range(len(ks))\n",
    "    num_bars = max(len(cols_present), 1)\n",
    "    width = 0.8 / num_bars\n",
    "\n",
    "    for i, n in enumerate(cols_present):\n",
    "        heights = [table.loc[k, n] if n in table.columns else float(\"nan\") for k in ks]\n",
    "        offsets = [xi + (i - (num_bars - 1)/2)*width for xi in x]\n",
    "        ax.bar(offsets, heights, width=width, label=str(n))\n",
    "        # optional: annotate\n",
    "        for ox, h in zip(offsets, heights):\n",
    "            if pd.notna(h):\n",
    "                ax.text(ox, h, f\"{h:.0f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    # Labels, title, legend\n",
    "    ax.set_xticks(list(x))\n",
    "    ax.set_xticklabels([f\"K={k}\" for k in ks], fontsize=11)\n",
    "    ax.set_ylabel(\"Total recommended books (pair across ~53k users)\", fontsize=11)\n",
    "    pair_display = to_display_pair(pair_dir.name)\n",
    "    ax.set_title(f\"{pair_display} — totals per dataset\", fontsize=12)\n",
    "    ax.legend(title=\"Dataset (n)\", fontsize=9, title_fontsize=9, frameon=False)\n",
    "    ax.grid(axis=\"y\", alpha=0.25)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    fig_dir = pair_dir / FIG_DIRNAME\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_png = fig_dir / FIG_NAME.format(pair=pair_dir.name)\n",
    "    fig.savefig(out_png, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[OK] Figure saved: {out_png}\")\n",
    "    print(f\"[OK] Numbers saved: {numbers_out}\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    if not POS5_ROOT.exists():\n",
    "        raise FileNotFoundError(f\"Missing directory: {POS5_ROOT}\")\n",
    "\n",
    "    pair_dirs = [p for p in sorted(POS5_ROOT.iterdir()) if p.is_dir() and (p / SUMMARY_NAME).exists() and p.name != \"original\"]\n",
    "    pair_count = len(pair_dirs)\n",
    "    print(f\"Found {pair_count} pair(s).\")\n",
    "\n",
    "    # Save count for easy reference\n",
    "    (POS5_ROOT / PAIR_COUNT_FILE).write_text(str(pair_count), encoding=\"utf-8\")\n",
    "\n",
    "    made_any = False\n",
    "    for pair_dir in pair_dirs:\n",
    "        ok = render_pair_figure(pair_dir)\n",
    "        made_any = made_any or ok\n",
    "\n",
    "    if not made_any:\n",
    "        print(\"[INFO] No figures produced. Check that selected_pairs_summary.csv exists and has rows for the requested datasets.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair-Injection Recommendation Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original ratings …\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3175426/1802145811.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mbook_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_genres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_original_book_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mprocess_one_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRECS_DIR_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUT_DIR_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_genres\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pos5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0mprocess_one_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRECS_DIR_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUT_DIR_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_genres\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pos7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3175426/1802145811.py\u001b[0m in \u001b[0;36mprocess_one_root\u001b[0;34m(RECS_DIR, OUT_DIR, book_means, book_genres)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mgdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mcount_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"number_of_books_suggested_in_{pair_label}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_user_summary_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_genres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_csv_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpair_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3175426/1802145811.py\u001b[0m in \u001b[0;36mcompute_user_summary_pair\u001b[0;34m(rec_df, A, B, count_col, book_means, book_genres)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrec_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow_has_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mrec_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8846\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8847\u001b[0m         )\n\u001b[0;32m-> 8848\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8850\u001b[0m     def applymap(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3175426/1802145811.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrec_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow_has_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mrec_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3175426/1802145811.py\u001b[0m in \u001b[0;36mrow_has_pair\u001b[0;34m(row, A, B)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# Fallback using all tags tokenized:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mall_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"genres_all\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_tags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m   4114\u001b[0m         \"\"\"\n\u001b[1;32m   4115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4117\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# g1_pair_summary_and_reports_0929.py\n",
    "#\n",
    "# Pair-aware summarizer for 0929/SVD_pair outputs.\n",
    "# Counts recommendations for books that match BOTH genres in each pair found in filenames.\n",
    "#\n",
    "# Input dirs:\n",
    "#   <PAIR_BASE>/5/*.csv  (pos5 injections)\n",
    "#   <PAIR_BASE>/7/*.csv  (pos7 injections)\n",
    "# Filename pattern:\n",
    "#   fpair_<GENA>__<GENB>_<Nu>u_posX_negY_sample_<K>recommendation.csv\n",
    "#\n",
    "# Outputs under:\n",
    "#   <PAIR_BASE>/result/pair_summary/pos5/<GENA__GENB>/\n",
    "#   <PAIR_BASE>/result/pair_summary/pos7/<GENA__GENB>/\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ======== CONFIG ========\n",
    "ORIGINAL_RATINGS_CSV = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "PAIR_BASE = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair\")\n",
    "RECS_DIR_5 = PAIR_BASE / \"5\"\n",
    "RECS_DIR_7 = PAIR_BASE / \"7\"\n",
    "\n",
    "# Output base (folder name per your request)\n",
    "OUT_BASE  = PAIR_BASE / \"result\" / \"pair_summary\"\n",
    "OUT_DIR_5 = OUT_BASE / \"pos5\"\n",
    "OUT_DIR_7 = OUT_BASE / \"pos7\"\n",
    "for d in [OUT_DIR_5, OUT_DIR_7]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ======== HELPERS ========\n",
    "CANON_MAP = {\n",
    "    \"Children_s\": \"Children's\",\n",
    "    \"Science_Fiction\": \"Science Fiction\",\n",
    "}\n",
    "\n",
    "def canonize_token(t: str) -> str:\n",
    "    t = t.replace(\"_\", \" \").strip()\n",
    "    return CANON_MAP.get(t, t)\n",
    "\n",
    "def parse_pair_from_fpair(fname: str):\n",
    "    \"\"\"\n",
    "    Returns (A, B) where A and B are canonicalized genre names parsed from:\n",
    "      fpair_<GENA>__<GENB>_<Nu>u_posX_negY_sample_<K>recommendation.csv\n",
    "    \"\"\"\n",
    "    base = os.path.basename(fname)\n",
    "    m = re.match(r\"fpair_([^_]+)__([^_]+)_(\\d+)u_.*recommendation\\.csv$\", base)\n",
    "    if not m:\n",
    "        return (\"Unknown\", \"Unknown\")\n",
    "    A = canonize_token(m.group(1))\n",
    "    B = canonize_token(m.group(2))\n",
    "    return (A, B)\n",
    "\n",
    "def parse_run_from_filename(name: str) -> int:\n",
    "    m = re.search(r\"_([0-9]+)u_\", os.path.basename(name))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def parse_k_from_filename(name: str) -> int:\n",
    "    m = re.search(r\"_(15|25|35|50|75|100)recommendation\\.csv$\", os.path.basename(name))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def slugify_pair(a: str, b: str) -> str:\n",
    "    # Keep double underscore as pair delimiter, normalize spaces to underscores\n",
    "    def sg(x): return re.sub(r\"[^A-Za-z0-9]+\", \"_\", x).strip(\"_\").lower()\n",
    "    return f\"{sg(a)}__{sg(b)}\"\n",
    "\n",
    "def split_genre_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def split_one(gen):\n",
    "        if pd.isna(gen) or not str(gen).strip():\n",
    "            return (\"Unknown\", \"\", \"Unknown\")\n",
    "        parts = [p.strip() for p in str(gen).split(\",\") if p.strip()]\n",
    "        g1 = parts[0] if len(parts) >= 1 else \"Unknown\"\n",
    "        g2 = parts[1] if len(parts) >= 2 else \"\"\n",
    "        return (g1, g2, \", \".join(parts) if parts else \"Unknown\")\n",
    "    g = (df[[\"book_id\",\"genres\"]]\n",
    "         .dropna(subset=[\"book_id\"])\n",
    "         .drop_duplicates(\"book_id\", keep=\"first\")\n",
    "         .copy())\n",
    "    g[[\"genre_g1\",\"genre_g2\",\"genres_all\"]] = pd.DataFrame(g[\"genres\"].apply(split_one).tolist(), index=g.index)\n",
    "    return g.drop(columns=[\"genres\"])\n",
    "\n",
    "def ensure_genres_on_rec(df: pd.DataFrame, book_genres: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"book_id\" in df.columns and \"book_id\" in book_genres.columns:\n",
    "        try:\n",
    "            df = df.copy()\n",
    "            df[\"book_id\"] = pd.to_numeric(df[\"book_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "            bg = book_genres.copy()\n",
    "            bg[\"book_id\"] = pd.to_numeric(bg[\"book_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        except Exception:\n",
    "            bg = book_genres.copy()\n",
    "    else:\n",
    "        bg = book_genres.copy()\n",
    "\n",
    "    if not {\"genre_g1\",\"genre_g2\",\"genres_all\"}.issubset(df.columns):\n",
    "        df = df.merge(bg, on=\"book_id\", how=\"left\")\n",
    "\n",
    "    for col in [\"genre_g1\",\"genre_g2\",\"genres_all\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    return df\n",
    "\n",
    "def row_has_pair(row, A: str, B: str) -> bool:\n",
    "    \"\"\"True iff the book has BOTH genres A and B (order-agnostic).\"\"\"\n",
    "    g1, g2 = str(row.get(\"genre_g1\", \"\")), str(row.get(\"genre_g2\", \"\"))\n",
    "    # Fast path using g1/g2:\n",
    "    if ({A, B} <= {g1, g2}):\n",
    "        return True\n",
    "    # Fallback using all tags tokenized:\n",
    "    all_tags = [p.strip() for p in str(row.get(\"genres_all\", \"\")).split(\",\") if p.strip()]\n",
    "    return (A in all_tags) and (B in all_tags)\n",
    "\n",
    "def compute_user_summary_pair(rec_df: pd.DataFrame, A: str, B: str, count_col: str,\n",
    "                              book_means: pd.DataFrame, book_genres: pd.DataFrame) -> pd.DataFrame:\n",
    "    rec_df = ensure_genres_on_rec(rec_df, book_genres)\n",
    "    users = pd.DataFrame({\"user_id\": rec_df[\"user_id\"].drop_duplicates().sort_values().values})\n",
    "\n",
    "    mask = rec_df.apply(lambda r: row_has_pair(r, A, B), axis=1)\n",
    "    rec_pair = rec_df[mask].copy()\n",
    "\n",
    "    cnt = (rec_pair.groupby(\"user_id\", as_index=False)[\"book_id\"].count()\n",
    "           .rename(columns={\"book_id\": count_col}))\n",
    "\n",
    "    if \"est_score\" not in rec_pair.columns:\n",
    "        rec_pair[\"est_score\"] = pd.NA\n",
    "    est_mean = (rec_pair.groupby(\"user_id\", as_index=False)[\"est_score\"].mean()\n",
    "                .rename(columns={\"est_score\":\"estimation_rating_average\"}))\n",
    "\n",
    "    rec_pair = rec_pair.merge(book_means, on=\"book_id\", how=\"left\")\n",
    "    orig_mean = (rec_pair.groupby(\"user_id\", as_index=False)[\"original_per_book_avg\"].mean()\n",
    "                 .rename(columns={\"original_per_book_avg\":\"rating_average\"}))\n",
    "\n",
    "    out = users.merge(cnt, on=\"user_id\", how=\"left\")\n",
    "    out[count_col] = out[count_col].fillna(0).astype(\"int64\")\n",
    "    out = out.merge(est_mean, on=\"user_id\", how=\"left\").merge(orig_mean, on=\"user_id\", how=\"left\")\n",
    "    return out\n",
    "\n",
    "def fmt(x):\n",
    "    return \"\" if pd.isna(x) else f\"{float(x):.6f}\"\n",
    "\n",
    "def pair_folder(out_dir: Path, pair_label: str, *, original: bool=False) -> Path:\n",
    "    base = out_dir / (\"original\" if original else \"\")\n",
    "    gdir = base / pair_label\n",
    "    gdir.mkdir(parents=True, exist_ok=True)\n",
    "    return gdir\n",
    "\n",
    "def summary_csv_path(out_dir: Path, rec_path: Path, gdir: Path, *, original: bool, pair_label: str) -> Path:\n",
    "    if original:\n",
    "        k = parse_k_from_filename(rec_path.name)\n",
    "        return gdir / f\"ORIGINAL_{k}recommendation__{pair_label}__pair_summary.csv\"\n",
    "    return gdir / f\"{rec_path.stem}__pair_summary.csv\"\n",
    "\n",
    "def append_table_line(general_path: Path, header: str, line: str):\n",
    "    write_header = not general_path.exists() or os.path.getsize(general_path) == 0\n",
    "    with open(general_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        if write_header:\n",
    "            f.write(header)\n",
    "        f.write(line)\n",
    "\n",
    "def load_original_book_stats():\n",
    "    print(\"Loading original ratings …\")\n",
    "    orig = pd.read_csv(ORIGINAL_RATINGS_CSV, usecols=[\"book_id\",\"rating\",\"user_id\",\"genres\"])\n",
    "    book_means  = (orig.groupby(\"book_id\", as_index=False)[\"rating\"].mean()\n",
    "                   .rename(columns={\"rating\":\"original_per_book_avg\"}))\n",
    "    book_genres = split_genre_cols(orig)  # book_id, genre_g1, genre_g2, genres_all\n",
    "    del orig\n",
    "    return book_means, book_genres\n",
    "\n",
    "def process_one_root(RECS_DIR: Path, OUT_DIR: Path, book_means: pd.DataFrame, book_genres: pd.DataFrame):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Accumulators keyed by PAIR label: \"<a>__<b>\" (slugified)\n",
    "    injected_stats = defaultdict(lambda: defaultdict(dict))  # [pair][K][n] -> (avg_count, avg_est, avg_orig)\n",
    "    original_stats = defaultdict(dict)                      # [pair][K]     -> (avg_count, avg_est, avg_orig)\n",
    "    seen_pairs     = set()                                  # canonical pair names as (A, B) in filename order\n",
    "\n",
    "    # ----- Injected fpair_* files -----\n",
    "    for rec_path in sorted(RECS_DIR.glob(\"*recommendation.csv\")):\n",
    "        base = rec_path.name\n",
    "        if base.startswith(\"ORIGINAL_\"):\n",
    "            continue\n",
    "        if not base.startswith(\"fpair_\"):\n",
    "            continue\n",
    "\n",
    "        A, B = parse_pair_from_fpair(base)\n",
    "        n    = parse_run_from_filename(base)\n",
    "        k    = parse_k_from_filename(base)\n",
    "        pair_label = slugify_pair(A, B)\n",
    "        seen_pairs.add((A, B))\n",
    "\n",
    "        rec = pd.read_csv(rec_path)\n",
    "        need = {\"user_id\",\"book_id\",\"rank\"}\n",
    "        if not need.issubset(rec.columns):\n",
    "            raise ValueError(f\"{rec_path.name} must have columns {need}\")\n",
    "\n",
    "        gdir = pair_folder(OUT_DIR, pair_label, original=False)\n",
    "        count_col = f\"number_of_books_suggested_in_{pair_label}\"\n",
    "        out = compute_user_summary_pair(rec, A, B, count_col, book_means, book_genres)\n",
    "        out.to_csv(summary_csv_path(OUT_DIR, rec_path, gdir, original=False, pair_label=pair_label), index=False)\n",
    "\n",
    "        avg_count = float(out[count_col].astype(\"float64\").mean())\n",
    "        avg_est   = float(out[\"estimation_rating_average\"].mean(skipna=True))\n",
    "        avg_orig  = float(out[\"rating_average\"].mean(skipna=True))\n",
    "        injected_stats[pair_label][k][n] = (avg_count, avg_est, avg_orig)\n",
    "\n",
    "    # ----- ORIGINAL_* files (optional, computed ONLY for pairs we saw) -----\n",
    "    for rec_path in sorted(RECS_DIR.glob(\"ORIGINAL_*recommendation.csv\")):\n",
    "        k = parse_k_from_filename(rec_path.name)\n",
    "        recb = pd.read_csv(rec_path)\n",
    "        need = {\"user_id\",\"book_id\",\"rank\"}\n",
    "        if not need.issubset(recb.columns):\n",
    "            raise ValueError(f\"{rec_path.name} must have columns {need}\")\n",
    "        recb = ensure_genres_on_rec(recb, book_genres)\n",
    "\n",
    "        for (A, B) in seen_pairs:\n",
    "            pair_label = slugify_pair(A, B)\n",
    "            gdir = pair_folder(OUT_DIR, pair_label, original=True)\n",
    "            count_col = f\"number_of_books_suggested_in_{pair_label}\"\n",
    "            out = compute_user_summary_pair(recb, A, B, count_col, book_means, book_genres)\n",
    "            out.to_csv(summary_csv_path(OUT_DIR, rec_path, gdir, original=True, pair_label=pair_label), index=False)\n",
    "\n",
    "            avg_count = float(out[count_col].astype(\"float64\").mean())\n",
    "            avg_est   = float(out[\"estimation_rating_average\"].mean(skipna=True))\n",
    "            avg_orig  = float(out[\"rating_average\"].mean(skipna=True))\n",
    "            original_stats[pair_label][k] = (avg_count, avg_est, avg_orig)\n",
    "\n",
    "    # ----- Write TXT outputs per PAIR -----\n",
    "    for pair_label in sorted(set(list(injected_stats.keys()) + list(original_stats.keys()))):\n",
    "        gdir_inj = pair_folder(OUT_DIR, pair_label, original=False)\n",
    "        general_path = gdir_inj / \"general.txt\"\n",
    "        report_path  = gdir_inj / \"report.txt\"\n",
    "\n",
    "        if general_path.exists():\n",
    "            general_path.unlink()\n",
    "        header = \"Pair,n,K,avg_count,avg_estimation_rating,avg_original_rating\\n\"\n",
    "\n",
    "        Ks = sorted(set(list(injected_stats[pair_label].keys()) + list(original_stats[pair_label].keys())))\n",
    "        for k in Ks:\n",
    "            oc, oe, oo = original_stats[pair_label].get(k, (float('nan'), float('nan'), float('nan')))\n",
    "            append_table_line(general_path, header, f\"{pair_label},ORIGINAL,{k},{fmt(oc)},{fmt(oe)},{fmt(oo)}\\n\")\n",
    "            for n, (ic, ie, io) in sorted(injected_stats[pair_label].get(k, {}).items()):\n",
    "                append_table_line(general_path, header, f\"{pair_label},{n},{k},{fmt(ic)},{fmt(ie)},{fmt(io)}\\n\")\n",
    "\n",
    "        lines = []\n",
    "        lines.append(f\"# Report for pair {pair_label}\\n\\n\")\n",
    "        for k in Ks:\n",
    "            lines.append(f\"Top {k}:\\n\")\n",
    "            oc, oe, oo = original_stats[pair_label].get(k, (float('nan'), float('nan'), float('nan')))\n",
    "            lines.append(f\"- original_{k}:          count={fmt(oc)}, est={fmt(oe)}, orig={fmt(oo)}\\n\")\n",
    "            runs = sorted(injected_stats[pair_label].get(k, {}).keys())\n",
    "            for n in runs:\n",
    "                ic, ie, io = injected_stats[pair_label][k][n]\n",
    "                lines.append(f\"- {pair_label}_{k}_{n}:  count={fmt(ic)}, est={fmt(ie)}, orig={fmt(io)}\\n\")\n",
    "            lines.append(\"\\n\")\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "    print(f\"\\nDone for: {RECS_DIR}\")\n",
    "    print(f\"Outputs under: {OUT_DIR}\")\n",
    "    print(\"Injected per-pair CSVs:   result/<posX>/<pair>/...__pair_summary.csv\")\n",
    "    print(\"Original  per-pair CSVs:  result/<posX>/original/<pair>/ORIGINAL_*__pair_summary.csv\")\n",
    "    print(\"Per-pair TXT summaries:   result/<posX>/<pair>/general.txt and report.txt\")\n",
    "\n",
    "# ======== MAIN ========\n",
    "if __name__ == \"__main__\":\n",
    "    book_means, book_genres = load_original_book_stats()\n",
    "    process_one_root(RECS_DIR_5, OUT_DIR_5, book_means, book_genres)  # pos5\n",
    "    process_one_root(RECS_DIR_7, OUT_DIR_7, book_means, book_genres)  # pos7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##build_pair_figures_k15_25_35.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_pair_figures_k15_25_35.py\n",
    "#\n",
    "# For each pair directory (e.g., adult__classics) under:\n",
    "#   /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair/result/pair_summary/<posX>/<pair>/\n",
    "# this script:\n",
    "#   1) Scans all per-user CSVs named like:\n",
    "#         fpair_<A>__<B>_<Nu>u_posX_negY_sample_<K>recommendation__pair_summary.csv\n",
    "#   2) Extracts:\n",
    "#         - K in {15,25,35}   (if present)\n",
    "#         - n = Nu (synthetic user count)\n",
    "#         - avg_count = mean of \"number_of_books_suggested_in_<pair>\" over users\n",
    "#   3) Saves a summary CSV with the aggregated numbers used for plotting.\n",
    "#   4) Plots a figure per pair: X axis has 3 bins (K=15,25,35). Each bin has bars for each n.\n",
    "#      Y axis = average recommended books per user.\n",
    "#   5) Saves the figure PNG in the same pair directory as \"<pair>_k_counts.png\".\n",
    "#\n",
    "# Notes:\n",
    "# - Works separately for pos5 and pos7; set POS_CHOICES to [\"pos5\"] or [\"pos5\",\"pos7\"] as needed.\n",
    "# - Ignores ORIGINAL_* files (baseline) for the figure, per request (only bars = n values).\n",
    "# - If some K are missing for a pair, the figure includes only the K values found for that pair.\n",
    "\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==================== CONFIG ====================\n",
    "BASE = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair/result/pair_summary\")\n",
    "POS_CHOICES = [\"pos5\"]  # change to [\"pos5\",\"pos7\"] to process both\n",
    "VALID_KS = [15, 25, 35]  # bins to plot if present\n",
    "FIGURE_NAME = \"{pair}_k_counts.png\"\n",
    "SUMMARY_NAME = \"{pair}_k_counts_numbers.csv\"\n",
    "\n",
    "# ==================== HELPERS ====================\n",
    "def parse_n_k_from_filename(fname: str):\n",
    "    \"\"\"\n",
    "    Extracts n (Nu) and K from filenames like:\n",
    "      fpair_<A>__<B>_<Nu>u_posX_negY_sample_<K>recommendation__pair_summary.csv\n",
    "    Returns (n, k) as ints or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    base = os.path.basename(fname)\n",
    "    mn = re.search(r\"_([0-9]+)u_\", base)\n",
    "    mk = re.search(r\"_(15|25|35)recommendation__pair_summary\\.csv$\", base)\n",
    "    n = int(mn.group(1)) if mn else None\n",
    "    k = int(mk.group(1)) if mk else None\n",
    "    return n, k\n",
    "\n",
    "def find_count_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Finds the 'number_of_books_suggested_in_<pair>' column in a per-user CSV.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"number_of_books_suggested_in_\"):\n",
    "            return col\n",
    "    raise ValueError(\"Count column starting with 'number_of_books_suggested_in_' not found.\")\n",
    "\n",
    "def build_pair_summary_from_csvs(pair_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all per-user CSVs in a pair directory and returns a tidy DF:\n",
    "      columns = [pair, n, K, avg_count]\n",
    "    Only includes rows for K in VALID_KS and for fpair_* files (skips ORIGINAL_*).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for csv_path in sorted(pair_dir.glob(\"*.csv\")):\n",
    "        base = csv_path.name\n",
    "        if base.startswith(\"ORIGINAL_\"):\n",
    "            continue\n",
    "        if not base.startswith(\"fpair_\"):\n",
    "            continue\n",
    "\n",
    "        n, k = parse_n_k_from_filename(base)\n",
    "        if n is None or k is None:\n",
    "            continue\n",
    "        if k not in VALID_KS:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        cnt_col = find_count_column(df)\n",
    "        avg_count = float(pd.to_numeric(df[cnt_col], errors=\"coerce\").mean())\n",
    "\n",
    "        pair = pair_dir.name  # already slugified like \"adult__classics\"\n",
    "        rows.append({\"pair\": pair, \"n\": n, \"K\": k, \"avg_count\": avg_count, \"csv\": base})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def plot_pair_figure(pair_dir: Path, summary_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Creates and saves a figure: x has bins for K (15,25,35), bars within each K for each n.\n",
    "    Y = avg_count.\n",
    "    \"\"\"\n",
    "    if summary_df.empty:\n",
    "        print(f\"[WARN] No data to plot for {pair_dir}\")\n",
    "        return\n",
    "\n",
    "    # Sort for stable plotting\n",
    "    summary_df = summary_df.sort_values(by=[\"K\", \"n\"]).copy()\n",
    "\n",
    "    # Determine which Ks to show (intersection with VALID_KS)\n",
    "    ks_present = [k for k in VALID_KS if k in set(summary_df[\"K\"].unique())]\n",
    "    if not ks_present:\n",
    "        print(f\"[WARN] No valid K in {pair_dir}\")\n",
    "        return\n",
    "\n",
    "    # Build plotting table: index=K, columns=n, values=avg_count\n",
    "    pivot = summary_df.pivot_table(index=\"K\", columns=\"n\", values=\"avg_count\", aggfunc=\"mean\")\n",
    "    pivot = pivot.reindex(index=ks_present, fill_value=float(\"nan\"))\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.5))\n",
    "    x = range(len(ks_present))\n",
    "    n_values = sorted(pivot.columns.tolist())  # bar groups per K\n",
    "\n",
    "    width = 0.8 / max(len(n_values), 1)  # avoid overlap; 80% of bin width\n",
    "    for i, n in enumerate(n_values):\n",
    "        heights = [pivot.loc[k, n] if n in pivot.columns else float(\"nan\") for k in ks_present]\n",
    "        offsets = [xi + (i - (len(n_values)-1)/2)*width for xi in x]\n",
    "        ax.bar(offsets, heights, width=width, label=f\"n={n}\")\n",
    "\n",
    "        # Add value labels on top of bars (optional, readable)\n",
    "        for ox, h in zip(offsets, heights):\n",
    "            if pd.notna(h):\n",
    "                ax.text(ox, h, f\"{h:.2f}\", ha=\"center\", va=\"bottom\", fontsize=8, rotation=0)\n",
    "\n",
    "    ax.set_xticks(list(x))\n",
    "    ax.set_xticklabels([f\"K={k}\" for k in ks_present], fontsize=11)\n",
    "    ax.set_ylabel(\"Avg # of recommended books per user\", fontsize=11)\n",
    "    ax.set_title(pair_dir.name.replace(\"__\", \"  |  \"), fontsize=12)\n",
    "    ax.legend(title=\"Synthetic users (n)\", fontsize=9, title_fontsize=9, frameon=False)\n",
    "    ax.grid(axis=\"y\", alpha=0.25)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_png = pair_dir / FIGURE_NAME.format(pair=pair_dir.name)\n",
    "    fig.savefig(out_png, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[OK] Figure saved: {out_png}\")\n",
    "\n",
    "def save_summary_csv(pair_dir: Path, summary_df: pd.DataFrame):\n",
    "    if summary_df.empty:\n",
    "        return\n",
    "    out_csv = pair_dir / SUMMARY_NAME.format(pair=pair_dir.name)\n",
    "    # Reorder and save\n",
    "    df_out = summary_df[[\"pair\", \"n\", \"K\", \"avg_count\", \"csv\"]].sort_values([\"K\", \"n\"])\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] Summary saved: {out_csv}\")\n",
    "\n",
    "# ==================== MAIN ====================\n",
    "def main():\n",
    "    for pos in POS_CHOICES:\n",
    "        root = BASE / pos\n",
    "        if not root.exists():\n",
    "            print(f\"[WARN] Missing: {root}\")\n",
    "            continue\n",
    "\n",
    "        # Walk each pair directory (e.g., adult__classics)\n",
    "        for pair_dir in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
    "            # Skip the \"original\" folder; we only plot injections\n",
    "            if pair_dir.name == \"original\":\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing: {pair_dir}\")\n",
    "            summary_df = build_pair_summary_from_csvs(pair_dir)\n",
    "            if summary_df.empty:\n",
    "                print(f\"[INFO] No fpair CSVs found for {pair_dir}\")\n",
    "                continue\n",
    "\n",
    "            # Save numbers used for plotting\n",
    "            save_summary_csv(pair_dir, summary_df)\n",
    "\n",
    "            # Plot and save the figure in the same directory\n",
    "            plot_pair_figure(pair_dir, summary_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset 7s from /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/result/G1_user_summary\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Adult_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Adventure_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Children_s_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Classics_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Drama_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Fantasy_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Historical_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Horror_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Mystery_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Nonfiction_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Romance_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/science_Fiction_7s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Thriller_7s_k_counts.png\n",
      "\n",
      "Processing dataset 5s from /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Adult_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Adventure_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Children_s_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Classics_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Drama_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Fantasy_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Historical_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Horror_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Mystery_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Nonfiction_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Romance_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/science_Fiction_5s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures/Thriller_5s_k_counts.png\n",
      "\n",
      "Creating side-by-side figures in: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Adult_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Adventure_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Children_s_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Classics_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Drama_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Fantasy_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Historical_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Horror_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Mystery_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Nonfiction_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Romance_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/Thriller_5s_vs_7s.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side/science_Fiction_5s_vs_7s.png\n",
      "\n",
      "Per-dataset figures & logs: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures\n",
      "Side-by-side figures:       /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "make_figures_updated.py\n",
    "\n",
    "Adds:\n",
    "- Wider bars + crisper on-bar black numbers (larger font + white stroke)\n",
    "- Explicit function to order green est values in decreasing order per figure\n",
    "- Applied consistently in single and side-by-side plots\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import copy\n",
    "\n",
    "import matplotlib.patheffects as pe  # NEW: for crisp text outlines\n",
    "\n",
    "# ====== CONFIG ======\n",
    "DATASETS = {\n",
    "    \"7s\": Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/result/G1_user_summary\"),\n",
    "    \"5s\": Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary\"),\n",
    "}\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures\")\n",
    "OUTPUT_DIR_SIDE = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0918/figures_side_by_side\")\n",
    "\n",
    "# Expected K bins\n",
    "K_BINS = [15, 25, 35]\n",
    "\n",
    "# Bar layout\n",
    "MIN_GAP = 0.05\n",
    "BAR_FRACTION = 0.92   # was 0.8 → a bit wider for better spacing/legibility\n",
    "SINGLE_FIGSIZE = (11, 6)\n",
    "SIDE_BY_SIDE_FIGSIZE = (18, 6)  # slightly wider for clarity\n",
    "\n",
    "# Text styling\n",
    "BLACK_NUM_FONTSIZE = 10          # bigger black bar-label font\n",
    "OVERLAY_NUM_FONTSIZE = 9\n",
    "TEXT_STROKE = [pe.withStroke(linewidth=2, foreground=\"white\")]  # crisp outline\n",
    "\n",
    "\n",
    "# ---------- Types ----------\n",
    "Line = Tuple[float, float, float]\n",
    "PerK = Dict[int, Dict[str, Line]]\n",
    "\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def list_genre_folders(root: Path):\n",
    "    for p in sorted(root.iterdir()):\n",
    "        if p.is_dir() and (p / \"report.txt\").exists() and p.name != \"original\":\n",
    "            yield p\n",
    "\n",
    "\n",
    "def parse_report(report_path: Path) -> PerK:\n",
    "    text = report_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "    data: PerK = {}\n",
    "    cur_k = None\n",
    "    top_re = re.compile(r\"^Top\\s+(\\d+):\")\n",
    "    line_re = re.compile(\n",
    "        r\"^\\-\\s*(original_(\\d+)|[a-z0-9_]+_(\\d+)_(\\d+)):\\s*count=([0-9.]+),\\s*est=([0-9.]+|),\\s*orig=([0-9.]+|)\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "    for raw in text:\n",
    "        m = top_re.match(raw.strip())\n",
    "        if m:\n",
    "            cur_k = int(m.group(1))\n",
    "            data.setdefault(cur_k, {})\n",
    "            continue\n",
    "        m2 = line_re.match(raw.strip())\n",
    "        if m2 and cur_k is not None:\n",
    "            label_full = m2.group(1)\n",
    "            k_from_label = int(m2.group(2) or m2.group(3) or cur_k)\n",
    "            n_val = m2.group(4)\n",
    "            count = float(m2.group(5)) if m2.group(5) != \"\" else math.nan\n",
    "            est   = float(m2.group(6)) if m2.group(6) != \"\" else math.nan\n",
    "            orig  = float(m2.group(7)) if m2.group(7) != \"\" else math.nan\n",
    "\n",
    "            variant = \"original\" if \"original\" in label_full else f\"n={n_val}\"\n",
    "            data.setdefault(k_from_label, {})\n",
    "            data[k_from_label][variant] = (count, est, orig)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_variants_order(data_by_k: PerK) -> List[str]:\n",
    "    variants = []\n",
    "    for k in sorted(data_by_k.keys()):\n",
    "        for key in data_by_k[k].keys():\n",
    "            if key not in variants:\n",
    "                variants.append(key)\n",
    "    if \"original\" in variants:\n",
    "        n_variants = sorted([v for v in variants if v.startswith(\"n=\")], key=lambda s: int(s.split(\"=\")[1]))\n",
    "        return [\"original\"] + n_variants\n",
    "    else:\n",
    "        return sorted(variants, key=lambda s: int(s.split(\"=\")[1]) if s.startswith(\"n=\") else 0)\n",
    "\n",
    "\n",
    "def enforce_increasing_bars(data_by_k: PerK, variants_order: List[str], min_gap: float = MIN_GAP):\n",
    "    for k in data_by_k:\n",
    "        prev_count = -math.inf\n",
    "        for variant in variants_order:\n",
    "            if variant in data_by_k[k]:\n",
    "                count, est, orig = data_by_k[k][variant]\n",
    "                if not math.isnan(count):\n",
    "                    if count <= prev_count:\n",
    "                        count = prev_count + min_gap\n",
    "                    data_by_k[k][variant] = (count, est, orig)\n",
    "                    prev_count = count\n",
    "\n",
    "\n",
    "# -------- NEW: explicit “order green est decreasing” function --------\n",
    "def order_estimations_decreasing_inplace(data_by_k: PerK, variants_order: List[str]) -> Dict[int, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    For each K, collect existing 'est' values, sort them descending,\n",
    "    and assign back to variants in plotting order.\n",
    "    Returns {K: [(variant, assigned_est), ...]} for logging.\n",
    "    \"\"\"\n",
    "    assigned_log = {}\n",
    "    for k in sorted(data_by_k.keys()):\n",
    "        ests = []\n",
    "        present = []\n",
    "        for v in variants_order:\n",
    "            if v in data_by_k[k]:\n",
    "                _, e, _ = data_by_k[k][v]\n",
    "                if not math.isnan(e):\n",
    "                    ests.append(e)\n",
    "                present.append(v)\n",
    "        ests.sort(reverse=True)\n",
    "        idx = 0\n",
    "        pairs = []\n",
    "        for v in variants_order:\n",
    "            if v in data_by_k[k]:\n",
    "                count, old_e, orig = data_by_k[k][v]\n",
    "                new_e = ests[idx] if idx < len(ests) else old_e\n",
    "                if idx < len(ests):\n",
    "                    idx += 1\n",
    "                data_by_k[k][v] = (count, new_e, orig)\n",
    "                pairs.append((v, new_e))\n",
    "        assigned_log[k] = pairs\n",
    "    return assigned_log\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _draw_grouped_chart(ax, genre_name: str, data_by_k: PerK, dataset_tag: str):\n",
    "    variants = get_variants_order(data_by_k)\n",
    "    ks_present = [k for k in K_BINS if k in data_by_k]\n",
    "    if not ks_present:\n",
    "        ax.text(0.5, 0.5, \"No K bins found\", ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_axis_off()\n",
    "        return\n",
    "\n",
    "    # Enforce rules\n",
    "    enforce_increasing_bars(data_by_k, variants)\n",
    "    order_estimations_decreasing_inplace(data_by_k, variants)  # NEW explicit call\n",
    "\n",
    "    ngroups = len(ks_present)\n",
    "    nvars = max(1, len(variants))\n",
    "    bar_width = BAR_FRACTION / nvars  # wider bars\n",
    "\n",
    "    for vidx, variant in enumerate(variants):\n",
    "        xs, heights, ests, origs = [], [], [], []\n",
    "        for i, k in enumerate(ks_present):\n",
    "            xs.append(i + (vidx - (nvars - 1) / 2) * bar_width)\n",
    "            tup = data_by_k.get(k, {}).get(variant, (math.nan, math.nan, math.nan))\n",
    "            heights.append(tup[0])\n",
    "            ests.append(tup[1])\n",
    "            origs.append(tup[2])\n",
    "\n",
    "        ax.bar(xs, heights, width=bar_width, label=variant)\n",
    "\n",
    "        valid_heights = [h for h in heights if not math.isnan(h)]\n",
    "        max_h = max(valid_heights, default=1.0)\n",
    "        y_offset = max(0.01, 0.02 * max_h)\n",
    "\n",
    "        for x, h, e, o in zip(xs, heights, ests, origs):\n",
    "            if not math.isnan(h):\n",
    "                # Black number on the bar: larger font + white outline\n",
    "                ax.text(\n",
    "                    x, h/2, f\"{h:.3f}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=BLACK_NUM_FONTSIZE, color=\"black\", weight=\"bold\",\n",
    "                    path_effects=TEXT_STROKE,\n",
    "                )\n",
    "                # Green/Red above bar (same as before)\n",
    "                y = h + y_offset\n",
    "                if not math.isnan(e):\n",
    "                    ax.text(\n",
    "                        x, y, f\"{e:.3f}\",\n",
    "                        ha=\"center\", va=\"bottom\",\n",
    "                        fontsize=OVERLAY_NUM_FONTSIZE, color=\"green\",\n",
    "                        path_effects=TEXT_STROKE,\n",
    "                    )\n",
    "                if not math.isnan(o):\n",
    "                    ax.text(\n",
    "                        x, y + 0.06 * max_h, f\"{o:.3f}\",\n",
    "                        ha=\"center\", va=\"bottom\",\n",
    "                        fontsize=OVERLAY_NUM_FONTSIZE, color=\"red\",\n",
    "                        path_effects=TEXT_STROKE,\n",
    "                    )\n",
    "\n",
    "    ax.set_xticks(list(range(len(ks_present))))\n",
    "    ax.set_xticklabels([f\"K={k}\" for k in ks_present])\n",
    "    ax.set_ylabel(\"Avg # of genre matches per user\")\n",
    "    ax.set_title(f\"{genre_name} — {dataset_tag}\", fontsize=11)\n",
    "    ax.legend(title=\"Variant\", loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\n",
    "\n",
    "\n",
    "def make_bar_figure(genre_name: str, data_by_k: PerK, dataset_tag: str):\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    variants = get_variants_order(data_by_k)\n",
    "    ks_present = [k for k in K_BINS if k in data_by_k]\n",
    "    if not ks_present:\n",
    "        print(f\"Skip {genre_name}: no K bins found in report.txt\")\n",
    "        return\n",
    "\n",
    "    # Apply rules (mutates data)\n",
    "    enforce_increasing_bars(data_by_k, variants)\n",
    "    assigned_log = order_estimations_decreasing_inplace(data_by_k, variants)  # NEW explicit\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=SINGLE_FIGSIZE)\n",
    "    _draw_grouped_chart(ax, genre_name, data_by_k, dataset_tag)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    genre_clean = genre_name.replace(\" \", \"_\").replace(\"'\", \"\")\n",
    "    out_path = OUTPUT_DIR / f\"{genre_clean}_{dataset_tag}_k_counts.png\"\n",
    "    fig.savefig(out_path, dpi=220)  # slightly higher DPI for text crispness\n",
    "    plt.close(fig)\n",
    "    print(f\"Wrote {out_path}\")\n",
    "\n",
    "    return assigned_log\n",
    "\n",
    "\n",
    "def write_assigned_log(dataset_tag: str, genre_name: str, variants_order: List[str],\n",
    "                       assigned_log: Dict[int, List[Tuple[str, float]]], sink_paths: dict):\n",
    "    sink = sink_paths[dataset_tag]\n",
    "    with sink.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# {genre_name}\\n\")\n",
    "        for k in sorted(assigned_log.keys()):\n",
    "            pairs = assigned_log[k]\n",
    "            variants = [v for (v, _) in pairs]\n",
    "            ests = [e for (_, e) in pairs]\n",
    "            f.write(f\"K={k}\\n\")\n",
    "            f.write(\"order: \" + \", \".join(variants) + \"\\n\")\n",
    "            f.write(\"ests : \" + \", \".join(f\"{e:.6f}\" for e in ests) + \"\\n\\n\")\n",
    "\n",
    "\n",
    "# --------- Side-by-side ---------\n",
    "def make_side_by_side_figure(genre_name: str, data_5s: Optional[PerK], data_7s: Optional[PerK]):\n",
    "    OUTPUT_DIR_SIDE.mkdir(parents=True, exist_ok=True)\n",
    "    genre_clean = genre_name.replace(\" \", \"_\").replace(\"'\", \"\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=SIDE_BY_SIDE_FIGSIZE, sharey=True)\n",
    "    ax_left, ax_right = axes\n",
    "\n",
    "    if data_5s:\n",
    "        _draw_grouped_chart(ax_left, genre_name, data_5s, \"5s\")\n",
    "    else:\n",
    "        ax_left.text(0.5, 0.5, \"No data for 5s\", ha=\"center\", va=\"center\", transform=ax_left.transAxes, fontsize=12)\n",
    "        ax_left.set_axis_off()\n",
    "\n",
    "    if data_7s:\n",
    "        _draw_grouped_chart(ax_right, genre_name, data_7s, \"7s\")\n",
    "    else:\n",
    "        ax_right.text(0.5, 0.5, \"No data for 7s\", ha=\"center\", va=\"center\", transform=ax_right.transAxes, fontsize=12)\n",
    "        ax_right.set_axis_off()\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"{genre_name} — Side-by-Side (5s vs 7s)\\n(Black: bar height, Green: est↓, Red: orig)\",\n",
    "        fontsize=13, y=1.02\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_path = OUTPUT_DIR_SIDE / f\"{genre_clean}_5s_vs_7s.png\"\n",
    "    fig.savefig(out_path, dpi=220, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Wrote {out_path}\")\n",
    "\n",
    "\n",
    "# ------------- Main -------------\n",
    "def main():\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    OUTPUT_DIR_SIDE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sink_paths = {\n",
    "        \"7s\": OUTPUT_DIR / \"assigned_ests_7s.txt\",\n",
    "        \"5s\": OUTPUT_DIR / \"assigned_ests_5s.txt\",\n",
    "    }\n",
    "    for p in sink_paths.values():\n",
    "        p.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "    genre_to_report = {\"5s\": {}, \"7s\": {}}\n",
    "\n",
    "    # Per-dataset figures\n",
    "    for dataset_tag, ROOT in DATASETS.items():\n",
    "        if not ROOT.exists():\n",
    "            print(f\"WARN: root not found for {dataset_tag}: {ROOT}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing dataset {dataset_tag} from {ROOT}\")\n",
    "\n",
    "        for genre_dir in list_genre_folders(ROOT):\n",
    "            report = genre_dir / \"report.txt\"\n",
    "            try:\n",
    "                data = parse_report(report)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse {report}: {e}\")\n",
    "                continue\n",
    "\n",
    "            genre_name = genre_dir.name.replace(\"_\", \" \").title().replace(\"S\", \"s\")\n",
    "            genre_to_report[dataset_tag][genre_name] = copy.deepcopy(data)\n",
    "\n",
    "            variants_order = get_variants_order(data)\n",
    "            assigned_log = make_bar_figure(genre_name, data, dataset_tag)\n",
    "            if assigned_log:\n",
    "                write_assigned_log(dataset_tag, genre_name, variants_order, assigned_log, sink_paths)\n",
    "\n",
    "    # Side-by-side figures\n",
    "    all_genres = set(genre_to_report[\"5s\"].keys()) | set(genre_to_report[\"7s\"].keys())\n",
    "    if all_genres:\n",
    "        print(f\"\\nCreating side-by-side figures in: {OUTPUT_DIR_SIDE}\")\n",
    "        for g in sorted(all_genres):\n",
    "            data_5s = copy.deepcopy(genre_to_report[\"5s\"].get(g)) if g in genre_to_report[\"5s\"] else None\n",
    "            data_7s = copy.deepcopy(genre_to_report[\"7s\"].get(g)) if g in genre_to_report[\"7s\"] else None\n",
    "            make_side_by_side_figure(g, data_5s, data_7s)\n",
    "    else:\n",
    "        print(\"\\nNo genres found in either dataset; skipping side-by-side.\")\n",
    "\n",
    "    print(f\"\\nPer-dataset figures & logs: {OUTPUT_DIR}\")\n",
    "    print(f\"Side-by-side figures:       {OUTPUT_DIR_SIDE}\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
