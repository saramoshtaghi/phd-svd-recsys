{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done.\n",
      "  ‚Ä¢ Datasets: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0928/data/improved_synthetic_heavy\n",
      "  ‚Ä¢ Summary (txt): /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0928/data/improved_synthetic_heavy_summary.txt\n",
      "  ‚Ä¢ Summary (csv): /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0928/data/improved_synthetic_heavy_summary.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_heavy_bias.py\n",
    "# Heavily biased injection: for each genre G1 (primary),\n",
    "# create synthetic users who rate *all* books whose primary genre == G1.\n",
    "# No caps, no fancy sampling.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "BASE_DIR   = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book\")\n",
    "INPUT_CSV  = BASE_DIR / \"data/df_final_with_genres.csv\"   # must have: user_id, book_id, rating, genres\n",
    "OUT_DIR    = BASE_DIR / \"result/rec/top_re/0928/data/improved_synthetic_heavy\"   # output datasets (one per genre/run)\n",
    "SUMMARY_TXT= BASE_DIR / \"result/rec/top_re/0928/data/improved_synthetic_heavy_summary.txt\"\n",
    "SUMMARY_CSV= BASE_DIR / \"result/rec/top_re/0928/data/improved_synthetic_heavy_summary.csv\"\n",
    "\n",
    "GENRE_COL  = \"genres\"\n",
    "USER_COL   = \"user_id\"\n",
    "BOOK_COL   = \"book_id\"\n",
    "RATING_COL = \"rating\"\n",
    "\n",
    "RUNS = [25, 400, 1000, 5000, 10000]     # number of synthetic users to inject per genre\n",
    "SYNTH_RATING = 5               # rating to assign\n",
    "# =========================\n",
    "\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def primary_genre(cell: str) -> str:\n",
    "    if not isinstance(cell, str) or not cell.strip():\n",
    "        return \"\"\n",
    "    # primary genre = the first token before comma\n",
    "    return cell.split(\",\")[0].strip()\n",
    "\n",
    "def main():\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---------- Load ----------\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    # numeric & basic hygiene\n",
    "    df[USER_COL]   = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]   = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "    df[GENRE_COL]  = df[GENRE_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    # baseline stats\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "\n",
    "    # build a quick book->genres lookup (to preserve original genres strings)\n",
    "    book_to_genres = dict(df[[BOOK_COL, GENRE_COL]].drop_duplicates().values)\n",
    "\n",
    "    # compute primary genre per row and build per-genre unique book list\n",
    "    work = df[[BOOK_COL, GENRE_COL]].copy()\n",
    "    work[\"_primary\"] = work[GENRE_COL].apply(primary_genre)\n",
    "    work = work[work[\"_primary\"] != \"\"].drop_duplicates(subset=[BOOK_COL, \"_primary\"])\n",
    "\n",
    "    per_genre = (\n",
    "        work.groupby(\"_primary\")[BOOK_COL]\n",
    "            .apply(lambda s: sorted(pd.Series(s.unique()).astype(int).tolist()))\n",
    "            .to_frame(\"book_list\")\n",
    "            .reset_index()\n",
    "    )\n",
    "    per_genre[\"n_books\"] = per_genre[\"book_list\"].apply(len)\n",
    "\n",
    "    # Make a stable, nice order of genres\n",
    "    target_genres = sorted(per_genre[\"_primary\"].tolist(), key=lambda x: x.lower())\n",
    "\n",
    "    # ID block to avoid collisions: allocate a huge block for each (genre, run)\n",
    "    BLOCK = 1_000_000\n",
    "\n",
    "    # logging containers\n",
    "    rows_summary = []\n",
    "    with open(SUMMARY_TXT, \"w\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=== BASELINE ===\\n\")\n",
    "        log.write(f\"üë§ Unique users: {baseline_users:,}\\n\")\n",
    "        log.write(f\"üßæ Rows: {baseline_rows:,}\\n\")\n",
    "        log.write(f\"üî¢ Synthetic user_id base start: {base_start_uid}\\n\")\n",
    "        log.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    grand_added = 0\n",
    "    made_any = False\n",
    "\n",
    "    for gi, g in enumerate(target_genres):\n",
    "        book_list = per_genre.loc[per_genre[\"_primary\"] == g, \"book_list\"].iloc[0]\n",
    "        n_books   = int(per_genre.loc[per_genre[\"_primary\"] == g, \"n_books\"].iloc[0])\n",
    "\n",
    "        if n_books <= 0 or not book_list:\n",
    "            continue\n",
    "\n",
    "        safe_g = sanitize_fn(g)\n",
    "        with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"üé≠ {g} | primary-genre unique books = {n_books}\\n\")\n",
    "\n",
    "        for r_i, run in enumerate(RUNS):\n",
    "            # disjoint user id space for (genre, run)\n",
    "            start_uid = base_start_uid + gi * (len(RUNS) * BLOCK) + r_i * BLOCK\n",
    "            new_uids = list(range(start_uid, start_uid + run))\n",
    "\n",
    "            # build synthetic rows: each new user rates *all* primary-genre books\n",
    "            synth = {\n",
    "                USER_COL:   [],\n",
    "                BOOK_COL:   [],\n",
    "                RATING_COL: [],\n",
    "                GENRE_COL:  []\n",
    "            }\n",
    "            synth[USER_COL]   = [uid for uid in new_uids for _ in range(n_books)]\n",
    "            synth[BOOK_COL]   = [b for _ in new_uids for b in book_list]\n",
    "            synth[RATING_COL] = [SYNTH_RATING] * (run * n_books)\n",
    "            # preserve the original, full genres string of each book\n",
    "            synth[GENRE_COL]  = [book_to_genres.get(b, \"\") for _ in new_uids for b in book_list]\n",
    "\n",
    "            synth_df = pd.DataFrame(synth, columns=[USER_COL, BOOK_COL, RATING_COL, GENRE_COL])\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            # quick checks\n",
    "            expected = run * n_books\n",
    "            assert len(synth_df) == expected, f\"Row count mismatch for {g}, run={run}\"\n",
    "            new_users_total = combined[USER_COL].nunique()\n",
    "\n",
    "            out_path = OUT_DIR / f\"enhanced_{safe_g}_{run}.csv\"\n",
    "            combined.to_csv(out_path, index=False)\n",
    "\n",
    "            # log lines\n",
    "            with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"  run={run:>4} ‚Üí records_added={expected:>9,} | \"\n",
    "                          f\"new_rows={len(combined):,} | new_users={new_users_total:,}\\n\")\n",
    "\n",
    "            rows_summary.append({\n",
    "                \"genre\": g,\n",
    "                \"safe_genre\": safe_g,\n",
    "                \"primary_unique_books\": n_books,\n",
    "                \"run_users\": run,\n",
    "                \"records_added\": expected,\n",
    "                \"new_total_rows\": len(combined),\n",
    "                \"new_total_users\": new_users_total,\n",
    "                \"output_csv\": str(out_path)\n",
    "            })\n",
    "\n",
    "            grand_added += expected\n",
    "            made_any = True\n",
    "\n",
    "        with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(\"\\n\")\n",
    "\n",
    "    # write summary CSV\n",
    "    if rows_summary:\n",
    "        pd.DataFrame(rows_summary).to_csv(SUMMARY_CSV, index=False)\n",
    "\n",
    "    with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"================================================================================\\n\")\n",
    "        log.write(f\"Grand total injected rows (across all genres & runs): {grand_added:,}\\n\")\n",
    "        log.write(f\"Outputs folder: {OUT_DIR}\\n\")\n",
    "        log.write(f\"Per-run summary CSV: {SUMMARY_CSV}\\n\")\n",
    "\n",
    "    if not made_any:\n",
    "        print(\"‚ö†Ô∏è  No datasets were produced. Check genre names and input columns.\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Done.\")\n",
    "        print(\"  ‚Ä¢ Datasets:\", OUT_DIR)\n",
    "        print(\"  ‚Ä¢ Summary (txt):\", SUMMARY_TXT)\n",
    "        print(\"  ‚Ä¢ Summary (csv):\", SUMMARY_CSV)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
