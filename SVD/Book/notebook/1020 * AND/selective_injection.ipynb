{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_pair_bias_pos5_neg1_all_smallcohorts.py\n",
    "# Changes:\n",
    "#   • Count all pairs (and adult-only subsets) before generating.\n",
    "#   • Generate ONLY “adult-like” pairs (any genre containing \"adult\", case-insensitive).\n",
    "#   • Optional gzip compression to save disk space.\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "INPUT_CSV   = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "BASE_OUT_DIR= Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1020/data/PAIR_INJECTION\")\n",
    "\n",
    "GENRE_COL   = \"genres\"\n",
    "USER_COL    = \"user_id\"\n",
    "BOOK_COL    = \"book_id\"\n",
    "RATING_COL  = \"rating\"\n",
    "\n",
    "RUN_USERS   = [2, 4, 6, 25, 50, 100, 200, 300, 500, 1000]\n",
    "ZERO_MODE   = \"all\"\n",
    "NEG_RATING  = 1\n",
    "BLOCK       = 1_000_000\n",
    "\n",
    "# Space-saving: write gzip-compressed CSVs (set to None to disable)\n",
    "COMPRESSION = None  # or None\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def parse_genres(cell: str):\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # try list/tuple literal first\n",
    "    if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "        try:\n",
    "            import ast\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # else, split by common separators\n",
    "    for sep in [\",\", \"|\", \";\", \"//\", \"/\"]:\n",
    "        if sep in s:\n",
    "            parts = [p.strip() for p in s.split(sep) if p.strip()]\n",
    "            seen, out = set(), []\n",
    "            for p in parts:\n",
    "                if p not in seen:\n",
    "                    out.append(p); seen.add(p)\n",
    "            return out\n",
    "    return [s]\n",
    "\n",
    "def prepare_books(df: pd.DataFrame):\n",
    "    books = df[[BOOK_COL, GENRE_COL]].drop_duplicates(subset=[BOOK_COL]).copy()\n",
    "    books[\"genre_list\"] = books[GENRE_COL].apply(parse_genres)\n",
    "    books = books[books[\"genre_list\"].map(len) > 0].copy()\n",
    "    book_to_list = dict(zip(books[BOOK_COL].astype(int), books[\"genre_list\"]))\n",
    "    book_to_set  = {int(b): set(l) for b, l in book_to_list.items()}\n",
    "    all_books = sorted(book_to_list.keys())\n",
    "    return all_books, book_to_list, book_to_set\n",
    "\n",
    "def is_adult_like(genre: str) -> bool:\n",
    "    return \"adult\" in (genre or \"\").lower()  # matches \"Adult\", \"Young Adult\", etc.\n",
    "\n",
    "# ========= GENERATOR (pos=5 only) =========\n",
    "def run_for_pos5(df: pd.DataFrame, base_start_uid: int):\n",
    "    pos_rating = 5\n",
    "    out_dir = BASE_OUT_DIR / str(pos_rating)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_books, book_to_list, book_to_set = prepare_books(df)\n",
    "    GENRES = sorted({g for gl in book_to_list.values() for g in gl})\n",
    "\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "\n",
    "    summary_txt = out_dir / \"summary.txt\"\n",
    "    summary_csv = out_dir / \"summary.csv\"\n",
    "    pairs_overview_csv = out_dir / \"pairs_overview.csv\"\n",
    "    missing_pairs_csv = out_dir / \"missing_pairs.csv\"\n",
    "\n",
    "    # ----- Pair counts (global, before filtering) -----\n",
    "    all_pairs = list(combinations(GENRES, 2))\n",
    "    total_pairs = len(all_pairs)\n",
    "\n",
    "    # Count how many pairs have >=1 positive book anywhere\n",
    "    def count_pairs_with_positives(pairs):\n",
    "        cnt = 0\n",
    "        for g1, g2 in pairs:\n",
    "            # fast short-circuit using sets per book\n",
    "            n_pos = sum(1 for b in all_books if (g1 in book_to_set[b] and g2 in book_to_set[b]))\n",
    "            if n_pos > 0:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "\n",
    "    # Adult-like subsets for counting\n",
    "    adult_pairs_all = [(g1, g2) for (g1, g2) in all_pairs if is_adult_like(g1) or is_adult_like(g2)]\n",
    "    total_adult_pairs = len(adult_pairs_all)\n",
    "\n",
    "    # Count positives\n",
    "    total_pairs_with_pos = count_pairs_with_positives(all_pairs)\n",
    "    total_adult_pairs_with_pos = count_pairs_with_positives(adult_pairs_all)\n",
    "\n",
    "    with open(summary_txt, \"w\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=== BASELINE ===\\n\")\n",
    "        log.write(f\"👤 Unique users: {baseline_users:,}\\n\")\n",
    "        log.write(f\"🧾 Rows: {baseline_rows:,}\\n\")\n",
    "        log.write(f\"POS_RATING={pos_rating} | ZERO_MODE={ZERO_MODE} | NEG_RATING={NEG_RATING}\\n\")\n",
    "        log.write(f\"Discovered genres ({len(GENRES)}): {GENRES}\\n\\n\")\n",
    "        log.write(\"=== PAIR COUNTS (pre-filter) ===\\n\")\n",
    "        log.write(f\"All pairs (combinatorial): {total_pairs:,}\\n\")\n",
    "        log.write(f\"All pairs with ≥1 positive book: {total_pairs_with_pos:,}\\n\")\n",
    "        log.write(f\"Adult-like pairs (any side contains 'adult'): {total_adult_pairs:,}\\n\")\n",
    "        log.write(f\"Adult-like pairs with ≥1 positive book: {total_adult_pairs_with_pos:,}\\n\\n\")\n",
    "        log.write(\"Processing mode: ADULT-ONLY pairs to save disk space.\\n\\n\")\n",
    "\n",
    "    rows_summary = []\n",
    "    pairs_overview_rows = []\n",
    "    missing_pairs = []\n",
    "\n",
    "    # ----- Process ONLY adult-like pairs -----\n",
    "    pair_index = 0\n",
    "    for g1, g2 in adult_pairs_all:\n",
    "        # positives: books having BOTH g1 and g2\n",
    "        pos_books = [b for b in all_books if (g1 in book_to_set[b] and g2 in book_to_set[b])]\n",
    "        n_pos = len(pos_books)\n",
    "        neg_pool = [b for b in all_books if b not in pos_books]\n",
    "        n_neg_pool = len(neg_pool)\n",
    "\n",
    "        pairs_overview_rows.append({\"pair\": f\"{g1} + {g2}\", \"g1\": g1, \"g2\": g2,\n",
    "                                    \"n_pos_books\": n_pos, \"neg_pool\": n_neg_pool})\n",
    "        if n_pos == 0:\n",
    "            missing_pairs.append({\"pair\": f\"{g1} + {g2}\", \"g1\": g1, \"g2\": g2})\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"(skip) No overlapping books for pair: {g1} + {g2}\\n\")\n",
    "            pair_index += 1\n",
    "            continue\n",
    "\n",
    "        safe_p = f\"{sanitize_fn(g1)}__{sanitize_fn(g2)}\"\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"🔗 Pair: {g1} + {g2} | positives (pair-books) = {n_pos} | neg_pool = {n_neg_pool}\\n\")\n",
    "\n",
    "        neg_books_for_all_users = neg_pool  # ZERO_MODE == \"all\"\n",
    "\n",
    "        for run_idx, run_users in enumerate(RUN_USERS):\n",
    "            start_uid = base_start_uid + pair_index * (len(RUN_USERS) * BLOCK) + run_idx * BLOCK\n",
    "            new_uids = list(range(start_uid, start_uid + run_users))\n",
    "\n",
    "            # synth positives\n",
    "            df_pos = pd.DataFrame({\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_pos)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in pos_books],\n",
    "                RATING_COL: [pos_rating] * (run_users * n_pos),\n",
    "                GENRE_COL:  [\",\".join(sorted(book_to_list.get(b, []))) for _ in new_uids for b in pos_books]\n",
    "            })\n",
    "\n",
    "            # synth negatives\n",
    "            n_neg = len(neg_books_for_all_users)\n",
    "            df_neg = pd.DataFrame({\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_neg)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in neg_books_for_all_users],\n",
    "                RATING_COL: [NEG_RATING] * (run_users * n_neg),\n",
    "                GENRE_COL:  [\",\".join(sorted(book_to_list.get(b, []))) for _ in new_uids for b in neg_books_for_all_users]\n",
    "            })\n",
    "\n",
    "            synth_df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            # filename + optional compression\n",
    "            out_base = f\"fpair_{safe_p}_{run_users}u_pos{pos_rating}_neg{NEG_RATING}.csv\"\n",
    "            out_path = out_dir / (out_base + (\".gz\" if COMPRESSION else \"\"))\n",
    "            combined.to_csv(out_path, index=False, compression=COMPRESSION)\n",
    "\n",
    "            print(f\"✅ Completed injection file: {out_path.name}\")\n",
    "\n",
    "            rows_added = len(synth_df)\n",
    "            rows_pos = len(df_pos)\n",
    "            rows_neg = len(df_neg)\n",
    "            new_users_total = combined[USER_COL].nunique()\n",
    "\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(\n",
    "                    f\"  users={run_users:>5} → +rows={rows_added:>12,} (pos={rows_pos:,}, neg={rows_neg:,}) | \"\n",
    "                    f\"new_rows={len(combined):,} | new_users={new_users_total:,} | outfile={out_path.name}\\n\"\n",
    "                )\n",
    "\n",
    "            rows_summary.append({\n",
    "                \"pos_rating\": pos_rating,\n",
    "                \"pair\": f\"{g1} + {g2}\",\n",
    "                \"g1\": g1,\n",
    "                \"g2\": g2,\n",
    "                \"run_users\": run_users,\n",
    "                \"n_pos_books\": n_pos,\n",
    "                \"n_neg_books_per_user\": n_neg,\n",
    "                \"rows_added\": rows_added,\n",
    "                \"rows_pos\": rows_pos,\n",
    "                \"rows_neg\": rows_neg,\n",
    "                \"zero_mode\": ZERO_MODE,\n",
    "                \"output_csv\": str(out_path)\n",
    "            })\n",
    "\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(\"\\n\")\n",
    "\n",
    "        pair_index += 1\n",
    "\n",
    "    # ----- Outputs -----\n",
    "    if rows_summary:\n",
    "        pd.DataFrame(rows_summary).to_csv(summary_csv, index=False)\n",
    "    if pairs_overview_rows:\n",
    "        pd.DataFrame(pairs_overview_rows).sort_values([\"g1\",\"g2\"]).to_csv(pairs_overview_csv, index=False)\n",
    "    if missing_pairs:\n",
    "        pd.DataFrame(missing_pairs).to_csv(missing_pairs_csv, index=False)\n",
    "\n",
    "    with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=\"*80 + \"\\n\")\n",
    "        log.write(f\"Grand total injected rows (ADULT-ONLY, pos=5): {sum(r['rows_added'] for r in rows_summary):,}\\n\")\n",
    "        log.write(f\"Pairs overview (adult-only): {pairs_overview_csv}\\n\")\n",
    "        log.write(f\"Missing pairs (adult-only): {missing_pairs_csv}\\n\\n\")\n",
    "\n",
    "    print(f\"✅ Done for pos=5 (adult-only). Out: {out_dir}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Loading original CSV...\")\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    df[USER_COL]   = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]   = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "    df[GENRE_COL]  = df[GENRE_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "    run_for_pos5(df, base_start_uid)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adventure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original CSV...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2135417/1118318409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2135417/1118318409.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mbase_start_uid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUSER_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mrun_for_pos5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_start_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2135417/1118318409.py\u001b[0m in \u001b[0;36mrun_for_pos5\u001b[0;34m(df, base_start_uid)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOMPRESSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Completed injection file: {out_path.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3549\u001b[0m         )\n\u001b[1;32m   3550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3551\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3552\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         )\n\u001b[0;32m-> 1180\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m             )\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         libwriters.write_csv_rows(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_pair_bias_pos5_neg1_all_smallcohorts__ADVENTURE_ONLY.py\n",
    "# Changes from your adult-focused script:\n",
    "#   • Process ONLY pairs that involve \"Adventure\" (case-insensitive).\n",
    "#   • Explicitly SKIP any pair where the other side is \"adult\"-like (to avoid Adventure↔Adult duplicates).\n",
    "#   • Keep your existing layout, options, and logging style.\n",
    "#   • Skip writing an output if the file already exists (protects space & avoids repeats).\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "INPUT_CSV   = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "BASE_OUT_DIR= Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1020/data/PAIR_INJECTION\")\n",
    "\n",
    "GENRE_COL   = \"genres\"\n",
    "USER_COL    = \"user_id\"\n",
    "BOOK_COL    = \"book_id\"\n",
    "RATING_COL  = \"rating\"\n",
    "\n",
    "RUN_USERS   = [2, 4, 6, 25, 50, 100, 200, 300, 500, 1000]\n",
    "ZERO_MODE   = \"all\"   # negatives = ALL non-pair books\n",
    "NEG_RATING  = 1\n",
    "BLOCK       = 1_000_000\n",
    "\n",
    "# Space-saving: write gzip-compressed CSVs (set to None to disable)\n",
    "COMPRESSION = None  # or \"gzip\"\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def parse_genres(cell: str):\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # try list/tuple literal first\n",
    "    if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "        try:\n",
    "            import ast\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # else, split by common separators\n",
    "    for sep in [\",\", \"|\", \";\", \"//\", \"/\"]:\n",
    "        if sep in s:\n",
    "            parts = [p.strip() for p in s.split(sep) if p.strip()]\n",
    "            seen, out = set(), []\n",
    "            for p in parts:\n",
    "                if p not in seen:\n",
    "                    out.append(p); seen.add(p)\n",
    "            return out\n",
    "    return [s]\n",
    "\n",
    "def prepare_books(df: pd.DataFrame):\n",
    "    books = df[[BOOK_COL, GENRE_COL]].drop_duplicates(subset=[BOOK_COL]).copy()\n",
    "    books[\"genre_list\"] = books[GENRE_COL].apply(parse_genres)\n",
    "    books = books[books[\"genre_list\"].map(len) > 0].copy()\n",
    "    book_to_list = dict(zip(books[BOOK_COL].astype(int), books[\"genre_list\"]))\n",
    "    book_to_set  = {int(b): set(l) for b, l in book_to_list.items()}\n",
    "    all_books = sorted(book_to_list.keys())\n",
    "    return all_books, book_to_list, book_to_set\n",
    "\n",
    "def is_adult_like(genre: str) -> bool:\n",
    "    return \"adult\" in (genre or \"\").lower()  # matches \"Adult\", \"Young Adult\", etc.\n",
    "\n",
    "def is_adventure_like(genre: str) -> bool:\n",
    "    # use 'adventur' stem to catch \"Adventure\", \"Action & Adventure\", etc.\n",
    "    return \"adventur\" in (genre or \"\").lower()\n",
    "\n",
    "def should_process_pair(g1: str, g2: str) -> bool:\n",
    "    \"\"\"\n",
    "    Include this pair if exactly one side is Adventure-like.\n",
    "    EXCLUDE if the non-adventure side is adult-like (avoid Adventure↔Adult duplicates already done).\n",
    "    \"\"\"\n",
    "    g1_adv, g2_adv = is_adventure_like(g1), is_adventure_like(g2)\n",
    "    # must involve Adventure on one side\n",
    "    if not (g1_adv ^ g2_adv):\n",
    "        return False\n",
    "    # skip Adventure paired with Adult-like\n",
    "    if (g1_adv and is_adult_like(g2)) or (g2_adv and is_adult_like(g1)):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# ========= GENERATOR (pos=5 only) =========\n",
    "def run_for_pos5(df: pd.DataFrame, base_start_uid: int):\n",
    "    pos_rating = 5\n",
    "    out_dir = BASE_OUT_DIR / str(pos_rating)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_books, book_to_list, book_to_set = prepare_books(df)\n",
    "    GENRES = sorted({g for gl in book_to_list.values() for g in gl})\n",
    "\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "\n",
    "    summary_txt = out_dir / \"summary.txt\"\n",
    "    summary_csv = out_dir / \"summary.csv\"\n",
    "    pairs_overview_csv = out_dir / \"pairs_overview.csv\"\n",
    "    missing_pairs_csv = out_dir / \"missing_pairs.csv\"\n",
    "\n",
    "    # ----- Pair counts (global, before filtering) -----\n",
    "    all_pairs = list(combinations(GENRES, 2))\n",
    "    total_pairs = len(all_pairs)\n",
    "\n",
    "    # Count how many pairs have >=1 positive book anywhere\n",
    "    def count_pairs_with_positives(pairs):\n",
    "        cnt = 0\n",
    "        for g1, g2 in pairs:\n",
    "            n_pos = sum(1 for b in all_books if (g1 in book_to_set[b] and g2 in book_to_set[b]))\n",
    "            if n_pos > 0:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "\n",
    "    # Adventure-only candidate pairs (excluding Adventure↔Adult)\n",
    "    adv_pairs_all = [(g1, g2) for (g1, g2) in all_pairs if should_process_pair(g1, g2)]\n",
    "    total_adv_pairs = len(adv_pairs_all)\n",
    "    total_pairs_with_pos = count_pairs_with_positives(all_pairs)\n",
    "    total_adv_pairs_with_pos = count_pairs_with_positives(adv_pairs_all)\n",
    "\n",
    "    with open(summary_txt, \"w\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=== BASELINE ===\\n\")\n",
    "        log.write(f\"👤 Unique users: {baseline_users:,}\\n\")\n",
    "        log.write(f\"🧾 Rows: {baseline_rows:,}\\n\")\n",
    "        log.write(f\"POS_RATING={pos_rating} | ZERO_MODE={ZERO_MODE} | NEG_RATING={NEG_RATING}\\n\")\n",
    "        log.write(f\"Discovered genres ({len(GENRES)}): {GENRES}\\n\\n\")\n",
    "        log.write(\"=== PAIR COUNTS (pre-filter) ===\\n\")\n",
    "        log.write(f\"All pairs (combinatorial): {total_pairs:,}\\n\")\n",
    "        log.write(f\"All pairs with ≥1 positive book: {total_pairs_with_pos:,}\\n\")\n",
    "        log.write(f\"Adventure-only pairs (one side Adventure, other NOT Adult-like): {total_adv_pairs:,}\\n\")\n",
    "        log.write(f\"Adventure-only pairs with ≥1 positive book: {total_adv_pairs_with_pos:,}\\n\\n\")\n",
    "        log.write(\"Processing mode: ADVENTURE-ONLY (EXCLUDING any Adventure↔Adult pair).\\n\\n\")\n",
    "\n",
    "    rows_summary = []\n",
    "    pairs_overview_rows = []\n",
    "    missing_pairs = []\n",
    "\n",
    "    # ----- Process ONLY Adventure pairs (excluding Adventure↔Adult) -----\n",
    "    pair_index = 0\n",
    "    for g1, g2 in adv_pairs_all:\n",
    "        # positives: books having BOTH g1 and g2\n",
    "        pos_books = [b for b in all_books if (g1 in book_to_set[b] and g2 in book_to_set[b])]\n",
    "        n_pos = len(pos_books)\n",
    "        neg_pool = [b for b in all_books if b not in pos_books]\n",
    "        n_neg_pool = len(neg_pool)\n",
    "\n",
    "        pairs_overview_rows.append({\"pair\": f\"{g1} + {g2}\", \"g1\": g1, \"g2\": g2,\n",
    "                                    \"n_pos_books\": n_pos, \"neg_pool\": n_neg_pool})\n",
    "        if n_pos == 0:\n",
    "            missing_pairs.append({\"pair\": f\"{g1} + {g2}\", \"g1\": g1, \"g2\": g2})\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"(skip) No overlapping books for pair: {g1} + {g2}\\n\")\n",
    "            pair_index += 1\n",
    "            continue\n",
    "\n",
    "        safe_p = f\"{sanitize_fn(g1)}__{sanitize_fn(g2)}\"\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"🔗 Pair: {g1} + {g2} | positives (pair-books) = {n_pos} | neg_pool = {n_neg_pool}\\n\")\n",
    "\n",
    "        neg_books_for_all_users = neg_pool  # ZERO_MODE == \"all\"\n",
    "\n",
    "        for run_idx, run_users in enumerate(RUN_USERS):\n",
    "            start_uid = base_start_uid + pair_index * (len(RUN_USERS) * BLOCK) + run_idx * BLOCK\n",
    "            new_uids = list(range(start_uid, start_uid + run_users))\n",
    "\n",
    "            # synth positives\n",
    "            df_pos = pd.DataFrame({\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_pos)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in pos_books],\n",
    "                RATING_COL: [pos_rating] * (run_users * n_pos),\n",
    "                GENRE_COL:  [\",\".join(sorted(book_to_list.get(b, []))) for _ in new_uids for b in pos_books]\n",
    "            })\n",
    "\n",
    "            # synth negatives\n",
    "            n_neg = len(neg_books_for_all_users)\n",
    "            df_neg = pd.DataFrame({\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_neg)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in neg_books_for_all_users],\n",
    "                RATING_COL: [NEG_RATING] * (run_users * n_neg),\n",
    "                GENRE_COL:  [\",\".join(sorted(book_to_list.get(b, []))) for _ in new_uids for b in neg_books_for_all_users]\n",
    "            })\n",
    "\n",
    "            synth_df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            # filename + optional compression\n",
    "            out_base = f\"fpair_{safe_p}_{run_users}u_pos{pos_rating}_neg{NEG_RATING}.csv\"\n",
    "            out_path = out_dir / (out_base + (\".gz\" if COMPRESSION else \"\"))\n",
    "\n",
    "            # ⛔️ Already created? Skip to avoid repeats & save space\n",
    "            if out_path.exists():\n",
    "                print(f\"⏭️  Skip existing: {out_path.name}\")\n",
    "                continue\n",
    "\n",
    "            combined.to_csv(out_path, index=False, compression=COMPRESSION)\n",
    "            print(f\"✅ Completed injection file: {out_path.name}\")\n",
    "\n",
    "            rows_added = len(synth_df)\n",
    "            rows_pos = len(df_pos)\n",
    "            rows_neg = len(df_neg)\n",
    "            new_users_total = combined[USER_COL].nunique()\n",
    "\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(\n",
    "                    f\"  users={run_users:>5} → +rows={rows_added:>12,} (pos={rows_pos:,}, neg={rows_neg:,}) | \"\n",
    "                    f\"new_rows={len(combined):,} | new_users={new_users_total:,} | outfile={out_path.name}\\n\"\n",
    "                )\n",
    "\n",
    "            rows_summary.append({\n",
    "                \"pos_rating\": pos_rating,\n",
    "                \"pair\": f\"{g1} + {g2}\",\n",
    "                \"g1\": g1,\n",
    "                \"g2\": g2,\n",
    "                \"run_users\": run_users,\n",
    "                \"n_pos_books\": n_pos,\n",
    "                \"n_neg_books_per_user\": n_neg,\n",
    "                \"rows_added\": rows_added,\n",
    "                \"rows_pos\": rows_pos,\n",
    "                \"rows_neg\": rows_neg,\n",
    "                \"zero_mode\": ZERO_MODE,\n",
    "                \"output_csv\": str(out_path)\n",
    "            })\n",
    "\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(\"\\n\")\n",
    "\n",
    "        pair_index += 1\n",
    "\n",
    "    # ----- Outputs -----\n",
    "    if rows_summary:\n",
    "        pd.DataFrame(rows_summary).to_csv(summary_csv, index=False)\n",
    "    if pairs_overview_rows:\n",
    "        pd.DataFrame(pairs_overview_rows).sort_values([\"g1\",\"g2\"]).to_csv(pairs_overview_csv, index=False)\n",
    "    if missing_pairs:\n",
    "        pd.DataFrame(missing_pairs).to_csv(missing_pairs_csv, index=False)\n",
    "\n",
    "    with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=\"*80 + \"\\n\")\n",
    "        log.write(f\"Grand total injected rows (ADVENTURE-ONLY, pos=5): {sum(r['rows_added'] for r in rows_summary):,}\\n\")\n",
    "        log.write(f\"Pairs overview (adventure-only): {pairs_overview_csv}\\n\")\n",
    "        log.write(f\"Missing pairs (adventure-only): {missing_pairs_csv}\\n\\n\")\n",
    "\n",
    "    print(f\"✅ Done for pos=5 (adventure-only, excluding Adventure↔Adult). Out: {out_dir}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Loading original CSV...\")\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    df[USER_COL]   = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]   = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "    df[GENRE_COL]  = df[GENRE_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "    run_for_pos5(df, base_start_uid)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
