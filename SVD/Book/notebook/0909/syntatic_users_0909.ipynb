{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done.\n",
      "  ‚Ä¢ Datasets: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/data/improved_synthetic_heavy_pos7_neg0\n",
      "  ‚Ä¢ Summary: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/data/improved_synthetic_heavy_pos7_neg0/summary.txt\n",
      "  ‚Ä¢ Summary CSV: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/data/improved_synthetic_heavy_pos7_neg0/summary.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_heavy_bias_pos5_neg0.py\n",
    "# For each primary genre G:\n",
    "#   - positives: all books with primary==G rated 5\n",
    "#   - negatives: all other books rated 0  (configurable: ALL or sampled)\n",
    "#\n",
    "# IMPORTANT: Train Surprise with Reader(rating_scale=(0, 5)) to accept zeros.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "BASE_DIR    = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book\")\n",
    "INPUT_CSV   = BASE_DIR / \"data/df_final_with_genres.csv\"   # requires: user_id, book_id, rating, genres\n",
    "OUT_DIR     = BASE_DIR / \"result/rec/top_re/0909/data/improved_synthetic_heavy_pos7_neg0\"\n",
    "SUMMARY_TXT = OUT_DIR / \"summary.txt\"\n",
    "SUMMARY_CSV = OUT_DIR / \"summary.csv\"\n",
    "\n",
    "GENRE_COL   = \"genres\"\n",
    "USER_COL    = \"user_id\"\n",
    "BOOK_COL    = \"book_id\"\n",
    "RATING_COL  = \"rating\"\n",
    "\n",
    "RUNS = [25, 50, 100, 200]  # number of synthetic users per genre\n",
    "\n",
    "POS_RATING  = 7\n",
    "NEG_RATING  = 0\n",
    "\n",
    "# ---- NEGATIVE assignment mode ----\n",
    "# \"all\"    ‚Üí rate EVERY non-target book as 0  (exactly what you asked; WARNING: HUGE FILES)\n",
    "# \"sample\" ‚Üí sample a subset of non-target books per user to keep datasets manageable\n",
    "ZERO_MODE   = \"sample\"   # change to \"all\" for literal ‚Äúrate rest 0‚Äù\n",
    "NEG_RATIO   = 4          # when ZERO_MODE=\"sample\": negatives per user ‚âà NEG_RATIO * (#positives)\n",
    "RNG_SEED    = 42         # deterministic sampling\n",
    "# ================================\n",
    "\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def primary_genre(cell: str) -> str:\n",
    "    if not isinstance(cell, str) or not cell.strip():\n",
    "        return \"\"\n",
    "    return cell.split(\",\")[0].strip()\n",
    "\n",
    "def main():\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    random.seed(RNG_SEED)\n",
    "\n",
    "    # ---------- Load ----------\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    # hygiene\n",
    "    df[USER_COL]   = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]   = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "    df[GENRE_COL]  = df[GENRE_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    # baseline stats\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "\n",
    "    # lookups\n",
    "    book_to_genres = dict(df[[BOOK_COL, GENRE_COL]].drop_duplicates().values)\n",
    "\n",
    "    # primary genre per book (dedup by book)\n",
    "    book_gen = (df[[BOOK_COL, GENRE_COL]].drop_duplicates()\n",
    "                  .assign(_primary=lambda x: x[GENRE_COL].apply(primary_genre)))\n",
    "    book_gen = book_gen[book_gen[\"_primary\"] != \"\"].copy()\n",
    "\n",
    "    # all unique books and per-genre positive book lists\n",
    "    all_books = sorted(book_gen[BOOK_COL].astype(int).unique().tolist())\n",
    "    per_genre = (\n",
    "        book_gen.groupby(\"_primary\")[BOOK_COL]\n",
    "        .apply(lambda s: sorted(pd.Series(s.unique()).astype(int).tolist()))\n",
    "        .to_frame(\"pos_books\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    per_genre[\"n_pos\"] = per_genre[\"pos_books\"].apply(len)\n",
    "\n",
    "    target_genres = sorted(per_genre[\"_primary\"].tolist(), key=lambda x: x.lower())\n",
    "\n",
    "    # ID block to avoid collisions\n",
    "    BLOCK = 1_000_000\n",
    "\n",
    "    # logging\n",
    "    rows_summary = []\n",
    "    with open(SUMMARY_TXT, \"w\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=== BASELINE ===\\n\")\n",
    "        log.write(f\"üë§ Unique users: {baseline_users:,}\\n\")\n",
    "        log.write(f\"üßæ Rows: {baseline_rows:,}\\n\")\n",
    "        log.write(f\"üî¢ Synthetic user_id base start: {base_start_uid}\\n\")\n",
    "        log.write(f\"ZERO_MODE={ZERO_MODE} | NEG_RATIO={NEG_RATIO} | RNG_SEED={RNG_SEED}\\n\")\n",
    "        log.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    grand_added = 0\n",
    "    made_any = False\n",
    "\n",
    "    for gi, g in enumerate(target_genres):\n",
    "        pos_books = per_genre.loc[per_genre[\"_primary\"] == g, \"pos_books\"].iloc[0]\n",
    "        n_pos     = int(per_genre.loc[per_genre[\"_primary\"] == g, \"n_pos\"].iloc[0])\n",
    "        if n_pos == 0:\n",
    "            continue\n",
    "\n",
    "        pos_set = set(pos_books)\n",
    "        neg_pool = [b for b in all_books if b not in pos_set]\n",
    "\n",
    "        safe_g = sanitize_fn(g)\n",
    "        with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"üé≠ {g} | positives (primary-genre books) = {n_pos} | neg_pool = {len(neg_pool)}\\n\")\n",
    "\n",
    "        for r_i, run in enumerate(RUNS):\n",
    "            start_uid = base_start_uid + gi * (len(RUNS) * BLOCK) + r_i * BLOCK\n",
    "            new_uids = list(range(start_uid, start_uid + run))\n",
    "\n",
    "            # ----- choose negatives (either ALL or sampled) -----\n",
    "            if ZERO_MODE == \"all\":\n",
    "                neg_books_for_all_users = neg_pool  # WARNING: huge\n",
    "            else:\n",
    "                # sample a fixed subset once per (genre, run), same for all new users (fast & reproducible)\n",
    "                target_neg = min(len(neg_pool), NEG_RATIO * n_pos)\n",
    "                rng = random.Random(RNG_SEED + gi*1000 + r_i)\n",
    "                neg_books_for_all_users = rng.sample(neg_pool, target_neg) if target_neg > 0 else []\n",
    "\n",
    "            n_neg = len(neg_books_for_all_users)\n",
    "\n",
    "            # ----- build synthetic block -----\n",
    "            # Positives (5)\n",
    "            pos_rows = {\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_pos)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in pos_books],\n",
    "                RATING_COL: [POS_RATING] * (run * n_pos),\n",
    "                GENRE_COL:  [book_to_genres.get(b, \"\") for _ in new_uids for b in pos_books],\n",
    "            }\n",
    "\n",
    "            # Negatives (0)\n",
    "            neg_rows = {\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_neg)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in neg_books_for_all_users],\n",
    "                RATING_COL: [NEG_RATING] * (run * n_neg),\n",
    "                GENRE_COL:  [book_to_genres.get(b, \"\") for _ in new_uids for b in neg_books_for_all_users],\n",
    "            }\n",
    "\n",
    "            synth_df = pd.concat([pd.DataFrame(pos_rows), pd.DataFrame(neg_rows)], ignore_index=True)\n",
    "            expected_added = run * (n_pos + n_neg)\n",
    "\n",
    "            # quick check\n",
    "            assert len(synth_df) == expected_added, f\"Row count mismatch for {g}, run={run}\"\n",
    "\n",
    "            # combine and save\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "            new_users_total = combined[USER_COL].nunique()\n",
    "\n",
    "            out_path = OUT_DIR / f\"f_{safe_g}_{run}_pos7_neg0_{ZERO_MODE}.csv\"\n",
    "            combined.to_csv(out_path, index=False)\n",
    "\n",
    "            with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"  run={str(run):>5} ‚Üí +rows={expected_added:>12,} \"\n",
    "                          f\"(pos={run*n_pos:,}, neg={run*n_neg:,}) | \"\n",
    "                          f\"new_rows={len(combined):,} | new_users={new_users_total:,}\\n\")\n",
    "\n",
    "            rows_summary.append({\n",
    "                \"genre\": g,\n",
    "                \"safe_genre\": safe_g,\n",
    "                \"run_users\": run,\n",
    "                \"n_pos_books\": n_pos,\n",
    "                \"n_neg_books_per_user\": n_neg if ZERO_MODE==\"all\" else n_neg,\n",
    "                \"rows_added\": expected_added,\n",
    "                \"rows_pos\": run*n_pos,\n",
    "                \"rows_neg\": run*n_neg,\n",
    "                \"zero_mode\": ZERO_MODE,\n",
    "                \"neg_ratio\": NEG_RATIO if ZERO_MODE==\"sample\" else None,\n",
    "                \"output_csv\": str(out_path)\n",
    "            })\n",
    "\n",
    "            grand_added += expected_added\n",
    "            made_any = True\n",
    "\n",
    "        with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(\"\\n\")\n",
    "\n",
    "    if rows_summary:\n",
    "        pd.DataFrame(rows_summary).to_csv(SUMMARY_CSV, index=False)\n",
    "\n",
    "    with open(SUMMARY_TXT, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=\"*80 + \"\\n\")\n",
    "        log.write(f\"Grand total injected rows (all genres & runs): {grand_added:,}\\n\")\n",
    "        log.write(f\"Outputs folder: {OUT_DIR}\\n\")\n",
    "        log.write(f\"Per-run summary CSV: {SUMMARY_CSV}\\n\")\n",
    "\n",
    "    if not made_any:\n",
    "        print(\"‚ö†Ô∏è No datasets were produced. Check genre names / columns.\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Done.\")\n",
    "        print(\"  ‚Ä¢ Datasets:\", OUT_DIR)\n",
    "        print(\"  ‚Ä¢ Summary:\", SUMMARY_TXT)\n",
    "        print(\"  ‚Ä¢ Summary CSV:\", SUMMARY_CSV)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
