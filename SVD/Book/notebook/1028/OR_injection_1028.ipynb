{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## injection analogy: OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all books that have Fantasy OR Adventure â€” meaning books that have either one or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR set = (books with G1) âˆª (books with G2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§® Therefore:\n",
    "\n",
    "The number of books injected per pair (OR)\n",
    "â‰ˆ size of single(G1) + size of single(G2) âˆ’ overlap(G1,G2)\n",
    "\n",
    "So if you already had single-genre injections and pair-AND injections:\n",
    "\n",
    "Single injections â†’ each G alone\n",
    "\n",
    "Pair-AND injections â†’ intersection\n",
    "\n",
    "Pair-OR injections â†’ union = singles combined minus AND overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Case            | Mathematical Meaning   | Dataset Size   | Interpretation      |\n",
      "|:----------------|:-----------------------|:---------------|:--------------------|\n",
      "| Single(G1)      | A                      | Medium         | Likes genre A       |\n",
      "| Pair(G1,G2) AND | A âˆ© B                  | Small          | Likes both together |\n",
      "| Pair(G1,G2) OR  | A âˆª B                  | Large          | Likes either genre  |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    [\"Single(G1)\", \"A\", \"Medium\", \"Likes genre A\"],\n",
    "    [\"Pair(G1,G2) AND\", \"A âˆ© B\", \"Small\", \"Likes both together\"],\n",
    "    [\"Pair(G1,G2) OR\", \"A âˆª B\", \"Large\", \"Likes either genre\"]\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Case\", \"Mathematical Meaning\", \"Dataset Size\", \"Interpretation\"])\n",
    "\n",
    "# Display\n",
    "print(df.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original CSV...\n",
      "#books in Adult=331 | #books in Adventure=1,789 | #Adult&Adventure=0 | total OR=2,120 | neg_pool=7,295\n",
      "âœ… Completed injection file (OR): forpair_Adult__Adventure_2u_pos5_neg1_OR.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1227912/1601777176.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1227912/1601777176.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mbase_start_uid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUSER_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mrun_for_pos5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_start_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1227912/1601777176.py\u001b[0m in \u001b[0;36mrun_for_pos5\u001b[0;34m(df, base_start_uid)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mout_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"forpair_{safe_p}_{run_users}u_pos{pos_rating}_neg{NEG_RATING}_OR.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_base\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".gz\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mCOMPRESSION\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOMPRESSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… Completed injection file (OR): {out_path.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3549\u001b[0m         )\n\u001b[1;32m   3550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3551\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3552\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         )\n\u001b[0;32m-> 1180\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m             )\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         libwriters.write_csv_rows(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_pair_bias_pos5_neg1_all_smallcohorts_OR_allpairs.py\n",
    "# Date: 2025-10-28\n",
    "#\n",
    "# What it does:\n",
    "#   â€¢ Uses OR logic for positives (book has g1 OR g2).\n",
    "#   â€¢ Processes ALL genre pairs (no adult-only filtering).\n",
    "#   â€¢ Saves under /1028/... to keep separate from previous runs.\n",
    "#   â€¢ Prints and logs counts: |G1|, |G2|, |G1âˆ©G2|, |G1âˆªG2|, neg_pool size.\n",
    "#   â€¢ Distinguishes files with prefix \"forpair_\" and suffix \"_OR\".\n",
    "#\n",
    "# Tip: Set COMPRESSION=\"gzip\" if you later want to save disk space.\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "INPUT_CSV   = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "BASE_OUT_DIR= Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1028/data/PAIR_INJECTION\")\n",
    "\n",
    "GENRE_COL   = \"genres\"\n",
    "USER_COL    = \"user_id\"\n",
    "BOOK_COL    = \"book_id\"\n",
    "RATING_COL  = \"rating\"\n",
    "\n",
    "RUN_USERS   = [2, 4, 6, 25, 50, 100, 200, 300, 500, 1000]\n",
    "ZERO_MODE   = \"all\"   # negatives per user: \"all\" means every non-positive book\n",
    "NEG_RATING  = 1\n",
    "BLOCK       = 1_000_000\n",
    "\n",
    "# Write gzip-compressed CSVs (set to None to disable)\n",
    "COMPRESSION = None  # or \"gzip\"\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def parse_genres(cell: str):\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # try list/tuple literal first\n",
    "    if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "        try:\n",
    "            import ast\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # else, split by common separators\n",
    "    for sep in [\",\", \"|\", \";\", \"//\", \"/\"]:\n",
    "        if sep in s:\n",
    "            parts = [p.strip() for p in s.split(sep) if p.strip()]\n",
    "            seen, out = set(), []\n",
    "            for p in parts:\n",
    "                if p not in seen:\n",
    "                    out.append(p); seen.add(p)\n",
    "            return out\n",
    "    return [s]\n",
    "\n",
    "def prepare_books(df: pd.DataFrame):\n",
    "    books = df[[BOOK_COL, GENRE_COL]].drop_duplicates(subset=[BOOK_COL]).copy()\n",
    "    books[\"genre_list\"] = books[GENRE_COL].apply(parse_genres)\n",
    "    books = books[books[\"genre_list\"].map(len) > 0].copy()\n",
    "    book_to_list = dict(zip(books[BOOK_COL].astype(int), books[\"genre_list\"]))\n",
    "    book_to_set  = {int(b): set(l) for b, l in book_to_list.items()}\n",
    "    all_books = sorted(book_to_list.keys())\n",
    "    return all_books, book_to_list, book_to_set\n",
    "\n",
    "# ========= GENERATOR (pos=5 only, OR logic) =========\n",
    "def run_for_pos5(df: pd.DataFrame, base_start_uid: int):\n",
    "    pos_rating = 5\n",
    "    out_dir = BASE_OUT_DIR / str(pos_rating)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_books, book_to_list, book_to_set = prepare_books(df)\n",
    "    GENRES = sorted({g for gl in book_to_list.values() for g in gl})\n",
    "\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "\n",
    "    summary_txt = out_dir / \"summary.txt\"\n",
    "    summary_csv = out_dir / \"summary.csv\"\n",
    "    pairs_overview_csv = out_dir / \"pairs_overview.csv\"\n",
    "    missing_pairs_csv = out_dir / \"missing_pairs.csv\"\n",
    "\n",
    "    # ----- Pair lists -----\n",
    "    all_pairs = list(combinations(GENRES, 2))\n",
    "    total_pairs = len(all_pairs)\n",
    "\n",
    "    # Count how many pairs have â‰¥1 positive book under OR (union)\n",
    "    def count_pairs_with_positives_or(pairs):\n",
    "        cnt = 0\n",
    "        for g1, g2 in pairs:\n",
    "            n_pos_or = sum(1 for b in all_books if (g1 in book_to_set[b] or g2 in book_to_set[b]))\n",
    "            if n_pos_or > 0:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "\n",
    "    total_pairs_with_pos_or = count_pairs_with_positives_or(all_pairs)\n",
    "\n",
    "    with open(summary_txt, \"w\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=== BASELINE ===\\n\")\n",
    "        log.write(f\"ðŸ‘¤ Unique users: {baseline_users:,}\\n\")\n",
    "        log.write(f\"ðŸ§¾ Rows: {baseline_rows:,}\\n\")\n",
    "        log.write(f\"POS_RATING={pos_rating} | ZERO_MODE={ZERO_MODE} | NEG_RATING={NEG_RATING}\\n\")\n",
    "        log.write(f\"Discovered genres ({len(GENRES)}): {GENRES}\\n\\n\")\n",
    "        log.write(\"=== PAIR COUNTS (OR / union, pre-filter) ===\\n\")\n",
    "        log.write(f\"All pairs (combinatorial): {total_pairs:,}\\n\")\n",
    "        log.write(f\"All pairs with â‰¥1 OR-positive book: {total_pairs_with_pos_or:,}\\n\\n\")\n",
    "        log.write(\"Processing mode: ALL pairs (OR / union).\\n\\n\")\n",
    "\n",
    "    rows_summary = []\n",
    "    pairs_overview_rows = []\n",
    "    missing_pairs = []\n",
    "\n",
    "    # ----- Process ALL pairs -----\n",
    "    pair_index = 0\n",
    "    for g1, g2 in all_pairs:\n",
    "        # === OR positives: books having g1 OR g2 ===\n",
    "        books_g1   = [b for b in all_books if g1 in book_to_set[b]]\n",
    "        books_g2   = [b for b in all_books if g2 in book_to_set[b]]\n",
    "        books_both = [b for b in all_books if (g1 in book_to_set[b] and g2 in book_to_set[b])]\n",
    "        pos_books  = sorted(set(books_g1) | set(books_g2))  # union (OR)\n",
    "\n",
    "        n_g1   = len(books_g1)\n",
    "        n_g2   = len(books_g2)\n",
    "        n_both = len(books_both)\n",
    "        n_pos  = len(pos_books)\n",
    "\n",
    "        neg_pool = [b for b in all_books if b not in pos_books]\n",
    "        n_neg_pool = len(neg_pool)\n",
    "\n",
    "        # Per-pair overview row\n",
    "        pairs_overview_rows.append({\n",
    "            \"pair\": f\"{g1} OR {g2}\",\n",
    "            \"g1\": g1,\n",
    "            \"g2\": g2,\n",
    "            \"n_books_g1\": n_g1,\n",
    "            \"n_books_g2\": n_g2,\n",
    "            \"n_books_both_AND\": n_both,\n",
    "            \"n_pos_books_OR\": n_pos,\n",
    "            \"neg_pool\": n_neg_pool\n",
    "        })\n",
    "\n",
    "        if n_pos == 0:\n",
    "            missing_pairs.append({\"pair\": f\"{g1} OR {g2}\", \"g1\": g1, \"g2\": g2})\n",
    "            msg = f\"(skip) No OR-positive books for pair: {g1} OR {g2}\"\n",
    "            print(msg)\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(msg + \"\\n\")\n",
    "            pair_index += 1\n",
    "            continue\n",
    "\n",
    "        # Print + log the counts in your requested format\n",
    "        human_counts = (\n",
    "            f\"#books in {g1}={n_g1:,} | #books in {g2}={n_g2:,} | \"\n",
    "            f\"#{g1}&{g2}={n_both:,} | total OR={n_pos:,} | neg_pool={n_neg_pool:,}\"\n",
    "        )\n",
    "        print(human_counts)\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(human_counts + \"\\n\")\n",
    "\n",
    "        safe_p = f\"{sanitize_fn(g1)}__{sanitize_fn(g2)}\"\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"ðŸ”— Pair (OR): {g1} OR {g2}\\n\")\n",
    "\n",
    "        neg_books_for_all_users = neg_pool  # ZERO_MODE == \"all\"\n",
    "\n",
    "        for run_idx, run_users in enumerate(RUN_USERS):\n",
    "            start_uid = base_start_uid + pair_index * (len(RUN_USERS) * BLOCK) + run_idx * BLOCK\n",
    "            new_uids = list(range(start_uid, start_uid + run_users))\n",
    "\n",
    "            # synth positives (OR)\n",
    "            df_pos = pd.DataFrame({\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_pos)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in pos_books],\n",
    "                RATING_COL: [pos_rating] * (run_users * n_pos),\n",
    "                GENRE_COL:  [\",\".join(sorted(book_to_list.get(b, []))) for _ in new_uids for b in pos_books]\n",
    "            })\n",
    "\n",
    "            # synth negatives\n",
    "            n_neg = len(neg_books_for_all_users)\n",
    "            df_neg = pd.DataFrame({\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_neg)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in neg_books_for_all_users],\n",
    "                RATING_COL: [NEG_RATING] * (run_users * n_neg),\n",
    "                GENRE_COL:  [\",\".join(sorted(book_to_list.get(b, []))) for _ in new_uids for b in neg_books_for_all_users]\n",
    "            })\n",
    "\n",
    "            synth_df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            # filename + optional compression\n",
    "            out_base = f\"forpair_{safe_p}_{run_users}u_pos{pos_rating}_neg{NEG_RATING}_OR.csv\"\n",
    "            out_path = out_dir / (out_base + (\".gz\" if COMPRESSION else \"\"))\n",
    "            combined.to_csv(out_path, index=False, compression=COMPRESSION)\n",
    "\n",
    "            print(f\"âœ… Completed injection file (OR): {out_path.name}\")\n",
    "\n",
    "            rows_added = len(synth_df)\n",
    "            rows_pos = len(df_pos)\n",
    "            rows_neg = len(df_neg)\n",
    "            new_users_total = combined[USER_COL].nunique()\n",
    "\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(\n",
    "                    f\"  users={run_users:>5} â†’ +rows={rows_added:>12,} \"\n",
    "                    f\"(pos={rows_pos:,}, neg={rows_neg:,}) | \"\n",
    "                    f\"new_rows={len(combined):,} | new_users={new_users_total:,} | \"\n",
    "                    f\"outfile={out_path.name}\\n\"\n",
    "                )\n",
    "\n",
    "            rows_summary.append({\n",
    "                \"pos_rating\": pos_rating,\n",
    "                \"pair_or\": f\"{g1} OR {g2}\",\n",
    "                \"g1\": g1,\n",
    "                \"g2\": g2,\n",
    "                \"run_users\": run_users,\n",
    "                \"n_books_g1\": n_g1,\n",
    "                \"n_books_g2\": n_g2,\n",
    "                \"n_books_both_AND\": n_both,\n",
    "                \"n_pos_books_OR\": n_pos,\n",
    "                \"n_neg_books_per_user\": n_neg,\n",
    "                \"rows_added\": rows_added,\n",
    "                \"rows_pos\": rows_pos,\n",
    "                \"rows_neg\": rows_neg,\n",
    "                \"zero_mode\": ZERO_MODE,\n",
    "                \"output_csv\": str(out_path)\n",
    "            })\n",
    "\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(\"\\n\")\n",
    "\n",
    "        pair_index += 1\n",
    "\n",
    "    # ----- Outputs -----\n",
    "    if rows_summary:\n",
    "        pd.DataFrame(rows_summary).to_csv(summary_csv, index=False)\n",
    "    if pairs_overview_rows:\n",
    "        pd.DataFrame(pairs_overview_rows).sort_values([\"g1\",\"g2\"]).to_csv(pairs_overview_csv, index=False)\n",
    "    if missing_pairs:\n",
    "        pd.DataFrame(missing_pairs).to_csv(missing_pairs_csv, index=False)\n",
    "\n",
    "    with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=\"*80 + \"\\n\")\n",
    "        log.write(f\"Grand total injected rows (ALL pairs, OR, pos=5): {sum(r['rows_added'] for r in rows_summary):,}\\n\")\n",
    "        log.write(f\"Pairs overview (ALL pairs, OR): {pairs_overview_csv}\\n\")\n",
    "        log.write(f\"Missing pairs (ALL pairs, OR): {missing_pairs_csv}\\n\\n\")\n",
    "\n",
    "    print(f\"âœ… Done for pos=5 (ALL pairs, OR). Out: {out_dir}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Loading original CSV...\")\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    df[USER_COL]   = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]   = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "    df[GENRE_COL]  = df[GENRE_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "    run_for_pos5(df, base_start_uid)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
