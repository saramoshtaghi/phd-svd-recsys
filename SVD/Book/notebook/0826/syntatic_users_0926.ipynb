{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE ===\n",
      "👤 Unique users: 53,424\n",
      "🧾 Rows: 5,976,479\n",
      "🔢 Synthetic user_id base start: 53425\n",
      "================================================================================\n",
      "\n",
      "🎭 Adult | run=25\n",
      "   • n_books=106, cap_g=41, records_added=1025\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adult_25.csv\n",
      "\n",
      "🎭 Adult | run=50\n",
      "   • n_books=106, cap_g=41, records_added=2050\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adult_50.csv\n",
      "\n",
      "🎭 Adult | run=100\n",
      "   • n_books=106, cap_g=41, records_added=4100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adult_100.csv\n",
      "\n",
      "🎭 Adult | run=200\n",
      "   • n_books=106, cap_g=41, records_added=8200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adult_200.csv\n",
      "\n",
      "🎭 Adventure | run=25\n",
      "   • n_books=185, cap_g=52, records_added=1300\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adventure_25.csv\n",
      "\n",
      "🎭 Adventure | run=50\n",
      "   • n_books=185, cap_g=52, records_added=2600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adventure_50.csv\n",
      "\n",
      "🎭 Adventure | run=100\n",
      "   • n_books=185, cap_g=52, records_added=5200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adventure_100.csv\n",
      "\n",
      "🎭 Adventure | run=200\n",
      "   • n_books=185, cap_g=52, records_added=10400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adventure_200.csv\n",
      "\n",
      "🎭 Children's | run=25\n",
      "   • n_books=694, cap_g=92, records_added=2300\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Children_s_25.csv\n",
      "\n",
      "🎭 Children's | run=50\n",
      "   • n_books=694, cap_g=92, records_added=4600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Children_s_50.csv\n",
      "\n",
      "🎭 Children's | run=100\n",
      "   • n_books=694, cap_g=92, records_added=9200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Children_s_100.csv\n",
      "\n",
      "🎭 Children's | run=200\n",
      "   • n_books=694, cap_g=92, records_added=18400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Children_s_200.csv\n",
      "\n",
      "🎭 Classics | run=25\n",
      "   • n_books=392, cap_g=71, records_added=1775\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Classics_25.csv\n",
      "\n",
      "🎭 Classics | run=50\n",
      "   • n_books=392, cap_g=71, records_added=3550\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Classics_50.csv\n",
      "\n",
      "🎭 Classics | run=100\n",
      "   • n_books=392, cap_g=71, records_added=7100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Classics_100.csv\n",
      "\n",
      "🎭 Classics | run=200\n",
      "   • n_books=392, cap_g=71, records_added=14200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Classics_200.csv\n",
      "\n",
      "🎭 Drama | run=25\n",
      "   • n_books=229, cap_g=56, records_added=1400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Drama_25.csv\n",
      "\n",
      "🎭 Drama | run=50\n",
      "   • n_books=229, cap_g=56, records_added=2800\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Drama_50.csv\n",
      "\n",
      "🎭 Drama | run=100\n",
      "   • n_books=229, cap_g=56, records_added=5600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Drama_100.csv\n",
      "\n",
      "🎭 Drama | run=200\n",
      "   • n_books=229, cap_g=56, records_added=11200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Drama_200.csv\n",
      "\n",
      "🎭 Fantasy | run=25\n",
      "   • n_books=1794, cap_g=144, records_added=3600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Fantasy_25.csv\n",
      "\n",
      "🎭 Fantasy | run=50\n",
      "   • n_books=1794, cap_g=144, records_added=7200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Fantasy_50.csv\n",
      "\n",
      "🎭 Fantasy | run=100\n",
      "   • n_books=1794, cap_g=144, records_added=14400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Fantasy_100.csv\n",
      "\n",
      "🎭 Fantasy | run=200\n",
      "   • n_books=1794, cap_g=144, records_added=28800\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Fantasy_200.csv\n",
      "\n",
      "🎭 Historical | run=25\n",
      "   • n_books=497, cap_g=79, records_added=1975\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Historical_25.csv\n",
      "\n",
      "🎭 Historical | run=50\n",
      "   • n_books=497, cap_g=79, records_added=3950\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Historical_50.csv\n",
      "\n",
      "🎭 Historical | run=100\n",
      "   • n_books=497, cap_g=79, records_added=7900\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Historical_100.csv\n",
      "\n",
      "🎭 Historical | run=200\n",
      "   • n_books=497, cap_g=79, records_added=15800\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Historical_200.csv\n",
      "\n",
      "🎭 Horror | run=25\n",
      "   • n_books=427, cap_g=74, records_added=1850\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Horror_25.csv\n",
      "\n",
      "🎭 Horror | run=50\n",
      "   • n_books=427, cap_g=74, records_added=3700\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Horror_50.csv\n",
      "\n",
      "🎭 Horror | run=100\n",
      "   • n_books=427, cap_g=74, records_added=7400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Horror_100.csv\n",
      "\n",
      "🎭 Horror | run=200\n",
      "   • n_books=427, cap_g=74, records_added=14800\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Horror_200.csv\n",
      "\n",
      "🎭 Mystery | run=25\n",
      "   • n_books=1315, cap_g=124, records_added=3100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Mystery_25.csv\n",
      "\n",
      "🎭 Mystery | run=50\n",
      "   • n_books=1315, cap_g=124, records_added=6200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Mystery_50.csv\n",
      "\n",
      "🎭 Mystery | run=100\n",
      "   • n_books=1315, cap_g=124, records_added=12400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Mystery_100.csv\n",
      "\n",
      "🎭 Mystery | run=200\n",
      "   • n_books=1315, cap_g=124, records_added=24800\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Mystery_200.csv\n",
      "\n",
      "🎭 Nonfiction | run=25\n",
      "   • n_books=878, cap_g=103, records_added=2575\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Nonfiction_25.csv\n",
      "\n",
      "🎭 Nonfiction | run=50\n",
      "   • n_books=878, cap_g=103, records_added=5150\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Nonfiction_50.csv\n",
      "\n",
      "🎭 Nonfiction | run=100\n",
      "   • n_books=878, cap_g=103, records_added=10300\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Nonfiction_100.csv\n",
      "\n",
      "🎭 Nonfiction | run=200\n",
      "   • n_books=878, cap_g=103, records_added=20600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Nonfiction_200.csv\n",
      "\n",
      "🎭 Romance | run=25\n",
      "   • n_books=1704, cap_g=140, records_added=3500\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Romance_25.csv\n",
      "\n",
      "🎭 Romance | run=50\n",
      "   • n_books=1704, cap_g=140, records_added=7000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Romance_50.csv\n",
      "\n",
      "🎭 Romance | run=100\n",
      "   • n_books=1704, cap_g=140, records_added=14000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Romance_100.csv\n",
      "\n",
      "🎭 Romance | run=200\n",
      "   • n_books=1704, cap_g=140, records_added=28000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Romance_200.csv\n",
      "\n",
      "🎭 Science Fiction | run=25\n",
      "   • n_books=776, cap_g=97, records_added=2425\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Science_Fiction_25.csv\n",
      "\n",
      "🎭 Science Fiction | run=50\n",
      "   • n_books=776, cap_g=97, records_added=4850\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Science_Fiction_50.csv\n",
      "\n",
      "🎭 Science Fiction | run=100\n",
      "   • n_books=776, cap_g=97, records_added=9700\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Science_Fiction_100.csv\n",
      "\n",
      "🎭 Science Fiction | run=200\n",
      "   • n_books=776, cap_g=97, records_added=19400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Science_Fiction_200.csv\n",
      "\n",
      "🎭 Thriller | run=25\n",
      "   • n_books=418, cap_g=73, records_added=1825\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Thriller_25.csv\n",
      "\n",
      "🎭 Thriller | run=50\n",
      "   • n_books=418, cap_g=73, records_added=3650\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Thriller_50.csv\n",
      "\n",
      "🎭 Thriller | run=100\n",
      "   • n_books=418, cap_g=73, records_added=7300\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Thriller_100.csv\n",
      "\n",
      "🎭 Thriller | run=200\n",
      "   • n_books=418, cap_g=73, records_added=14600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Thriller_200.csv\n",
      "\n",
      "✅ Done. Datasets saved under: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_0926_datasets.py\n",
    "\n",
    "import os, re, math\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "BASE_DIR   = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book\")\n",
    "INPUT_CSV  = BASE_DIR / \"data/df_final_with_genres.csv\"  # must have: user_id, book_id, rating, genres\n",
    "OUT_DIR    = BASE_DIR / \"result/rec/top_re/0926\"\n",
    "GENRE_COL  = \"genres\"\n",
    "USER_COL   = \"user_id\"\n",
    "BOOK_COL   = \"book_id\"\n",
    "RATING_COL = \"rating\"\n",
    "\n",
    "RUNS = [25, 100, 200, 1000]\n",
    "SYNTH_RATING = 5\n",
    "\n",
    "# cap_g = min(n_books, round(alpha * sqrt(n_books)) + bias)\n",
    "ALPHA = 3.2\n",
    "BIAS  = 8\n",
    "# =========================\n",
    "\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def primary_genre(s: str) -> str:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return \"\"\n",
    "    return s.split(\",\")[0].strip()\n",
    "\n",
    "def compute_cap(n_books: int) -> int:\n",
    "    if n_books <= 0: return 0\n",
    "    cap = int(round(ALPHA * math.sqrt(n_books)) + BIAS)\n",
    "    cap = max(10, min(cap, n_books))  # at least 10, never more than n_books\n",
    "    return cap\n",
    "\n",
    "def main():\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---------- Load ----------\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    # numeric IDs\n",
    "    df[USER_COL]  = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]  = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL]= pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "\n",
    "    # baseline stats\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "\n",
    "    # ---------- Prepare primary-genre view ----------\n",
    "    work = df.copy()\n",
    "    work[GENRE_COL] = work[GENRE_COL].fillna(\"\").astype(str)\n",
    "    work[\"_primary\"] = work[GENRE_COL].apply(primary_genre)\n",
    "    work = work[work[\"_primary\"] != \"\"].copy()\n",
    "\n",
    "    # Per-primary-genre unique book lists\n",
    "    per_genre = (\n",
    "        work.groupby(\"_primary\")[BOOK_COL]\n",
    "            .apply(lambda s: sorted(pd.Series(s.unique()).astype(int).tolist()))\n",
    "            .to_frame(\"book_list\")\n",
    "            .reset_index()\n",
    "    )\n",
    "    per_genre[\"n_books\"] = per_genre[\"book_list\"].apply(len)\n",
    "\n",
    "    # Fixed ordered list of genres to try to cover all 13\n",
    "    target_genres = [\n",
    "        \"Adult\",\"Adventure\",\"Children's\",\"Classics\",\"Drama\",\"Fantasy\",\n",
    "        \"Historical\",\"Horror\",\"Mystery\",\"Nonfiction\",\"Romance\",\"Science Fiction\",\"Thriller\"\n",
    "    ]\n",
    "    # Map normalization for matching\n",
    "    def norm(s): return s.lower().replace(\"_\",\" \").replace(\"’\",\"'\").strip()\n",
    "\n",
    "    # Build an index for quick lookup by normalized primary genre\n",
    "    idx = { norm(g): g for g in per_genre[\"_primary\"] }\n",
    "    # Helper to find a canonical match in per_genre even if spelling differs\n",
    "    def pick_row_for(need):\n",
    "        key = norm(need)\n",
    "        # exact\n",
    "        if key in idx:\n",
    "            return per_genre[per_genre[\"_primary\"] == idx[key]].iloc[0]\n",
    "        # common fixes\n",
    "        aliases = {\n",
    "            \"science_fiction\":\"science fiction\",\n",
    "            \"children_s\":\"children's\",\n",
    "            \"childrens\":\"children's\",\n",
    "        }\n",
    "        key2 = aliases.get(key, key)\n",
    "        if key2 in idx:\n",
    "            return per_genre[per_genre[\"_primary\"] == idx[key2]].iloc[0]\n",
    "        # fallback: fuzzy-ish linear scan (contains)\n",
    "        for g in per_genre[\"_primary\"]:\n",
    "            if norm(g) == key or key in norm(g):\n",
    "                return per_genre[per_genre[\"_primary\"] == g].iloc[0]\n",
    "        return None\n",
    "\n",
    "    print(\"=== BASELINE ===\")\n",
    "    print(f\"👤 Unique users: {baseline_users:,}\")\n",
    "    print(f\"🧾 Rows: {baseline_rows:,}\")\n",
    "    print(f\"🔢 Synthetic user_id base start: {base_start_uid}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # We allocate a disjoint user-id block per (genre, run):\n",
    "    # block_size = 1_000_000 to guarantee no collisions even across big runs.\n",
    "    BLOCK = 1_000_000\n",
    "\n",
    "    made_any = False\n",
    "    for gi, g in enumerate(target_genres):\n",
    "        row = pick_row_for(g)\n",
    "        if row is None:\n",
    "            print(f\"⚠️  Skipping genre not found in primary-genre index: {g}\")\n",
    "            continue\n",
    "\n",
    "        book_list = list(row[\"book_list\"])\n",
    "        n_books   = int(row[\"n_books\"])\n",
    "        if n_books <= 0 or not book_list:\n",
    "            print(f\"⚠️  Skipping {g}: no books.\")\n",
    "            continue\n",
    "\n",
    "        cap_g = compute_cap(n_books)\n",
    "        # Take the first cap_g books (deterministic, stable)\n",
    "        picked_books = book_list[:cap_g]\n",
    "\n",
    "        for run in RUNS:\n",
    "            # Disjoint synthetic user ids for this (genre, run)\n",
    "            block_offset = gi * (len(RUNS) * BLOCK) + (RUNS.index(run) * BLOCK)\n",
    "            start_uid = base_start_uid + block_offset\n",
    "            new_uids = list(range(start_uid, start_uid + run))\n",
    "\n",
    "            # Build synthetic block\n",
    "            synth = {\n",
    "                USER_COL:  [],\n",
    "                BOOK_COL:  [],\n",
    "                RATING_COL:[],\n",
    "                GENRE_COL: []\n",
    "            }\n",
    "            for uid in new_uids:\n",
    "                synth[USER_COL].extend([uid] * len(picked_books))\n",
    "                synth[BOOK_COL].extend(picked_books)\n",
    "                synth[RATING_COL].extend([SYNTH_RATING] * len(picked_books))\n",
    "                # keep original full genre string for each book\n",
    "                # fetch once via a lookup table for speed\n",
    "            # Precompute book -> genres mapping\n",
    "            genres_lookup = dict(df[[BOOK_COL, GENRE_COL]].drop_duplicates().values)\n",
    "            synth[GENRE_COL].extend([genres_lookup.get(b, \"\") for _ in new_uids for b in picked_books])\n",
    "\n",
    "            synth_df = pd.DataFrame(synth)\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            # Validity checks\n",
    "            exp_rows = run * len(picked_books)\n",
    "            assert len(synth_df) == exp_rows, f\"Bad synth rows for {g}, run={run}\"\n",
    "            assert combined[USER_COL].nunique() >= baseline_users + 1, \"No new users added?\"\n",
    "\n",
    "            safe_g = sanitize_fn(g)\n",
    "            out_path = OUT_DIR / f\"p_{safe_g}_{run}.csv\"\n",
    "            combined.to_csv(out_path, index=False)\n",
    "\n",
    "            print(f\"\\n🎭 {g} | run={run}\")\n",
    "            print(f\"   • n_books={n_books}, cap_g={cap_g}, records_added={exp_rows}\")\n",
    "            print(f\"     💾 Saved → {out_path}\")\n",
    "            made_any = True\n",
    "\n",
    "    if not made_any:\n",
    "        print(\"⚠️  No datasets were produced. Check genre names and input columns.\")\n",
    "    else:\n",
    "        print(\"\\n✅ Done. Datasets saved under:\", OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE ===\n",
      "👤 Unique users: 53,424\n",
      "🧾 Rows: 5,976,479\n",
      "🔢 Synthetic user_id base start: 53425\n",
      "================================================================================\n",
      "\n",
      "🎭 Adult | run=25\n",
      "   • n_books=106, cap_g(B)=41, M=66, core(c)=14, tail(r)=27, stride=2\n",
      "   • records_added=1025, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Adult_run25_B41_M66_c14_r27_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Adult | run=100\n",
      "   • n_books=106, cap_g(B)=41, M=66, core(c)=14, tail(r)=27, stride=2\n",
      "   • records_added=4100, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Adult_run100_B41_M66_c14_r27_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Adult | run=200\n",
      "   • n_books=106, cap_g(B)=41, M=66, core(c)=14, tail(r)=27, stride=2\n",
      "   • records_added=8200, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Adult_run200_B41_M66_c14_r27_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Adult | run=1000\n",
      "   • n_books=106, cap_g(B)=41, M=66, core(c)=14, tail(r)=27, stride=2\n",
      "   • records_added=41000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Adult_run1000_B41_M66_c14_r27_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Adventure | run=25\n",
      "   • n_books=185, cap_g(B)=52, M=83, core(c)=18, tail(r)=34, stride=3\n",
      "   • records_added=1300, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Adventure_run25_B52_M83_c18_r34_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Adventure | run=100\n",
      "   • n_books=185, cap_g(B)=52, M=83, core(c)=18, tail(r)=34, stride=3\n",
      "   • records_added=5200, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Adventure_run100_B52_M83_c18_r34_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Adventure | run=200\n",
      "   • n_books=185, cap_g(B)=52, M=83, core(c)=18, tail(r)=34, stride=3\n",
      "   • records_added=10400, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Adventure_run200_B52_M83_c18_r34_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Adventure | run=1000\n",
      "   • n_books=185, cap_g(B)=52, M=83, core(c)=18, tail(r)=34, stride=3\n",
      "   • records_added=52000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Adventure_run1000_B52_M83_c18_r34_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Children's | run=25\n",
      "   • n_books=694, cap_g(B)=92, M=147, core(c)=32, tail(r)=60, stride=5\n",
      "   • records_added=2300, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Children_s_run25_B92_M147_c32_r60_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Children's | run=100\n",
      "   • n_books=694, cap_g(B)=92, M=147, core(c)=32, tail(r)=60, stride=5\n",
      "   • records_added=9200, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Children_s_run100_B92_M147_c32_r60_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Children's | run=200\n",
      "   • n_books=694, cap_g(B)=92, M=147, core(c)=32, tail(r)=60, stride=5\n",
      "   • records_added=18400, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Children_s_run200_B92_M147_c32_r60_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Children's | run=1000\n",
      "   • n_books=694, cap_g(B)=92, M=147, core(c)=32, tail(r)=60, stride=5\n",
      "   • records_added=92000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Children_s_run1000_B92_M147_c32_r60_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Classics | run=25\n",
      "   • n_books=392, cap_g(B)=71, M=114, core(c)=25, tail(r)=46, stride=4\n",
      "   • records_added=1775, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Classics_run25_B71_M114_c25_r46_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Classics | run=100\n",
      "   • n_books=392, cap_g(B)=71, M=114, core(c)=25, tail(r)=46, stride=4\n",
      "   • records_added=7100, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Classics_run100_B71_M114_c25_r46_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Classics | run=200\n",
      "   • n_books=392, cap_g(B)=71, M=114, core(c)=25, tail(r)=46, stride=4\n",
      "   • records_added=14200, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Classics_run200_B71_M114_c25_r46_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Classics | run=1000\n",
      "   • n_books=392, cap_g(B)=71, M=114, core(c)=25, tail(r)=46, stride=4\n",
      "   • records_added=71000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Classics_run1000_B71_M114_c25_r46_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Drama | run=25\n",
      "   • n_books=229, cap_g(B)=56, M=90, core(c)=20, tail(r)=36, stride=3\n",
      "   • records_added=1400, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Drama_run25_B56_M90_c20_r36_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Drama | run=100\n",
      "   • n_books=229, cap_g(B)=56, M=90, core(c)=20, tail(r)=36, stride=3\n",
      "   • records_added=5600, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Drama_run100_B56_M90_c20_r36_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Drama | run=200\n",
      "   • n_books=229, cap_g(B)=56, M=90, core(c)=20, tail(r)=36, stride=3\n",
      "   • records_added=11200, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Drama_run200_B56_M90_c20_r36_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Drama | run=1000\n",
      "   • n_books=229, cap_g(B)=56, M=90, core(c)=20, tail(r)=36, stride=3\n",
      "   • records_added=56000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Drama_run1000_B56_M90_c20_r36_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Fantasy | run=25\n",
      "   • n_books=1794, cap_g(B)=144, M=230, core(c)=50, tail(r)=94, stride=7\n",
      "   • records_added=3600, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Fantasy_run25_B144_M230_c50_r94_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Fantasy | run=100\n",
      "   • n_books=1794, cap_g(B)=144, M=230, core(c)=50, tail(r)=94, stride=7\n",
      "   • records_added=14400, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Fantasy_run100_B144_M230_c50_r94_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Fantasy | run=200\n",
      "   • n_books=1794, cap_g(B)=144, M=230, core(c)=50, tail(r)=94, stride=7\n",
      "   • records_added=28800, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Fantasy_run200_B144_M230_c50_r94_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Fantasy | run=1000\n",
      "   • n_books=1794, cap_g(B)=144, M=230, core(c)=50, tail(r)=94, stride=7\n",
      "   • records_added=144000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Fantasy_run1000_B144_M230_c50_r94_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Historical | run=25\n",
      "   • n_books=497, cap_g(B)=79, M=126, core(c)=28, tail(r)=51, stride=4\n",
      "   • records_added=1975, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Historical_run25_B79_M126_c28_r51_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Historical | run=100\n",
      "   • n_books=497, cap_g(B)=79, M=126, core(c)=28, tail(r)=51, stride=4\n",
      "   • records_added=7900, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Historical_run100_B79_M126_c28_r51_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Historical | run=200\n",
      "   • n_books=497, cap_g(B)=79, M=126, core(c)=28, tail(r)=51, stride=4\n",
      "   • records_added=15800, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Historical_run200_B79_M126_c28_r51_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Historical | run=1000\n",
      "   • n_books=497, cap_g(B)=79, M=126, core(c)=28, tail(r)=51, stride=4\n",
      "   • records_added=79000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Historical_run1000_B79_M126_c28_r51_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Horror | run=25\n",
      "   • n_books=427, cap_g(B)=74, M=118, core(c)=26, tail(r)=48, stride=4\n",
      "   • records_added=1850, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Horror_run25_B74_M118_c26_r48_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Horror | run=100\n",
      "   • n_books=427, cap_g(B)=74, M=118, core(c)=26, tail(r)=48, stride=4\n",
      "   • records_added=7400, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Horror_run100_B74_M118_c26_r48_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Horror | run=200\n",
      "   • n_books=427, cap_g(B)=74, M=118, core(c)=26, tail(r)=48, stride=4\n",
      "   • records_added=14800, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Horror_run200_B74_M118_c26_r48_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Horror | run=1000\n",
      "   • n_books=427, cap_g(B)=74, M=118, core(c)=26, tail(r)=48, stride=4\n",
      "   • records_added=74000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Horror_run1000_B74_M118_c26_r48_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Mystery | run=25\n",
      "   • n_books=1315, cap_g(B)=124, M=198, core(c)=43, tail(r)=81, stride=6\n",
      "   • records_added=3100, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Mystery_run25_B124_M198_c43_r81_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Mystery | run=100\n",
      "   • n_books=1315, cap_g(B)=124, M=198, core(c)=43, tail(r)=81, stride=6\n",
      "   • records_added=12400, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Mystery_run100_B124_M198_c43_r81_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Mystery | run=200\n",
      "   • n_books=1315, cap_g(B)=124, M=198, core(c)=43, tail(r)=81, stride=6\n",
      "   • records_added=24800, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Mystery_run200_B124_M198_c43_r81_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Mystery | run=1000\n",
      "   • n_books=1315, cap_g(B)=124, M=198, core(c)=43, tail(r)=81, stride=6\n",
      "   • records_added=124000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Mystery_run1000_B124_M198_c43_r81_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Nonfiction | run=25\n",
      "   • n_books=878, cap_g(B)=103, M=165, core(c)=36, tail(r)=67, stride=5\n",
      "   • records_added=2575, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Nonfiction_run25_B103_M165_c36_r67_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Nonfiction | run=100\n",
      "   • n_books=878, cap_g(B)=103, M=165, core(c)=36, tail(r)=67, stride=5\n",
      "   • records_added=10300, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Nonfiction_run100_B103_M165_c36_r67_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Nonfiction | run=200\n",
      "   • n_books=878, cap_g(B)=103, M=165, core(c)=36, tail(r)=67, stride=5\n",
      "   • records_added=20600, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Nonfiction_run200_B103_M165_c36_r67_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Nonfiction | run=1000\n",
      "   • n_books=878, cap_g(B)=103, M=165, core(c)=36, tail(r)=67, stride=5\n",
      "   • records_added=103000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Nonfiction_run1000_B103_M165_c36_r67_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Romance | run=25\n",
      "   • n_books=1704, cap_g(B)=140, M=224, core(c)=49, tail(r)=91, stride=7\n",
      "   • records_added=3500, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Romance_run25_B140_M224_c49_r91_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Romance | run=100\n",
      "   • n_books=1704, cap_g(B)=140, M=224, core(c)=49, tail(r)=91, stride=7\n",
      "   • records_added=14000, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Romance_run100_B140_M224_c49_r91_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Romance | run=200\n",
      "   • n_books=1704, cap_g(B)=140, M=224, core(c)=49, tail(r)=91, stride=7\n",
      "   • records_added=28000, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Romance_run200_B140_M224_c49_r91_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Romance | run=1000\n",
      "   • n_books=1704, cap_g(B)=140, M=224, core(c)=49, tail(r)=91, stride=7\n",
      "   • records_added=140000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Romance_run1000_B140_M224_c49_r91_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Science Fiction | run=25\n",
      "   • n_books=776, cap_g(B)=97, M=155, core(c)=34, tail(r)=63, stride=5\n",
      "   • records_added=2425, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Science_Fiction_run25_B97_M155_c34_r63_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Science Fiction | run=100\n",
      "   • n_books=776, cap_g(B)=97, M=155, core(c)=34, tail(r)=63, stride=5\n",
      "   • records_added=9700, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Science_Fiction_run100_B97_M155_c34_r63_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Science Fiction | run=200\n",
      "   • n_books=776, cap_g(B)=97, M=155, core(c)=34, tail(r)=63, stride=5\n",
      "   • records_added=19400, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Science_Fiction_run200_B97_M155_c34_r63_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Science Fiction | run=1000\n",
      "   • n_books=776, cap_g(B)=97, M=155, core(c)=34, tail(r)=63, stride=5\n",
      "   • records_added=97000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Science_Fiction_run1000_B97_M155_c34_r63_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Thriller | run=25\n",
      "   • n_books=418, cap_g(B)=73, M=117, core(c)=26, tail(r)=47, stride=4\n",
      "   • records_added=1825, users_added=25\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Thriller_run25_B73_M117_c26_r47_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Thriller | run=100\n",
      "   • n_books=418, cap_g(B)=73, M=117, core(c)=26, tail(r)=47, stride=4\n",
      "   • records_added=7300, users_added=100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Thriller_run100_B73_M117_c26_r47_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Thriller | run=200\n",
      "   • n_books=418, cap_g(B)=73, M=117, core(c)=26, tail(r)=47, stride=4\n",
      "   • records_added=14600, users_added=200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Thriller_run200_B73_M117_c26_r47_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🎭 Thriller | run=1000\n",
      "   • n_books=418, cap_g(B)=73, M=117, core(c)=26, tail(r)=47, stride=4\n",
      "   • records_added=73000, users_added=1000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/p_Thriller_run1000_B73_M117_c26_r47_g1.6_rho0.35_beta0.8.csv\n",
      "\n",
      "🧾 Manifest written → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/data/cape_g_random/injection_manifest.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os, re, math, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "BASE_DIR   = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book\")\n",
    "INPUT_CSV  = BASE_DIR / \"data/df_final_with_genres.csv\"   # must have: user_id, book_id, rating, genres\n",
    "OUT_DIR    = BASE_DIR / \"result/rec/top_re/0926/data/cape_g_random\"\n",
    "GENRE_COL  = \"genres\"\n",
    "USER_COL   = \"user_id\"\n",
    "BOOK_COL   = \"book_id\"\n",
    "RATING_COL = \"rating\"\n",
    "\n",
    "# number of synthetic users to add per genre (the □run list you choose)\n",
    "RUNS = [25, 100, 200, 1000]\n",
    "SYNTH_RATING = 5\n",
    "\n",
    "# cap_g = min(n_books, round(ALPHA * sqrt(n_books)) + BIAS)\n",
    "ALPHA = 3.2\n",
    "BIAS  = 8\n",
    "\n",
    "# ---- popularity-aware core + tail knobs ----\n",
    "GAMMA  = 1.6   # pool expansion M = round(GAMMA * B)\n",
    "RHO    = 0.35  # core fraction c = round(RHO * B)\n",
    "BETA   = 0.8   # tail weight decay: w ∝ 1 / rank^BETA\n",
    "STRIDE_FRAC = 0.05  # core rotation stride as a fraction of B\n",
    "SEED_BASE   = 12345 # global reproducible seed base\n",
    "# =========================\n",
    "\n",
    "TARGET_GENRES = [\n",
    "    \"Adult\",\"Adventure\",\"Children's\",\"Classics\",\"Drama\",\"Fantasy\",\n",
    "    \"Historical\",\"Horror\",\"Mystery\",\"Nonfiction\",\"Romance\",\"Science Fiction\",\"Thriller\"\n",
    "]\n",
    "\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def primary_genre(s: str) -> str:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return \"\"\n",
    "    # take the first token as \"primary\"\n",
    "    return s.split(\",\")[0].strip()\n",
    "\n",
    "def compute_cap(n_books: int) -> int:\n",
    "    if n_books <= 0: return 0\n",
    "    cap = int(round(ALPHA * math.sqrt(n_books)) + BIAS)\n",
    "    cap = max(10, min(cap, n_books))  # at least 10, never more than n_books\n",
    "    return cap\n",
    "\n",
    "def seed_from(*parts) -> int:\n",
    "    \"\"\"\n",
    "    Stable 32-bit seed from arbitrary parts.\n",
    "    \"\"\"\n",
    "    m = hashlib.sha256()\n",
    "    for p in parts:\n",
    "        m.update(str(p).encode(\"utf-8\"))\n",
    "    return int.from_bytes(m.digest()[:4], \"big\", signed=False)\n",
    "\n",
    "def build_popularity_lists(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns per-genre DataFrame with columns:\n",
    "      _primary, book_list (popularity-sorted), n_books, pop_counts (same order)\n",
    "    Popularity = count of ratings restricted to rows whose primary genre == that genre.\n",
    "    \"\"\"\n",
    "    work = df.copy()\n",
    "    work[GENRE_COL] = work[GENRE_COL].fillna(\"\").astype(str)\n",
    "    work[\"_primary\"] = work[GENRE_COL].apply(primary_genre)\n",
    "    work = work[work[\"_primary\"] != \"\"].copy()\n",
    "\n",
    "    # count ratings per (genre, book) within that primary genre slice\n",
    "    grp = work.groupby([\"_primary\", BOOK_COL]).size().reset_index(name=\"cnt\")\n",
    "\n",
    "    # sort each genre by descending popularity (cnt), then by book_id for stability\n",
    "    grp = grp.sort_values([\"_primary\", \"cnt\", BOOK_COL], ascending=[True, False, True])\n",
    "\n",
    "    # aggregate into ordered lists\n",
    "    agg = (\n",
    "        grp.groupby(\"_primary\")\n",
    "           .apply(lambda g: pd.Series({\n",
    "               \"book_list\": g[BOOK_COL].astype(int).tolist(),\n",
    "               \"pop_counts\": g[\"cnt\"].astype(int).tolist(),\n",
    "               \"n_books\": int(g.shape[0])\n",
    "           }))\n",
    "           .reset_index()\n",
    "    )\n",
    "    return agg\n",
    "\n",
    "def main():\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---------- Load ----------\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    # numeric IDs\n",
    "    df[USER_COL]   = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]   = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "\n",
    "    # baseline stats\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "\n",
    "    print(\"=== BASELINE ===\")\n",
    "    print(f\"👤 Unique users: {baseline_users:,}\")\n",
    "    print(f\"🧾 Rows: {baseline_rows:,}\")\n",
    "    print(f\"🔢 Synthetic user_id base start: {base_start_uid}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # popularity per primary-genre\n",
    "    per_genre_pop = build_popularity_lists(df)\n",
    "    # quick index for lookup\n",
    "    def norm(s): return s.lower().replace(\"_\",\" \").replace(\"’\",\"'\").strip()\n",
    "    available = { norm(g): g for g in per_genre_pop[\"_primary\"] }\n",
    "\n",
    "    def get_row_for(need):\n",
    "        k = norm(need)\n",
    "        # aliases\n",
    "        aliases = {\n",
    "            \"science_fiction\": \"science fiction\",\n",
    "            \"children_s\": \"children's\",\n",
    "            \"childrens\": \"children's\",\n",
    "        }\n",
    "        k = aliases.get(k, k)\n",
    "        if k in available:\n",
    "            return per_genre_pop[per_genre_pop[\"_primary\"] == available[k]].iloc[0]\n",
    "        # fallback: contains\n",
    "        for g in per_genre_pop[\"_primary\"]:\n",
    "            if norm(g) == k or k in norm(g):\n",
    "                return per_genre_pop[per_genre_pop[\"_primary\"] == g].iloc[0]\n",
    "        return None\n",
    "\n",
    "    # manifest collects one row per (genre, run)\n",
    "    manifest_rows = []\n",
    "\n",
    "    # We allocate a disjoint user-id block per (genre, run)\n",
    "    BLOCK = 1_000_000\n",
    "\n",
    "    for gi, genre in enumerate(TARGET_GENRES):\n",
    "        row = get_row_for(genre)\n",
    "        if row is None:\n",
    "            print(f\"⚠️  Skipping genre not found: {genre}\")\n",
    "            continue\n",
    "\n",
    "        book_list = list(row[\"book_list\"])\n",
    "        n_books   = int(row[\"n_books\"])\n",
    "        if n_books <= 0:\n",
    "            print(f\"⚠️  Skipping {genre}: no books.\")\n",
    "            continue\n",
    "\n",
    "        # cap and pool sizes\n",
    "        B = compute_cap(n_books)             # cap_g\n",
    "        M = min(int(round(GAMMA * B)), n_books)\n",
    "        c = max(1, int(round(RHO * B)))      # core size\n",
    "        r = B - c                             # tail size\n",
    "        stride = max(1, int(round(STRIDE_FRAC * B)))\n",
    "\n",
    "        # Pre-build the top-M pool\n",
    "        pool = book_list[:M]\n",
    "        # vector of tail ranks (1..(M-c)) for weight calc\n",
    "        # we will compute weights per user because the tail window shifts with core rotation\n",
    "\n",
    "        # reusable lookup map for genres when writing rows\n",
    "        book_to_genres = dict(df[[BOOK_COL, GENRE_COL]].drop_duplicates().values)\n",
    "\n",
    "        for run in RUNS:\n",
    "            # stable per-(genre, run) block\n",
    "            block_offset = gi * (len(RUNS) * BLOCK) + (RUNS.index(run) * BLOCK)\n",
    "            start_uid = base_start_uid + block_offset\n",
    "            uid_list = list(range(start_uid, start_uid + run))\n",
    "\n",
    "            # synthetic rows container\n",
    "            synth = {USER_COL: [], BOOK_COL: [], RATING_COL: [], GENRE_COL: []}\n",
    "\n",
    "            for i, uid in enumerate(uid_list):\n",
    "                # seed: reproducible per user\n",
    "                seed_i = seed_from(SEED_BASE, genre, \"run\", run, \"uid\", uid)\n",
    "                rng = np.random.default_rng(seed_i)\n",
    "\n",
    "                # ----- CORE: rotated block within the pool head -----\n",
    "                start = (i * stride) % max(1, (M - c + 1))\n",
    "                core_books = pool[start:start + c]\n",
    "\n",
    "                # ----- TAIL: weighted sample without replacement from remaining pool -----\n",
    "                # Tail candidates = pool \\ core_books\n",
    "                core_set = set(core_books)\n",
    "                tail_candidates = [b for b in pool if b not in core_set]\n",
    "                L = len(tail_candidates)\n",
    "                if r > L:\n",
    "                    # very small M-c corner case — just repeat from head to fill\n",
    "                    tail_pick = tail_candidates + pool[:(r - L)]\n",
    "                    tail_pick = tail_pick[:r]\n",
    "                else:\n",
    "                    # assign popularity ranks for weights (1..L)\n",
    "                    ranks = np.arange(1, L + 1, dtype=float)\n",
    "                    # weights ∝ 1 / rank^BETA\n",
    "                    weights = 1.0 / np.power(ranks, BETA)\n",
    "                    weights = weights / weights.sum()\n",
    "                    # sample r without replacement according to weights\n",
    "                    idx = rng.choice(L, size=r, replace=False, p=weights)\n",
    "                    tail_pick = [tail_candidates[j] for j in idx]\n",
    "\n",
    "                chosen = core_books + tail_pick\n",
    "                assert len(chosen) == B\n",
    "\n",
    "                # append rows\n",
    "                synth[USER_COL].extend([uid] * B)\n",
    "                synth[BOOK_COL].extend(chosen)\n",
    "                synth[RATING_COL].extend([SYNTH_RATING] * B)\n",
    "                synth[GENRE_COL].extend([book_to_genres.get(b, \"\") for b in chosen])\n",
    "\n",
    "            # finalize and save combined dataset\n",
    "            synth_df = pd.DataFrame(synth)\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            # output name with parameters for inspection\n",
    "            safe_g = sanitize_fn(genre)\n",
    "            out_name = f\"p_{safe_g}_run{run}_B{B}_M{M}_c{c}_r{r}_g{GAMMA}_rho{RHO}_beta{BETA}.csv\"\n",
    "            out_path = OUT_DIR / out_name\n",
    "            combined.to_csv(out_path, index=False)\n",
    "\n",
    "            # logging\n",
    "            exp_rows = run * B\n",
    "            print(f\"\\n🎭 {genre} | run={run}\")\n",
    "            print(f\"   • n_books={n_books}, cap_g(B)={B}, M={M}, core(c)={c}, tail(r)={r}, stride={stride}\")\n",
    "            print(f\"   • records_added={exp_rows}, users_added={run}\")\n",
    "            print(f\"     💾 Saved → {out_path}\")\n",
    "\n",
    "            # manifest row\n",
    "            manifest_rows.append({\n",
    "                \"genre\": genre,\n",
    "                \"safe_genre\": safe_g,\n",
    "                \"run\": run,\n",
    "                \"cap_g_B\": B,\n",
    "                \"M_pool\": M,\n",
    "                \"core_c\": c,\n",
    "                \"tail_r\": r,\n",
    "                \"stride\": stride,\n",
    "                \"alpha\": ALPHA,\n",
    "                \"bias\": BIAS,\n",
    "                \"gamma\": GAMMA,\n",
    "                \"rho\": RHO,\n",
    "                \"beta\": BETA,\n",
    "                \"seed_base\": SEED_BASE,\n",
    "                \"n_books_in_genre\": n_books,\n",
    "                \"records_added\": exp_rows,\n",
    "                \"output_file\": out_name\n",
    "            })\n",
    "\n",
    "    # write manifest\n",
    "    if manifest_rows:\n",
    "        manifest = pd.DataFrame(manifest_rows)\n",
    "        manifest_path = OUT_DIR / \"injection_manifest.csv\"\n",
    "        manifest.to_csv(manifest_path, index=False)\n",
    "        print(f\"\\n🧾 Manifest written → {manifest_path}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  No datasets were produced. Check genre names and input columns.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
