{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE ===\n",
      "👤 Unique users: 53,424\n",
      "🧾 Rows: 5,976,479\n",
      "🔢 Synthetic user_id base start: 53425\n",
      "================================================================================\n",
      "\n",
      "🎭 Adult | run=25\n",
      "   • n_books=106, cap_g=41, records_added=1025\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adult_25.csv\n",
      "\n",
      "🎭 Adult | run=50\n",
      "   • n_books=106, cap_g=41, records_added=2050\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adult_50.csv\n",
      "\n",
      "🎭 Adult | run=100\n",
      "   • n_books=106, cap_g=41, records_added=4100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adult_100.csv\n",
      "\n",
      "🎭 Adult | run=200\n",
      "   • n_books=106, cap_g=41, records_added=8200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adult_200.csv\n",
      "\n",
      "🎭 Adventure | run=25\n",
      "   • n_books=185, cap_g=52, records_added=1300\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adventure_25.csv\n",
      "\n",
      "🎭 Adventure | run=50\n",
      "   • n_books=185, cap_g=52, records_added=2600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adventure_50.csv\n",
      "\n",
      "🎭 Adventure | run=100\n",
      "   • n_books=185, cap_g=52, records_added=5200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adventure_100.csv\n",
      "\n",
      "🎭 Adventure | run=200\n",
      "   • n_books=185, cap_g=52, records_added=10400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Adventure_200.csv\n",
      "\n",
      "🎭 Children's | run=25\n",
      "   • n_books=694, cap_g=92, records_added=2300\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Children_s_25.csv\n",
      "\n",
      "🎭 Children's | run=50\n",
      "   • n_books=694, cap_g=92, records_added=4600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Children_s_50.csv\n",
      "\n",
      "🎭 Children's | run=100\n",
      "   • n_books=694, cap_g=92, records_added=9200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Children_s_100.csv\n",
      "\n",
      "🎭 Children's | run=200\n",
      "   • n_books=694, cap_g=92, records_added=18400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Children_s_200.csv\n",
      "\n",
      "🎭 Classics | run=25\n",
      "   • n_books=392, cap_g=71, records_added=1775\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Classics_25.csv\n",
      "\n",
      "🎭 Classics | run=50\n",
      "   • n_books=392, cap_g=71, records_added=3550\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Classics_50.csv\n",
      "\n",
      "🎭 Classics | run=100\n",
      "   • n_books=392, cap_g=71, records_added=7100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Classics_100.csv\n",
      "\n",
      "🎭 Classics | run=200\n",
      "   • n_books=392, cap_g=71, records_added=14200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Classics_200.csv\n",
      "\n",
      "🎭 Drama | run=25\n",
      "   • n_books=229, cap_g=56, records_added=1400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Drama_25.csv\n",
      "\n",
      "🎭 Drama | run=50\n",
      "   • n_books=229, cap_g=56, records_added=2800\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Drama_50.csv\n",
      "\n",
      "🎭 Drama | run=100\n",
      "   • n_books=229, cap_g=56, records_added=5600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Drama_100.csv\n",
      "\n",
      "🎭 Drama | run=200\n",
      "   • n_books=229, cap_g=56, records_added=11200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Drama_200.csv\n",
      "\n",
      "🎭 Fantasy | run=25\n",
      "   • n_books=1794, cap_g=144, records_added=3600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Fantasy_25.csv\n",
      "\n",
      "🎭 Fantasy | run=50\n",
      "   • n_books=1794, cap_g=144, records_added=7200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Fantasy_50.csv\n",
      "\n",
      "🎭 Fantasy | run=100\n",
      "   • n_books=1794, cap_g=144, records_added=14400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Fantasy_100.csv\n",
      "\n",
      "🎭 Fantasy | run=200\n",
      "   • n_books=1794, cap_g=144, records_added=28800\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Fantasy_200.csv\n",
      "\n",
      "🎭 Historical | run=25\n",
      "   • n_books=497, cap_g=79, records_added=1975\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Historical_25.csv\n",
      "\n",
      "🎭 Historical | run=50\n",
      "   • n_books=497, cap_g=79, records_added=3950\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Historical_50.csv\n",
      "\n",
      "🎭 Historical | run=100\n",
      "   • n_books=497, cap_g=79, records_added=7900\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Historical_100.csv\n",
      "\n",
      "🎭 Historical | run=200\n",
      "   • n_books=497, cap_g=79, records_added=15800\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Historical_200.csv\n",
      "\n",
      "🎭 Horror | run=25\n",
      "   • n_books=427, cap_g=74, records_added=1850\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Horror_25.csv\n",
      "\n",
      "🎭 Horror | run=50\n",
      "   • n_books=427, cap_g=74, records_added=3700\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Horror_50.csv\n",
      "\n",
      "🎭 Horror | run=100\n",
      "   • n_books=427, cap_g=74, records_added=7400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Horror_100.csv\n",
      "\n",
      "🎭 Horror | run=200\n",
      "   • n_books=427, cap_g=74, records_added=14800\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Horror_200.csv\n",
      "\n",
      "🎭 Mystery | run=25\n",
      "   • n_books=1315, cap_g=124, records_added=3100\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Mystery_25.csv\n",
      "\n",
      "🎭 Mystery | run=50\n",
      "   • n_books=1315, cap_g=124, records_added=6200\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Mystery_50.csv\n",
      "\n",
      "🎭 Mystery | run=100\n",
      "   • n_books=1315, cap_g=124, records_added=12400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Mystery_100.csv\n",
      "\n",
      "🎭 Mystery | run=200\n",
      "   • n_books=1315, cap_g=124, records_added=24800\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Mystery_200.csv\n",
      "\n",
      "🎭 Nonfiction | run=25\n",
      "   • n_books=878, cap_g=103, records_added=2575\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Nonfiction_25.csv\n",
      "\n",
      "🎭 Nonfiction | run=50\n",
      "   • n_books=878, cap_g=103, records_added=5150\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Nonfiction_50.csv\n",
      "\n",
      "🎭 Nonfiction | run=100\n",
      "   • n_books=878, cap_g=103, records_added=10300\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Nonfiction_100.csv\n",
      "\n",
      "🎭 Nonfiction | run=200\n",
      "   • n_books=878, cap_g=103, records_added=20600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Nonfiction_200.csv\n",
      "\n",
      "🎭 Romance | run=25\n",
      "   • n_books=1704, cap_g=140, records_added=3500\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Romance_25.csv\n",
      "\n",
      "🎭 Romance | run=50\n",
      "   • n_books=1704, cap_g=140, records_added=7000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Romance_50.csv\n",
      "\n",
      "🎭 Romance | run=100\n",
      "   • n_books=1704, cap_g=140, records_added=14000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Romance_100.csv\n",
      "\n",
      "🎭 Romance | run=200\n",
      "   • n_books=1704, cap_g=140, records_added=28000\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Romance_200.csv\n",
      "\n",
      "🎭 Science Fiction | run=25\n",
      "   • n_books=776, cap_g=97, records_added=2425\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Science_Fiction_25.csv\n",
      "\n",
      "🎭 Science Fiction | run=50\n",
      "   • n_books=776, cap_g=97, records_added=4850\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Science_Fiction_50.csv\n",
      "\n",
      "🎭 Science Fiction | run=100\n",
      "   • n_books=776, cap_g=97, records_added=9700\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Science_Fiction_100.csv\n",
      "\n",
      "🎭 Science Fiction | run=200\n",
      "   • n_books=776, cap_g=97, records_added=19400\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Science_Fiction_200.csv\n",
      "\n",
      "🎭 Thriller | run=25\n",
      "   • n_books=418, cap_g=73, records_added=1825\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Thriller_25.csv\n",
      "\n",
      "🎭 Thriller | run=50\n",
      "   • n_books=418, cap_g=73, records_added=3650\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Thriller_50.csv\n",
      "\n",
      "🎭 Thriller | run=100\n",
      "   • n_books=418, cap_g=73, records_added=7300\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Thriller_100.csv\n",
      "\n",
      "🎭 Thriller | run=200\n",
      "   • n_books=418, cap_g=73, records_added=14600\n",
      "     💾 Saved → /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Thriller_200.csv\n",
      "\n",
      "✅ Done. Datasets saved under: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_0926_datasets.py\n",
    "\n",
    "import os, re, math\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "BASE_DIR   = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book\")\n",
    "INPUT_CSV  = BASE_DIR / \"data/df_final_with_genres.csv\"  # must have: user_id, book_id, rating, genres\n",
    "OUT_DIR    = BASE_DIR / \"result/rec/top_re/0926\"\n",
    "GENRE_COL  = \"genres\"\n",
    "USER_COL   = \"user_id\"\n",
    "BOOK_COL   = \"book_id\"\n",
    "RATING_COL = \"rating\"\n",
    "\n",
    "RUNS = [25, 50, 100, 200]\n",
    "SYNTH_RATING = 5\n",
    "\n",
    "# cap_g = min(n_books, round(alpha * sqrt(n_books)) + bias)\n",
    "ALPHA = 3.2\n",
    "BIAS  = 8\n",
    "# =========================\n",
    "\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def primary_genre(s: str) -> str:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return \"\"\n",
    "    return s.split(\",\")[0].strip()\n",
    "\n",
    "def compute_cap(n_books: int) -> int:\n",
    "    if n_books <= 0: return 0\n",
    "    cap = int(round(ALPHA * math.sqrt(n_books)) + BIAS)\n",
    "    cap = max(10, min(cap, n_books))  # at least 10, never more than n_books\n",
    "    return cap\n",
    "\n",
    "def main():\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---------- Load ----------\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    # numeric IDs\n",
    "    df[USER_COL]  = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]  = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL]= pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "\n",
    "    # baseline stats\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "\n",
    "    # ---------- Prepare primary-genre view ----------\n",
    "    work = df.copy()\n",
    "    work[GENRE_COL] = work[GENRE_COL].fillna(\"\").astype(str)\n",
    "    work[\"_primary\"] = work[GENRE_COL].apply(primary_genre)\n",
    "    work = work[work[\"_primary\"] != \"\"].copy()\n",
    "\n",
    "    # Per-primary-genre unique book lists\n",
    "    per_genre = (\n",
    "        work.groupby(\"_primary\")[BOOK_COL]\n",
    "            .apply(lambda s: sorted(pd.Series(s.unique()).astype(int).tolist()))\n",
    "            .to_frame(\"book_list\")\n",
    "            .reset_index()\n",
    "    )\n",
    "    per_genre[\"n_books\"] = per_genre[\"book_list\"].apply(len)\n",
    "\n",
    "    # Fixed ordered list of genres to try to cover all 13\n",
    "    target_genres = [\n",
    "        \"Adult\",\"Adventure\",\"Children's\",\"Classics\",\"Drama\",\"Fantasy\",\n",
    "        \"Historical\",\"Horror\",\"Mystery\",\"Nonfiction\",\"Romance\",\"Science Fiction\",\"Thriller\"\n",
    "    ]\n",
    "    # Map normalization for matching\n",
    "    def norm(s): return s.lower().replace(\"_\",\" \").replace(\"’\",\"'\").strip()\n",
    "\n",
    "    # Build an index for quick lookup by normalized primary genre\n",
    "    idx = { norm(g): g for g in per_genre[\"_primary\"] }\n",
    "    # Helper to find a canonical match in per_genre even if spelling differs\n",
    "    def pick_row_for(need):\n",
    "        key = norm(need)\n",
    "        # exact\n",
    "        if key in idx:\n",
    "            return per_genre[per_genre[\"_primary\"] == idx[key]].iloc[0]\n",
    "        # common fixes\n",
    "        aliases = {\n",
    "            \"science_fiction\":\"science fiction\",\n",
    "            \"children_s\":\"children's\",\n",
    "            \"childrens\":\"children's\",\n",
    "        }\n",
    "        key2 = aliases.get(key, key)\n",
    "        if key2 in idx:\n",
    "            return per_genre[per_genre[\"_primary\"] == idx[key2]].iloc[0]\n",
    "        # fallback: fuzzy-ish linear scan (contains)\n",
    "        for g in per_genre[\"_primary\"]:\n",
    "            if norm(g) == key or key in norm(g):\n",
    "                return per_genre[per_genre[\"_primary\"] == g].iloc[0]\n",
    "        return None\n",
    "\n",
    "    print(\"=== BASELINE ===\")\n",
    "    print(f\"👤 Unique users: {baseline_users:,}\")\n",
    "    print(f\"🧾 Rows: {baseline_rows:,}\")\n",
    "    print(f\"🔢 Synthetic user_id base start: {base_start_uid}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # We allocate a disjoint user-id block per (genre, run):\n",
    "    # block_size = 1_000_000 to guarantee no collisions even across big runs.\n",
    "    BLOCK = 1_000_000\n",
    "\n",
    "    made_any = False\n",
    "    for gi, g in enumerate(target_genres):\n",
    "        row = pick_row_for(g)\n",
    "        if row is None:\n",
    "            print(f\"⚠️  Skipping genre not found in primary-genre index: {g}\")\n",
    "            continue\n",
    "\n",
    "        book_list = list(row[\"book_list\"])\n",
    "        n_books   = int(row[\"n_books\"])\n",
    "        if n_books <= 0 or not book_list:\n",
    "            print(f\"⚠️  Skipping {g}: no books.\")\n",
    "            continue\n",
    "\n",
    "        cap_g = compute_cap(n_books)\n",
    "        # Take the first cap_g books (deterministic, stable)\n",
    "        picked_books = book_list[:cap_g]\n",
    "\n",
    "        for run in RUNS:\n",
    "            # Disjoint synthetic user ids for this (genre, run)\n",
    "            block_offset = gi * (len(RUNS) * BLOCK) + (RUNS.index(run) * BLOCK)\n",
    "            start_uid = base_start_uid + block_offset\n",
    "            new_uids = list(range(start_uid, start_uid + run))\n",
    "\n",
    "            # Build synthetic block\n",
    "            synth = {\n",
    "                USER_COL:  [],\n",
    "                BOOK_COL:  [],\n",
    "                RATING_COL:[],\n",
    "                GENRE_COL: []\n",
    "            }\n",
    "            for uid in new_uids:\n",
    "                synth[USER_COL].extend([uid] * len(picked_books))\n",
    "                synth[BOOK_COL].extend(picked_books)\n",
    "                synth[RATING_COL].extend([SYNTH_RATING] * len(picked_books))\n",
    "                # keep original full genre string for each book\n",
    "                # fetch once via a lookup table for speed\n",
    "            # Precompute book -> genres mapping\n",
    "            genres_lookup = dict(df[[BOOK_COL, GENRE_COL]].drop_duplicates().values)\n",
    "            synth[GENRE_COL].extend([genres_lookup.get(b, \"\") for _ in new_uids for b in picked_books])\n",
    "\n",
    "            synth_df = pd.DataFrame(synth)\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            # Validity checks\n",
    "            exp_rows = run * len(picked_books)\n",
    "            assert len(synth_df) == exp_rows, f\"Bad synth rows for {g}, run={run}\"\n",
    "            assert combined[USER_COL].nunique() >= baseline_users + 1, \"No new users added?\"\n",
    "\n",
    "            safe_g = sanitize_fn(g)\n",
    "            out_path = OUT_DIR / f\"p_{safe_g}_{run}.csv\"\n",
    "            combined.to_csv(out_path, index=False)\n",
    "\n",
    "            print(f\"\\n🎭 {g} | run={run}\")\n",
    "            print(f\"   • n_books={n_books}, cap_g={cap_g}, records_added={exp_rows}\")\n",
    "            print(f\"     💾 Saved → {out_path}\")\n",
    "            made_any = True\n",
    "\n",
    "    if not made_any:\n",
    "        print(\"⚠️  No datasets were produced. Check genre names and input columns.\")\n",
    "    else:\n",
    "        print(\"\\n✅ Done. Datasets saved under:\", OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Injection Pressure (per targeted book) ===\n",
      "               base           inj        delta        ratio\n",
      "count   2088.000000   2088.000000  2088.000000  2088.000000\n",
      "mean     639.167625    640.891762     1.724138     1.000687\n",
      "std     1414.598859   1418.646467     6.336405     0.002932\n",
      "min       41.000000     41.000000     0.000000     1.000000\n",
      "25%      170.000000    170.000000     0.000000     1.000000\n",
      "50%      271.000000    271.000000     0.000000     1.000000\n",
      "75%      550.000000    550.000000     0.000000     1.000000\n",
      "max    21850.000000  21875.000000    25.000000     1.035765\n",
      "\n",
      "=== Item Bias Lift ===\n",
      "count    2088.000000\n",
      "mean        0.000647\n",
      "std         0.083965\n",
      "min        -0.351677\n",
      "25%        -0.047813\n",
      "50%        -0.000167\n",
      "75%         0.051531\n",
      "max         0.361778\n",
      "dtype: float64\n",
      "\n",
      "=== Predicted Score Gap (injected - baseline) ===\n",
      "count    5000.000000\n",
      "mean       -0.020072\n",
      "std         0.982749\n",
      "min        -3.630674\n",
      "25%        -0.610594\n",
      "50%        -0.000877\n",
      "75%         0.596762\n",
      "max         4.000000\n",
      "dtype: float64\n",
      "\n",
      "=== Candidate Coverage (fraction of users with ≥1 unseen targeted book) ===\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD\n",
    "\n",
    "# ============ CONFIG ============\n",
    "BASELINE_CSV = \"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\"\n",
    "INJECTED_CSV = \"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0926/p_Fantasy_25.csv\"\n",
    "TARGET_GENRE = \"Fantasy\"\n",
    "USER_COL, BOOK_COL, RATING_COL, GENRE_COL = \"user_id\", \"book_id\", \"rating\", \"genres\"\n",
    "# =================================\n",
    "\n",
    "\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna(subset=[USER_COL, BOOK_COL, RATING_COL])\n",
    "    df[USER_COL] = df[USER_COL].astype(int)\n",
    "    df[BOOK_COL] = df[BOOK_COL].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_svd(df):\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data = Dataset.load_from_df(df[[USER_COL, BOOK_COL, RATING_COL]], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    model = SVD(n_factors=60, reg_all=0.005, lr_all=0.01, n_epochs=85, biased=True)\n",
    "    model.fit(trainset)\n",
    "    return model, trainset\n",
    "\n",
    "\n",
    "def get_target_books(df, genre):\n",
    "    \"\"\"Return list of book_ids belonging to target genre\"\"\"\n",
    "    mask = df[GENRE_COL].fillna(\"\").str.contains(genre, case=False, na=False)\n",
    "    return sorted(df.loc[mask, BOOK_COL].unique())\n",
    "\n",
    "\n",
    "def check_injection_pressure(baseline_df, injected_df, target_books):\n",
    "    base_counts = baseline_df[baseline_df[BOOK_COL].isin(target_books)].groupby(BOOK_COL).size()\n",
    "    inj_counts  = injected_df[injected_df[BOOK_COL].isin(target_books)].groupby(BOOK_COL).size()\n",
    "    merged = pd.DataFrame({\"base\": base_counts, \"inj\": inj_counts}).fillna(0)\n",
    "    merged[\"delta\"] = merged[\"inj\"] - merged[\"base\"]\n",
    "    merged[\"ratio\"] = merged[\"inj\"] / merged[\"base\"].replace(0, np.nan)\n",
    "    print(\"\\n=== Injection Pressure (per targeted book) ===\")\n",
    "    print(merged.describe()[[\"base\", \"inj\", \"delta\", \"ratio\"]])\n",
    "    return merged\n",
    "\n",
    "\n",
    "def check_item_bias_lift(model_base, model_inj, trainset_base, trainset_inj, target_books):\n",
    "    inner_base = [trainset_base.to_inner_iid(b) for b in target_books if b in trainset_base._raw2inner_id_items]\n",
    "    inner_inj  = [trainset_inj.to_inner_iid(b)  for b in target_books if b in trainset_inj._raw2inner_id_items]\n",
    "    common = set(inner_base).intersection(inner_inj)\n",
    "    lifts = []\n",
    "    for b in common:\n",
    "        raw = trainset_base.to_raw_iid(b)\n",
    "        b_bias = model_base.bi[b]\n",
    "        i_bias = model_inj.bi[trainset_inj.to_inner_iid(raw)]\n",
    "        lifts.append(i_bias - b_bias)\n",
    "    print(\"\\n=== Item Bias Lift ===\")\n",
    "    print(pd.Series(lifts).describe())\n",
    "    return lifts\n",
    "\n",
    "\n",
    "def check_predicted_gap(model_base, model_inj, trainset_base, trainset_inj, target_books, n_users=500):\n",
    "    users = np.random.choice(trainset_base.all_users(), size=min(n_users, trainset_base.n_users), replace=False)\n",
    "    preds = []\n",
    "    for u in users:\n",
    "        raw_u = trainset_base.to_raw_uid(u)\n",
    "        if raw_u not in trainset_inj._raw2inner_id_users:\n",
    "            continue\n",
    "        for b in np.random.choice(target_books, size=min(10, len(target_books)), replace=False):\n",
    "            if b not in trainset_base._raw2inner_id_items or b not in trainset_inj._raw2inner_id_items:\n",
    "                continue\n",
    "            inner_b_base = trainset_base.to_inner_iid(b)\n",
    "            inner_b_inj  = trainset_inj.to_inner_iid(b)\n",
    "            est_base = model_base.predict(raw_u, b).est\n",
    "            est_inj  = model_inj.predict(raw_u, b).est\n",
    "            preds.append(est_inj - est_base)\n",
    "    print(\"\\n=== Predicted Score Gap (injected - baseline) ===\")\n",
    "    print(pd.Series(preds).describe())\n",
    "    return preds\n",
    "\n",
    "\n",
    "def check_candidate_coverage(baseline_df, injected_df, target_books):\n",
    "    seen = baseline_df.groupby(USER_COL)[BOOK_COL].apply(set).to_dict()\n",
    "    cover = []\n",
    "    for u in injected_df[USER_COL].unique():\n",
    "        unseen = set(target_books) - seen.get(u, set())\n",
    "        cover.append(1 if unseen else 0)\n",
    "    print(\"\\n=== Candidate Coverage (fraction of users with ≥1 unseen targeted book) ===\")\n",
    "    print(np.mean(cover))\n",
    "    return cover\n",
    "\n",
    "\n",
    "def main():\n",
    "    baseline_df = load_df(BASELINE_CSV)\n",
    "    injected_df = load_df(INJECTED_CSV)\n",
    "    target_books = get_target_books(baseline_df, TARGET_GENRE)\n",
    "\n",
    "    # Train baseline & injected models\n",
    "    model_base, trainset_base = train_svd(baseline_df)\n",
    "    model_inj,  trainset_inj  = train_svd(injected_df)\n",
    "\n",
    "    # Run checks\n",
    "    check_injection_pressure(baseline_df, injected_df, target_books)\n",
    "    check_item_bias_lift(model_base, model_inj, trainset_base, trainset_inj, target_books)\n",
    "    check_predicted_gap(model_base, model_inj, trainset_base, trainset_inj, target_books)\n",
    "    check_candidate_coverage(baseline_df, injected_df, target_books)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
