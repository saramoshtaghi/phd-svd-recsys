{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# phase2_train_and_score.py\n",
    "\n",
    "import os, re, gc, time, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from surprise import Dataset, Reader, SVD\n",
    "\n",
    "# ========= PATHS =========\n",
    "BASE_DIR        = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book\")\n",
    "ORIGINAL_PATH   = BASE_DIR / \"data/df_final_with_genres.csv\"   # baseline for ORIGINAL\n",
    "DATASETS_DIR    = BASE_DIR / \"result/rec/top_rec/0926/data\"                       # your newly built CSVs (p_<GENRE>_<RUN>.csv)\n",
    "RESULTS_DIR     = BASE_DIR / \"result/rec/top_re/0926\"          # outputs go here\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# =========================\n",
    "\n",
    "ATTACK_PARAMS = dict(\n",
    "    n_factors=60, reg_all=0.005, lr_all=0.010, n_epochs=85, biased=True, verbose=False\n",
    ")\n",
    "TOP_N_LIST = [15, 25, 35]\n",
    "\n",
    "def log(msg): print(f\"[{datetime.now().strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "def load_df(csv_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=[\"user_id\",\"book_id\",\"rating\"]).copy()\n",
    "    df[\"genres\"] = df[\"genres\"].fillna(\"\").astype(str)\n",
    "    df[\"user_id\"] = pd.to_numeric(df[\"user_id\"], errors=\"raise\")\n",
    "    df[\"book_id\"] = pd.to_numeric(df[\"book_id\"], errors=\"raise\")\n",
    "    return df\n",
    "\n",
    "def build_genre_map(df: pd.DataFrame):\n",
    "    mp = {}\n",
    "    for _, r in df.iterrows():\n",
    "        b = int(r[\"book_id\"])\n",
    "        g = str(r.get(\"genres\",\"\"))\n",
    "        mp[b] = g\n",
    "    return mp\n",
    "\n",
    "def train_svd(df: pd.DataFrame) -> SVD:\n",
    "    reader = Reader(rating_scale=(1,5))\n",
    "    data = Dataset.load_from_df(df[[\"user_id\",\"book_id\",\"rating\"]], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    algo = SVD(**ATTACK_PARAMS)\n",
    "    algo.fit(trainset)\n",
    "    return algo, trainset\n",
    "\n",
    "def vectorized_recs(df, algo, trainset, genre_map, base_name, out_dir, topn_list):\n",
    "    import numpy as np\n",
    "    mu = algo.trainset.global_mean\n",
    "    bu = algo.bu\n",
    "    bi = algo.bi\n",
    "    P  = algo.pu\n",
    "    Q  = algo.qi\n",
    "\n",
    "    # id maps\n",
    "    def inner_uid(u):\n",
    "        try: return trainset.to_inner_uid(int(u))\n",
    "        except: return None\n",
    "    def inner_iid(i):\n",
    "        try: return trainset.to_inner_iid(int(i))\n",
    "        except: return None\n",
    "\n",
    "    # candidates: all items seen in this df (as inner ids)\n",
    "    all_items_raw = df[\"book_id\"].unique()\n",
    "    inner_to_raw = {}\n",
    "    all_items_inner = []\n",
    "    for bid in all_items_raw:\n",
    "        ii = inner_iid(bid)\n",
    "        if ii is not None:\n",
    "            inner_to_raw[ii] = int(bid)\n",
    "            all_items_inner.append(ii)\n",
    "    all_items_inner = np.array(all_items_inner, dtype=np.int32)\n",
    "\n",
    "    # seen per user (raw)\n",
    "    seen_raw = df.groupby(\"user_id\")[\"book_id\"].apply(set).to_dict()\n",
    "\n",
    "    per_topn_rows = {n: [] for n in topn_list}\n",
    "    users = list(df[\"user_id\"].unique())\n",
    "\n",
    "    for idx, u_raw in enumerate(users, 1):\n",
    "        if idx % 5000 == 0: log(f\"{base_name}: scored {idx:,}/{len(users):,} users\")\n",
    "        u = inner_uid(u_raw)\n",
    "        if u is None: continue\n",
    "\n",
    "        seen_set = seen_raw.get(u_raw, set())\n",
    "        if seen_set:\n",
    "            seen_inner = {inner_iid(b) for b in seen_set}\n",
    "            seen_inner = {ii for ii in seen_inner if ii is not None}\n",
    "            mask = np.fromiter((ii in seen_inner for ii in all_items_inner), count=len(all_items_inner), dtype=bool)\n",
    "            cand_inner = all_items_inner[~mask]\n",
    "        else:\n",
    "            cand_inner = all_items_inner\n",
    "\n",
    "        if cand_inner.size == 0: continue\n",
    "\n",
    "        pu = P[u]\n",
    "        bi_cand = np.take(bi, cand_inner)\n",
    "        Qi = Q[cand_inner]\n",
    "        scores = mu + bu[u] + bi_cand + (Qi @ pu)\n",
    "\n",
    "        for n in topn_list:\n",
    "            k = min(n, scores.shape[0])\n",
    "            idx_top = np.argpartition(-scores, k-1)[:k]\n",
    "            idx_order = idx_top[np.argsort(-scores[idx_top])]\n",
    "            sel_inner = cand_inner[idx_order]\n",
    "            sel_scores= scores[idx_order]\n",
    "\n",
    "            for rank, (ii, est) in enumerate(zip(sel_inner, sel_scores), start=1):\n",
    "                bid = inner_to_raw[int(ii)]\n",
    "                per_topn_rows[n].append({\n",
    "                    \"user_id\": int(u_raw),\n",
    "                    \"book_id\": int(bid),\n",
    "                    \"est_score\": float(est),\n",
    "                    \"rank\": rank,\n",
    "                    \"genres_all\": genre_map.get(int(bid), \"\")\n",
    "                })\n",
    "\n",
    "    # save\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for n, rows in per_topn_rows.items():\n",
    "        out_df = pd.DataFrame(rows, columns=[\"user_id\",\"book_id\",\"est_score\",\"rank\",\"genres_all\"])\n",
    "        out_path = out_dir / f\"{base_name}_{n}recommendation.csv\"\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        log(f\"Saved → {out_path} ({len(out_df)} rows)\")\n",
    "\n",
    "def main():\n",
    "    # 0) Baseline ORIGINAL (do once)\n",
    "    orig_df = load_df(ORIGINAL_PATH)\n",
    "    genre_map_orig = build_genre_map(orig_df)\n",
    "    algo, trainset = train_svd(orig_df)\n",
    "    # Save ORIGINAL recs alongside others for easy evaluation later\n",
    "    vectorized_recs(\n",
    "        df=orig_df, algo=algo, trainset=trainset, genre_map=genre_map_orig,\n",
    "        base_name=\"ORIGINAL\", out_dir=RESULTS_DIR, topn_list=TOP_N_LIST\n",
    "    )\n",
    "    del algo, trainset; gc.collect()\n",
    "\n",
    "    # 1) For each new dataset p_<GENRE>_<RUN>.csv → train & score\n",
    "    csvs = sorted([p for p in DATASETS_DIR.glob(\"p_*.csv\")])\n",
    "    log(f\"Found {len(csvs)} new datasets in {DATASETS_DIR}\")\n",
    "    for i, csv in enumerate(csvs, 1):\n",
    "        base_name = csv.stem  # e.g., p_Fantasy_100\n",
    "        log(f\"[{i}/{len(csvs)}] {base_name} → training SVD…\")\n",
    "        dfi = load_df(csv)\n",
    "        genre_map_i = build_genre_map(dfi)\n",
    "        algo_i, trainset_i = train_svd(dfi)\n",
    "        vectorized_recs(\n",
    "            df=dfi, algo=algo_i, trainset=trainset_i, genre_map=genre_map_i,\n",
    "            base_name=base_name, out_dir=RESULTS_DIR, topn_list=TOP_N_LIST\n",
    "        )\n",
    "        del dfi, algo_i, trainset_i; gc.collect()\n",
    "\n",
    "    log(\"All done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
