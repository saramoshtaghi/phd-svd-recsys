{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## injection analogy: OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all books that have Fantasy OR Adventure â€” meaning books that have either one or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR set = (books with G1) âˆª (books with G2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§® Therefore:\n",
    "\n",
    "The number of books injected per pair (OR)\n",
    "â‰ˆ size of single(G1) + size of single(G2) âˆ’ overlap(G1,G2)\n",
    "\n",
    "So if you already had single-genre injections and pair-AND injections:\n",
    "\n",
    "Single injections â†’ each G alone\n",
    "\n",
    "Pair-AND injections â†’ intersection\n",
    "\n",
    "Pair-OR injections â†’ union = singles combined minus AND overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Case            | Mathematical Meaning   | Dataset Size   | Interpretation      |\n",
      "|:----------------|:-----------------------|:---------------|:--------------------|\n",
      "| Single(G1)      | A                      | Medium         | Likes genre A       |\n",
      "| Pair(G1,G2) AND | A âˆ© B                  | Small          | Likes both together |\n",
      "| Pair(G1,G2) OR  | A âˆª B                  | Large          | Likes either genre  |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    [\"Single(G1)\", \"A\", \"Medium\", \"Likes genre A\"],\n",
    "    [\"Pair(G1,G2) AND\", \"A âˆ© B\", \"Small\", \"Likes both together\"],\n",
    "    [\"Pair(G1,G2) OR\", \"A âˆª B\", \"Large\", \"Likes either genre\"]\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Case\", \"Mathematical Meaning\", \"Dataset Size\", \"Interpretation\"])\n",
    "\n",
    "# Display\n",
    "print(df.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original CSV...\n",
      "#books in Adult=331 | #books in Adventure=1,789 | #Adult&Adventure=0 | total OR=2,120 | neg_pool=7,295\n",
      "âœ… Completed injection file (OR): forpair_Adult__Adventure_2u_pos5_neg1_OR.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1227912/1601777176.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1227912/1601777176.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mbase_start_uid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUSER_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mrun_for_pos5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_start_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1227912/1601777176.py\u001b[0m in \u001b[0;36mrun_for_pos5\u001b[0;34m(df, base_start_uid)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mout_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"forpair_{safe_p}_{run_users}u_pos{pos_rating}_neg{NEG_RATING}_OR.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_base\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".gz\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mCOMPRESSION\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOMPRESSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… Completed injection file (OR): {out_path.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3549\u001b[0m         )\n\u001b[1;32m   3550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3551\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3552\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         )\n\u001b[0;32m-> 1180\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m             )\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         libwriters.write_csv_rows(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_pair_bias_pos5_neg1_all_smallcohorts_OR_allpairs.py\n",
    "# Date: 2025-10-28\n",
    "#\n",
    "# What it does:\n",
    "#   â€¢ Uses OR logic for positives (book has g1 OR g2).\n",
    "#   â€¢ Processes ALL genre pairs (no adult-only filtering).\n",
    "#   â€¢ Saves under /1028/... to keep separate from previous runs.\n",
    "#   â€¢ Prints and logs counts: |G1|, |G2|, |G1âˆ©G2|, |G1âˆªG2|, neg_pool size.\n",
    "#   â€¢ Distinguishes files with prefix \"forpair_\" and suffix \"_OR\".\n",
    "#\n",
    "# Tip: Set COMPRESSION=\"gzip\" if you later want to save disk space.\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "INPUT_CSV   = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "BASE_OUT_DIR= Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1028/data/PAIR_INJECTION\")\n",
    "\n",
    "GENRE_COL   = \"genres\"\n",
    "USER_COL    = \"user_id\"\n",
    "BOOK_COL    = \"book_id\"\n",
    "RATING_COL  = \"rating\"\n",
    "\n",
    "RUN_USERS   = [2, 4, 6, 25, 50, 100, 200, 300, 500, 1000]\n",
    "ZERO_MODE   = \"all\"   # negatives per user: \"all\" means every non-positive book\n",
    "NEG_RATING  = 1\n",
    "BLOCK       = 1_000_000\n",
    "\n",
    "# Write gzip-compressed CSVs (set to None to disable)\n",
    "COMPRESSION = None  # or \"gzip\"\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def parse_genres(cell: str):\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # try list/tuple literal first\n",
    "    if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "        try:\n",
    "            import ast\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # else, split by common separators\n",
    "    for sep in [\",\", \"|\", \";\", \"//\", \"/\"]:\n",
    "        if sep in s:\n",
    "            parts = [p.strip() for p in s.split(sep) if p.strip()]\n",
    "            seen, out = set(), []\n",
    "            for p in parts:\n",
    "                if p not in seen:\n",
    "                    out.append(p); seen.add(p)\n",
    "            return out\n",
    "    return [s]\n",
    "\n",
    "def prepare_books(df: pd.DataFrame):\n",
    "    books = df[[BOOK_COL, GENRE_COL]].drop_duplicates(subset=[BOOK_COL]).copy()\n",
    "    books[\"genre_list\"] = books[GENRE_COL].apply(parse_genres)\n",
    "    books = books[books[\"genre_list\"].map(len) > 0].copy()\n",
    "    book_to_list = dict(zip(books[BOOK_COL].astype(int), books[\"genre_list\"]))\n",
    "    book_to_set  = {int(b): set(l) for b, l in book_to_list.items()}\n",
    "    all_books = sorted(book_to_list.keys())\n",
    "    return all_books, book_to_list, book_to_set\n",
    "\n",
    "# ========= GENERATOR (pos=5 only, OR logic) =========\n",
    "def run_for_pos5(df: pd.DataFrame, base_start_uid: int):\n",
    "    pos_rating = 5\n",
    "    out_dir = BASE_OUT_DIR / str(pos_rating)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_books, book_to_list, book_to_set = prepare_books(df)\n",
    "    GENRES = sorted({g for gl in book_to_list.values() for g in gl})\n",
    "\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "\n",
    "    summary_txt = out_dir / \"summary.txt\"\n",
    "    summary_csv = out_dir / \"summary.csv\"\n",
    "    pairs_overview_csv = out_dir / \"pairs_overview.csv\"\n",
    "    missing_pairs_csv = out_dir / \"missing_pairs.csv\"\n",
    "\n",
    "    # ----- Pair lists -----\n",
    "    all_pairs = list(combinations(GENRES, 2))\n",
    "    total_pairs = len(all_pairs)\n",
    "\n",
    "    # Count how many pairs have â‰¥1 positive book under OR (union)\n",
    "    def count_pairs_with_positives_or(pairs):\n",
    "        cnt = 0\n",
    "        for g1, g2 in pairs:\n",
    "            n_pos_or = sum(1 for b in all_books if (g1 in book_to_set[b] or g2 in book_to_set[b]))\n",
    "            if n_pos_or > 0:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "\n",
    "    total_pairs_with_pos_or = count_pairs_with_positives_or(all_pairs)\n",
    "\n",
    "    with open(summary_txt, \"w\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=== BASELINE ===\\n\")\n",
    "        log.write(f\"ðŸ‘¤ Unique users: {baseline_users:,}\\n\")\n",
    "        log.write(f\"ðŸ§¾ Rows: {baseline_rows:,}\\n\")\n",
    "        log.write(f\"POS_RATING={pos_rating} | ZERO_MODE={ZERO_MODE} | NEG_RATING={NEG_RATING}\\n\")\n",
    "        log.write(f\"Discovered genres ({len(GENRES)}): {GENRES}\\n\\n\")\n",
    "        log.write(\"=== PAIR COUNTS (OR / union, pre-filter) ===\\n\")\n",
    "        log.write(f\"All pairs (combinatorial): {total_pairs:,}\\n\")\n",
    "        log.write(f\"All pairs with â‰¥1 OR-positive book: {total_pairs_with_pos_or:,}\\n\\n\")\n",
    "        log.write(\"Processing mode: ALL pairs (OR / union).\\n\\n\")\n",
    "\n",
    "    rows_summary = []\n",
    "    pairs_overview_rows = []\n",
    "    missing_pairs = []\n",
    "\n",
    "    # ----- Process ALL pairs -----\n",
    "    pair_index = 0\n",
    "    for g1, g2 in all_pairs:\n",
    "        # === OR positives: books having g1 OR g2 ===\n",
    "        books_g1   = [b for b in all_books if g1 in book_to_set[b]]\n",
    "        books_g2   = [b for b in all_books if g2 in book_to_set[b]]\n",
    "        books_both = [b for b in all_books if (g1 in book_to_set[b] and g2 in book_to_set[b])]\n",
    "        pos_books  = sorted(set(books_g1) | set(books_g2))  # union (OR)\n",
    "\n",
    "        n_g1   = len(books_g1)\n",
    "        n_g2   = len(books_g2)\n",
    "        n_both = len(books_both)\n",
    "        n_pos  = len(pos_books)\n",
    "\n",
    "        neg_pool = [b for b in all_books if b not in pos_books]\n",
    "        n_neg_pool = len(neg_pool)\n",
    "\n",
    "        # Per-pair overview row\n",
    "        pairs_overview_rows.append({\n",
    "            \"pair\": f\"{g1} OR {g2}\",\n",
    "            \"g1\": g1,\n",
    "            \"g2\": g2,\n",
    "            \"n_books_g1\": n_g1,\n",
    "            \"n_books_g2\": n_g2,\n",
    "            \"n_books_both_AND\": n_both,\n",
    "            \"n_pos_books_OR\": n_pos,\n",
    "            \"neg_pool\": n_neg_pool\n",
    "        })\n",
    "\n",
    "        if n_pos == 0:\n",
    "            missing_pairs.append({\"pair\": f\"{g1} OR {g2}\", \"g1\": g1, \"g2\": g2})\n",
    "            msg = f\"(skip) No OR-positive books for pair: {g1} OR {g2}\"\n",
    "            print(msg)\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(msg + \"\\n\")\n",
    "            pair_index += 1\n",
    "            continue\n",
    "\n",
    "        # Print + log the counts in your requested format\n",
    "        human_counts = (\n",
    "            f\"#books in {g1}={n_g1:,} | #books in {g2}={n_g2:,} | \"\n",
    "            f\"#{g1}&{g2}={n_both:,} | total OR={n_pos:,} | neg_pool={n_neg_pool:,}\"\n",
    "        )\n",
    "        print(human_counts)\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(human_counts + \"\\n\")\n",
    "\n",
    "        safe_p = f\"{sanitize_fn(g1)}__{sanitize_fn(g2)}\"\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"ðŸ”— Pair (OR): {g1} OR {g2}\\n\")\n",
    "\n",
    "        neg_books_for_all_users = neg_pool  # ZERO_MODE == \"all\"\n",
    "\n",
    "        for run_idx, run_users in enumerate(RUN_USERS):\n",
    "            start_uid = base_start_uid + pair_index * (len(RUN_USERS) * BLOCK) + run_idx * BLOCK\n",
    "            new_uids = list(range(start_uid, start_uid + run_users))\n",
    "\n",
    "            # synth positives (OR)\n",
    "            df_pos = pd.DataFrame({\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_pos)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in pos_books],\n",
    "                RATING_COL: [pos_rating] * (run_users * n_pos),\n",
    "                GENRE_COL:  [\",\".join(sorted(book_to_list.get(b, []))) for _ in new_uids for b in pos_books]\n",
    "            })\n",
    "\n",
    "            # synth negatives\n",
    "            n_neg = len(neg_books_for_all_users)\n",
    "            df_neg = pd.DataFrame({\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_neg)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in neg_books_for_all_users],\n",
    "                RATING_COL: [NEG_RATING] * (run_users * n_neg),\n",
    "                GENRE_COL:  [\",\".join(sorted(book_to_list.get(b, []))) for _ in new_uids for b in neg_books_for_all_users]\n",
    "            })\n",
    "\n",
    "            synth_df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            # filename + optional compression\n",
    "            out_base = f\"forpair_{safe_p}_{run_users}u_pos{pos_rating}_neg{NEG_RATING}_OR.csv\"\n",
    "            out_path = out_dir / (out_base + (\".gz\" if COMPRESSION else \"\"))\n",
    "            combined.to_csv(out_path, index=False, compression=COMPRESSION)\n",
    "\n",
    "            print(f\"âœ… Completed injection file (OR): {out_path.name}\")\n",
    "\n",
    "            rows_added = len(synth_df)\n",
    "            rows_pos = len(df_pos)\n",
    "            rows_neg = len(df_neg)\n",
    "            new_users_total = combined[USER_COL].nunique()\n",
    "\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(\n",
    "                    f\"  users={run_users:>5} â†’ +rows={rows_added:>12,} \"\n",
    "                    f\"(pos={rows_pos:,}, neg={rows_neg:,}) | \"\n",
    "                    f\"new_rows={len(combined):,} | new_users={new_users_total:,} | \"\n",
    "                    f\"outfile={out_path.name}\\n\"\n",
    "                )\n",
    "\n",
    "            rows_summary.append({\n",
    "                \"pos_rating\": pos_rating,\n",
    "                \"pair_or\": f\"{g1} OR {g2}\",\n",
    "                \"g1\": g1,\n",
    "                \"g2\": g2,\n",
    "                \"run_users\": run_users,\n",
    "                \"n_books_g1\": n_g1,\n",
    "                \"n_books_g2\": n_g2,\n",
    "                \"n_books_both_AND\": n_both,\n",
    "                \"n_pos_books_OR\": n_pos,\n",
    "                \"n_neg_books_per_user\": n_neg,\n",
    "                \"rows_added\": rows_added,\n",
    "                \"rows_pos\": rows_pos,\n",
    "                \"rows_neg\": rows_neg,\n",
    "                \"zero_mode\": ZERO_MODE,\n",
    "                \"output_csv\": str(out_path)\n",
    "            })\n",
    "\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(\"\\n\")\n",
    "\n",
    "        pair_index += 1\n",
    "\n",
    "    # ----- Outputs -----\n",
    "    if rows_summary:\n",
    "        pd.DataFrame(rows_summary).to_csv(summary_csv, index=False)\n",
    "    if pairs_overview_rows:\n",
    "        pd.DataFrame(pairs_overview_rows).sort_values([\"g1\",\"g2\"]).to_csv(pairs_overview_csv, index=False)\n",
    "    if missing_pairs:\n",
    "        pd.DataFrame(missing_pairs).to_csv(missing_pairs_csv, index=False)\n",
    "\n",
    "    with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=\"*80 + \"\\n\")\n",
    "        log.write(f\"Grand total injected rows (ALL pairs, OR, pos=5): {sum(r['rows_added'] for r in rows_summary):,}\\n\")\n",
    "        log.write(f\"Pairs overview (ALL pairs, OR): {pairs_overview_csv}\\n\")\n",
    "        log.write(f\"Missing pairs (ALL pairs, OR): {missing_pairs_csv}\\n\\n\")\n",
    "\n",
    "    print(f\"âœ… Done for pos=5 (ALL pairs, OR). Out: {out_dir}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Loading original CSV...\")\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    df[USER_COL]   = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]   = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "    df[GENRE_COL]  = df[GENRE_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "    run_for_pos5(df, base_start_uid)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3827820173.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3963919/3827820173.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git add .\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git add .\n",
    "git commit -m \"figures\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/moshtasa/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in /usr/local/anaconda3/lib/python3.9/site-packages (3.5.2)\n",
      "Collecting matplotlib-venn\n",
      "  Downloading matplotlib-venn-1.1.2.tar.gz (40 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/anaconda3/lib/python3.9/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/moshtasa/.local/lib/python3.9/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/moshtasa/.local/lib/python3.9/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/anaconda3/lib/python3.9/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/anaconda3/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy in /home/moshtasa/.local/lib/python3.9/site-packages (from matplotlib-venn) (1.13.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Building wheels for collected packages: matplotlib-venn\n",
      "  Building wheel for matplotlib-venn (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for matplotlib-venn: filename=matplotlib_venn-1.1.2-py3-none-any.whl size=45388 sha256=d10e4b4d0d4f88689c63e3e213bd8834d2486a93341449238bc0e29b61ee97e4\n",
      "  Stored in directory: /home/moshtasa/.cache/pip/wheels/86/29/d8/0f3c5a37c967a34fb40aaabd414f92104d2ad5fb149c0114a1\n",
      "Successfully built matplotlib-venn\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/moshtasa/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: matplotlib-venn\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/moshtasa/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed matplotlib-venn-1.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib matplotlib-venn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 78 Venn figure(s) + calc files into: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1028/result/5_pair/OR_ven\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# make_pair_venns_1028.py\n",
    "#\n",
    "# Parse OR_Injection.log and save one Venn diagram + calc summary per pair.\n",
    "# Outputs:\n",
    "#   <OUT_DIR>/<A>__<B>_pair_explanation.png\n",
    "#   <OUT_DIR>/<A>__<B>_pair_explanation.txt\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to use matplotlib-venn; if unavailable, fallback later\n",
    "_HAVE_VENN = True\n",
    "try:\n",
    "    from matplotlib_venn import venn2\n",
    "except Exception:\n",
    "    _HAVE_VENN = False\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "LOG_PATH = Path(\"/home/moshtasa/Research/phd-svd-recsys/OR_Injection.log\")\n",
    "OUT_DIR  = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1028/result/5_pair/OR_ven\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DPI = 600\n",
    "\n",
    "# Normalize genre for filenames (e.g., \"Science Fiction\" -> \"Science_Fiction\"; \"Children's\" -> \"Children_s\")\n",
    "def norm_genre_for_filename(name: str) -> str:\n",
    "    s = re.sub(r\"[^0-9A-Za-z]+\", \"_\", name)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "# Clean numbers like \"3,006\" -> 3006\n",
    "def to_int(num_str: str) -> int:\n",
    "    return int(num_str.replace(\",\", \"\").strip())\n",
    "\n",
    "# Regex to capture a header line with counts\n",
    "LINE_RE = re.compile(\n",
    "    r\"#books in (?P<a>[^=]+)=(?P<na>[0-9,]+)\\s*\\|\\s*\"\n",
    "    r\"#books in (?P<b>[^=]+)=(?P<nb>[0-9,]+)\\s*\\|\\s*\"\n",
    "    r\"#(?P<a2>[^&]+)&(?P<b2>[^=]+)=(?P<both>[0-9,]+)\\s*\\|\\s*\"\n",
    "    r\"(?:total OR|total\\s*OR)=(?P<union>[0-9,]+)\\s*\\|\\s*\"\n",
    "    r\"(?:neg_pool|negative\\s*pool)=(?P<neg>[0-9,]+)\"\n",
    ")\n",
    "\n",
    "def draw_venn_matplotlib_fallback(a_label, b_label, only_a, only_b, both, union_total, neg_pool, save_path: Path):\n",
    "    # Draw two overlapping circles with plain matplotlib (no extra deps)\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Circles\n",
    "    c1 = plt.Circle((-0.6, 0), 1.0, alpha=0.3)\n",
    "    c2 = plt.Circle((0.6, 0), 1.0, alpha=0.3)\n",
    "    ax.add_patch(c1)\n",
    "    ax.add_patch(c2)\n",
    "\n",
    "    # Labels\n",
    "    ax.text(-0.6, 1.15, a_label, ha='center', va='bottom', fontsize=12)\n",
    "    ax.text( 0.6, 1.15, b_label, ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    # Region counts\n",
    "    ax.text(-1.0, 0.0, str(only_a), ha='center', va='center', fontsize=12)\n",
    "    ax.text( 1.0, 0.0, str(only_b), ha='center', va='center', fontsize=12)\n",
    "    ax.text( 0.0, 0.0, str(both),   ha='center', va='center', fontsize=12)\n",
    "\n",
    "    # Title and annotations\n",
    "    ax.set_title(f\"{a_label}â€“{b_label} Genre Overlap\", fontsize=13)\n",
    "    ax.text(-1.45, -1.25, f\"Total OR = {union_total}\\nNeg. pool = {neg_pool}\",\n",
    "            ha='left', va='top', fontsize=10)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=DPI, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def draw_venn(a_label, b_label, only_a, only_b, both, union_total, neg_pool, save_path: Path):\n",
    "    if _HAVE_VENN:\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        venn2(subsets=(only_a, only_b, both), set_labels=(a_label, b_label))\n",
    "        plt.title(f\"{a_label}â€“{b_label} Genre Overlap\")\n",
    "        plt.text(-0.95, -0.90, f\"Total OR = {union_total}\\nNeg. pool = {neg_pool}\",\n",
    "                 ha='left', va='top', fontsize=10)\n",
    "        plt.savefig(save_path, dpi=DPI, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        draw_venn_matplotlib_fallback(a_label, b_label, only_a, only_b, both, union_total, neg_pool, save_path)\n",
    "\n",
    "def write_txt(calc_path: Path, a_label, b_label, na, nb, both, only_a, only_b, union_total, neg_pool, union_check_ok):\n",
    "    with open(calc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{a_label} / {b_label} â€” Venn calculations\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(f\"#books in {a_label} = {na}\\n\")\n",
    "        f.write(f\"#books in {b_label} = {nb}\\n\")\n",
    "        f.write(f\"#both ({a_label} âˆ© {b_label}) = {both}\\n\")\n",
    "        f.write(f\"only {a_label} = {na} - {both} = {only_a}\\n\")\n",
    "        f.write(f\"only {b_label} = {nb} - {both} = {only_b}\\n\")\n",
    "        f.write(f\"union ({a_label} âˆª {b_label}) = {only_a} + {only_b} + {both} = {only_a + only_b + both}\\n\")\n",
    "        f.write(f\"union (from log) = {union_total}  [{'OK' if union_check_ok else 'MISMATCH'}]\\n\")\n",
    "        f.write(f\"negative pool (outside union) = {neg_pool}\\n\")\n",
    "\n",
    "def main():\n",
    "    if not LOG_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Log file not found: {LOG_PATH}\")\n",
    "\n",
    "    made = 0\n",
    "    with open(LOG_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "        for line in fh:\n",
    "            m = LINE_RE.search(line)\n",
    "            if not m:\n",
    "                continue\n",
    "\n",
    "            a = m.group(\"a\").strip()\n",
    "            b = m.group(\"b\").strip()\n",
    "\n",
    "            na = to_int(m.group(\"na\"))\n",
    "            nb = to_int(m.group(\"nb\"))\n",
    "            both = to_int(m.group(\"both\"))\n",
    "            union_total = to_int(m.group(\"union\"))\n",
    "            neg_pool = to_int(m.group(\"neg\"))\n",
    "\n",
    "            only_a = na - both\n",
    "            only_b = nb - both\n",
    "            union_calc = only_a + only_b + both\n",
    "            union_check_ok = (union_calc == union_total)\n",
    "\n",
    "            base = f\"{norm_genre_for_filename(a)}__{norm_genre_for_filename(b)}_pair_explanation\"\n",
    "            png_path = OUT_DIR / f\"{base}.png\"\n",
    "            txt_path = OUT_DIR / f\"{base}.txt\"\n",
    "\n",
    "            draw_venn(a, b, only_a, only_b, both, union_total, neg_pool, png_path)\n",
    "            write_txt(txt_path, a, b, na, nb, both, only_a, only_b, union_total, neg_pool, union_check_ok)\n",
    "            made += 1\n",
    "\n",
    "    print(f\"âœ… Generated {made} Venn figure(s) + calc files into: {OUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
