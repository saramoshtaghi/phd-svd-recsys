{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original CSV...\n",
      "✅ Completed injection file: fpair_Adult__Classics_2u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Classics_4u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Classics_6u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Classics_25u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Classics_50u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Classics_100u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Classics_200u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Classics_300u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Classics_500u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Classics_1000u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Drama_2u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Drama_4u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Drama_6u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Drama_25u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Drama_50u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Drama_100u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Drama_200u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Drama_300u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Drama_500u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Drama_1000u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Fantasy_2u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Fantasy_4u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Fantasy_6u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Fantasy_25u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Fantasy_50u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Fantasy_100u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Fantasy_200u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Fantasy_300u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Fantasy_500u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Fantasy_1000u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Historical_2u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Historical_4u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Historical_6u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Historical_25u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Historical_50u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Historical_100u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Historical_200u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Historical_300u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Historical_500u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Historical_1000u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Mystery_2u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Mystery_4u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Mystery_6u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Mystery_25u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Mystery_50u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Mystery_100u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Mystery_200u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Mystery_300u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Mystery_500u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Mystery_1000u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Nonfiction_2u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Nonfiction_4u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Nonfiction_6u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Nonfiction_25u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Nonfiction_50u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Nonfiction_100u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Nonfiction_200u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Nonfiction_300u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Nonfiction_500u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Nonfiction_1000u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Romance_2u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Romance_4u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Romance_6u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Romance_25u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Romance_50u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Romance_100u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Romance_200u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Romance_300u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Romance_500u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Romance_1000u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Thriller_2u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Thriller_4u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Thriller_6u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Thriller_25u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Thriller_50u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Thriller_100u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Thriller_200u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Thriller_300u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Thriller_500u_pos5_neg1.csv\n",
      "✅ Completed injection file: fpair_Adult__Thriller_1000u_pos5_neg1.csv\n",
      "✅ Done for pos=5 (adult-only). Out: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1020/data/PAIR_INJECTION/5\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_pair_bias_pos5_neg1_all_smallcohorts.py\n",
    "# Changes:\n",
    "#   • Count all pairs (and adult-only subsets) before generating.\n",
    "#   • Generate ONLY “adult-like” pairs (any genre containing \"adult\", case-insensitive).\n",
    "#   • Optional gzip compression to save disk space.\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "INPUT_CSV   = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "BASE_OUT_DIR= Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1020/data/PAIR_INJECTION\")\n",
    "\n",
    "GENRE_COL   = \"genres\"\n",
    "USER_COL    = \"user_id\"\n",
    "BOOK_COL    = \"book_id\"\n",
    "RATING_COL  = \"rating\"\n",
    "\n",
    "RUN_USERS   = [2, 4, 6, 25, 50, 100, 200, 300, 500, 1000]\n",
    "ZERO_MODE   = \"all\"\n",
    "NEG_RATING  = 1\n",
    "BLOCK       = 1_000_000\n",
    "\n",
    "# Space-saving: write gzip-compressed CSVs (set to None to disable)\n",
    "COMPRESSION = None  # or None\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def parse_genres(cell: str):\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # try list/tuple literal first\n",
    "    if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "        try:\n",
    "            import ast\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # else, split by common separators\n",
    "    for sep in [\",\", \"|\", \";\", \"//\", \"/\"]:\n",
    "        if sep in s:\n",
    "            parts = [p.strip() for p in s.split(sep) if p.strip()]\n",
    "            seen, out = set(), []\n",
    "            for p in parts:\n",
    "                if p not in seen:\n",
    "                    out.append(p); seen.add(p)\n",
    "            return out\n",
    "    return [s]\n",
    "\n",
    "def prepare_books(df: pd.DataFrame):\n",
    "    books = df[[BOOK_COL, GENRE_COL]].drop_duplicates(subset=[BOOK_COL]).copy()\n",
    "    books[\"genre_list\"] = books[GENRE_COL].apply(parse_genres)\n",
    "    books = books[books[\"genre_list\"].map(len) > 0].copy()\n",
    "    book_to_list = dict(zip(books[BOOK_COL].astype(int), books[\"genre_list\"]))\n",
    "    book_to_set  = {int(b): set(l) for b, l in book_to_list.items()}\n",
    "    all_books = sorted(book_to_list.keys())\n",
    "    return all_books, book_to_list, book_to_set\n",
    "\n",
    "def is_adult_like(genre: str) -> bool:\n",
    "    return \"adult\" in (genre or \"\").lower()  # matches \"Adult\", \"Young Adult\", etc.\n",
    "\n",
    "# ========= GENERATOR (pos=5 only) =========\n",
    "def run_for_pos5(df: pd.DataFrame, base_start_uid: int):\n",
    "    pos_rating = 5\n",
    "    out_dir = BASE_OUT_DIR / str(pos_rating)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_books, book_to_list, book_to_set = prepare_books(df)\n",
    "    GENRES = sorted({g for gl in book_to_list.values() for g in gl})\n",
    "\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "\n",
    "    summary_txt = out_dir / \"summary.txt\"\n",
    "    summary_csv = out_dir / \"summary.csv\"\n",
    "    pairs_overview_csv = out_dir / \"pairs_overview.csv\"\n",
    "    missing_pairs_csv = out_dir / \"missing_pairs.csv\"\n",
    "\n",
    "    # ----- Pair counts (global, before filtering) -----\n",
    "    all_pairs = list(combinations(GENRES, 2))\n",
    "    total_pairs = len(all_pairs)\n",
    "\n",
    "    # Count how many pairs have >=1 positive book anywhere\n",
    "    def count_pairs_with_positives(pairs):\n",
    "        cnt = 0\n",
    "        for g1, g2 in pairs:\n",
    "            # fast short-circuit using sets per book\n",
    "            n_pos = sum(1 for b in all_books if (g1 in book_to_set[b] and g2 in book_to_set[b]))\n",
    "            if n_pos > 0:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "\n",
    "    # Adult-like subsets for counting\n",
    "    adult_pairs_all = [(g1, g2) for (g1, g2) in all_pairs if is_adult_like(g1) or is_adult_like(g2)]\n",
    "    total_adult_pairs = len(adult_pairs_all)\n",
    "\n",
    "    # Count positives\n",
    "    total_pairs_with_pos = count_pairs_with_positives(all_pairs)\n",
    "    total_adult_pairs_with_pos = count_pairs_with_positives(adult_pairs_all)\n",
    "\n",
    "    with open(summary_txt, \"w\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=== BASELINE ===\\n\")\n",
    "        log.write(f\"👤 Unique users: {baseline_users:,}\\n\")\n",
    "        log.write(f\"🧾 Rows: {baseline_rows:,}\\n\")\n",
    "        log.write(f\"POS_RATING={pos_rating} | ZERO_MODE={ZERO_MODE} | NEG_RATING={NEG_RATING}\\n\")\n",
    "        log.write(f\"Discovered genres ({len(GENRES)}): {GENRES}\\n\\n\")\n",
    "        log.write(\"=== PAIR COUNTS (pre-filter) ===\\n\")\n",
    "        log.write(f\"All pairs (combinatorial): {total_pairs:,}\\n\")\n",
    "        log.write(f\"All pairs with ≥1 positive book: {total_pairs_with_pos:,}\\n\")\n",
    "        log.write(f\"Adult-like pairs (any side contains 'adult'): {total_adult_pairs:,}\\n\")\n",
    "        log.write(f\"Adult-like pairs with ≥1 positive book: {total_adult_pairs_with_pos:,}\\n\\n\")\n",
    "        log.write(\"Processing mode: ADULT-ONLY pairs to save disk space.\\n\\n\")\n",
    "\n",
    "    rows_summary = []\n",
    "    pairs_overview_rows = []\n",
    "    missing_pairs = []\n",
    "\n",
    "    # ----- Process ONLY adult-like pairs -----\n",
    "    pair_index = 0\n",
    "    for g1, g2 in adult_pairs_all:\n",
    "        # positives: books having BOTH g1 and g2\n",
    "        pos_books = [b for b in all_books if (g1 in book_to_set[b] and g2 in book_to_set[b])]\n",
    "        n_pos = len(pos_books)\n",
    "        neg_pool = [b for b in all_books if b not in pos_books]\n",
    "        n_neg_pool = len(neg_pool)\n",
    "\n",
    "        pairs_overview_rows.append({\"pair\": f\"{g1} + {g2}\", \"g1\": g1, \"g2\": g2,\n",
    "                                    \"n_pos_books\": n_pos, \"neg_pool\": n_neg_pool})\n",
    "        if n_pos == 0:\n",
    "            missing_pairs.append({\"pair\": f\"{g1} + {g2}\", \"g1\": g1, \"g2\": g2})\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"(skip) No overlapping books for pair: {g1} + {g2}\\n\")\n",
    "            pair_index += 1\n",
    "            continue\n",
    "\n",
    "        safe_p = f\"{sanitize_fn(g1)}__{sanitize_fn(g2)}\"\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"🔗 Pair: {g1} + {g2} | positives (pair-books) = {n_pos} | neg_pool = {n_neg_pool}\\n\")\n",
    "\n",
    "        neg_books_for_all_users = neg_pool  # ZERO_MODE == \"all\"\n",
    "\n",
    "        for run_idx, run_users in enumerate(RUN_USERS):\n",
    "            start_uid = base_start_uid + pair_index * (len(RUN_USERS) * BLOCK) + run_idx * BLOCK\n",
    "            new_uids = list(range(start_uid, start_uid + run_users))\n",
    "\n",
    "            # synth positives\n",
    "            df_pos = pd.DataFrame({\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_pos)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in pos_books],\n",
    "                RATING_COL: [pos_rating] * (run_users * n_pos),\n",
    "                GENRE_COL:  [\",\".join(sorted(book_to_list.get(b, []))) for _ in new_uids for b in pos_books]\n",
    "            })\n",
    "\n",
    "            # synth negatives\n",
    "            n_neg = len(neg_books_for_all_users)\n",
    "            df_neg = pd.DataFrame({\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_neg)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in neg_books_for_all_users],\n",
    "                RATING_COL: [NEG_RATING] * (run_users * n_neg),\n",
    "                GENRE_COL:  [\",\".join(sorted(book_to_list.get(b, []))) for _ in new_uids for b in neg_books_for_all_users]\n",
    "            })\n",
    "\n",
    "            synth_df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            # filename + optional compression\n",
    "            out_base = f\"fpair_{safe_p}_{run_users}u_pos{pos_rating}_neg{NEG_RATING}.csv\"\n",
    "            out_path = out_dir / (out_base + (\".gz\" if COMPRESSION else \"\"))\n",
    "            combined.to_csv(out_path, index=False, compression=COMPRESSION)\n",
    "\n",
    "            print(f\"✅ Completed injection file: {out_path.name}\")\n",
    "\n",
    "            rows_added = len(synth_df)\n",
    "            rows_pos = len(df_pos)\n",
    "            rows_neg = len(df_neg)\n",
    "            new_users_total = combined[USER_COL].nunique()\n",
    "\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(\n",
    "                    f\"  users={run_users:>5} → +rows={rows_added:>12,} (pos={rows_pos:,}, neg={rows_neg:,}) | \"\n",
    "                    f\"new_rows={len(combined):,} | new_users={new_users_total:,} | outfile={out_path.name}\\n\"\n",
    "                )\n",
    "\n",
    "            rows_summary.append({\n",
    "                \"pos_rating\": pos_rating,\n",
    "                \"pair\": f\"{g1} + {g2}\",\n",
    "                \"g1\": g1,\n",
    "                \"g2\": g2,\n",
    "                \"run_users\": run_users,\n",
    "                \"n_pos_books\": n_pos,\n",
    "                \"n_neg_books_per_user\": n_neg,\n",
    "                \"rows_added\": rows_added,\n",
    "                \"rows_pos\": rows_pos,\n",
    "                \"rows_neg\": rows_neg,\n",
    "                \"zero_mode\": ZERO_MODE,\n",
    "                \"output_csv\": str(out_path)\n",
    "            })\n",
    "\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(\"\\n\")\n",
    "\n",
    "        pair_index += 1\n",
    "\n",
    "    # ----- Outputs -----\n",
    "    if rows_summary:\n",
    "        pd.DataFrame(rows_summary).to_csv(summary_csv, index=False)\n",
    "    if pairs_overview_rows:\n",
    "        pd.DataFrame(pairs_overview_rows).sort_values([\"g1\",\"g2\"]).to_csv(pairs_overview_csv, index=False)\n",
    "    if missing_pairs:\n",
    "        pd.DataFrame(missing_pairs).to_csv(missing_pairs_csv, index=False)\n",
    "\n",
    "    with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=\"*80 + \"\\n\")\n",
    "        log.write(f\"Grand total injected rows (ADULT-ONLY, pos=5): {sum(r['rows_added'] for r in rows_summary):,}\\n\")\n",
    "        log.write(f\"Pairs overview (adult-only): {pairs_overview_csv}\\n\")\n",
    "        log.write(f\"Missing pairs (adult-only): {missing_pairs_csv}\\n\\n\")\n",
    "\n",
    "    print(f\"✅ Done for pos=5 (adult-only). Out: {out_dir}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Loading original CSV...\")\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    df[USER_COL]   = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]   = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "    df[GENRE_COL]  = df[GENRE_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "    run_for_pos5(df, base_start_uid)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
