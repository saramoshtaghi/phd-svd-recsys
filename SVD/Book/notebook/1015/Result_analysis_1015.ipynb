{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2,4,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Found 72 pairs across poisoned dir /5.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1248483/3387709127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1248483/3387709127.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moriginal_df_by_k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0moriginal_df_by_k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_rec_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# Collect tall rows for pos=5 + Original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1248483/3387709127.py\u001b[0m in \u001b[0;36mload_rec_csv\u001b[0;34m(fp)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_rec_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Ensure genres_all exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"genres_all\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# summarize_pairs_figs_per_pos_1015.py\n",
    "#\n",
    "# One figure per (pair, pos=5), each with 3 bins (K in {15,25,35}).\n",
    "# Bars inside each bin: Original, n=2, n=4, n=6.\n",
    "# Matches your file naming:\n",
    "#   ORIGINAL_{K}recommendation.csv                          (under SVD_pair root)\n",
    "#   fpair_<A>__<B>_{2|4|6}u_pos5_neg1_all_{K}recommendation.csv (under SVD_pair/5)\n",
    "#\n",
    "# Output:\n",
    "#   /.../1015/result/figures/5/\n",
    "#       - <pair_slug>__pos5.png\n",
    "#       - summary_pos5.csv\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple, List, Set, Optional, Dict\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # headless-safe\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======== PATHS (1015) ========\n",
    "BASE = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/SVD_pair\")\n",
    "ORIG_DIR = BASE                      # ORIGINAL_{K}recommendation.csv lives here\n",
    "POS_DIRS = [BASE / \"5\"]              # poisoned branch /5 only (pos=5, neg=1)\n",
    "\n",
    "OUT_ROOT = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/result/figures\")\n",
    "OUT_5 = OUT_ROOT / \"5\"\n",
    "\n",
    "K_LIST = [15, 25, 35]\n",
    "N_LIST = [2, 4, 6]  # << changed to 2,4,6\n",
    "\n",
    "# ======== HELPERS ========\n",
    "def slugify_pair(a: str, b: str) -> str:\n",
    "    import re as _re\n",
    "    def sg(x): return _re.sub(r\"[^A-Za-z0-9]+\", \"_\", x).strip(\"_\").lower()\n",
    "    a2, b2 = sorted([a, b], key=lambda x: x.lower())\n",
    "    return f\"{sg(a2)}__{sg(b2)}\"\n",
    "\n",
    "def normalize_tag(t: str) -> str:\n",
    "    t = str(t).strip().replace(\"_\", \" \")\n",
    "    if t == \"Children s\":\n",
    "        t = \"Children's\"\n",
    "    low = t.lower()\n",
    "    if low == \"science fiction\": t = \"Science Fiction\"\n",
    "    elif low == \"historical\":    t = \"Historical\"\n",
    "    elif low == \"nonfiction\":    t = \"Nonfiction\"\n",
    "    elif low == \"thriller\":      t = \"Thriller\"\n",
    "    elif low == \"drama\":         t = \"Drama\"\n",
    "    elif low == \"fantasy\":       t = \"Fantasy\"\n",
    "    elif low == \"mystery\":       t = \"Mystery\"\n",
    "    elif low == \"romance\":       t = \"Romance\"\n",
    "    elif low == \"horror\":        t = \"Horror\"\n",
    "    elif low == \"classics\":      t = \"Classics\"\n",
    "    elif low == \"adventure\":     t = \"Adventure\"\n",
    "    elif low == \"adult\":         t = \"Adult\"\n",
    "    return t\n",
    "\n",
    "def book_has_both(gen_all: str, A: str, B: str) -> bool:\n",
    "    if pd.isna(gen_all) or not str(gen_all).strip():\n",
    "        return False\n",
    "    parts = [x.strip() for x in str(gen_all).split(\",\") if str(x).strip()]\n",
    "    tags = [normalize_tag(x) for x in parts]\n",
    "    return (A in tags) and (B in tags)\n",
    "\n",
    "def per_user_avg_pair_count(rec_df: pd.DataFrame, A: str, B: str) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    rec_df columns expected: user_id, book_id, genres_all\n",
    "    Returns (average_count_per_user, num_users_in_this_csv)\n",
    "    \"\"\"\n",
    "    need = {\"user_id\", \"book_id\", \"genres_all\"}\n",
    "    missing = need - set(rec_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV missing columns: {missing}\")\n",
    "    users = rec_df[\"user_id\"].drop_duplicates().sort_values()\n",
    "    users_count = int(users.shape[0])\n",
    "    mask = rec_df[\"genres_all\"].apply(lambda s: book_has_both(s, A, B))\n",
    "    pair_df = rec_df[mask].copy()\n",
    "    if pair_df.empty:\n",
    "        return (0.0, users_count)\n",
    "    per_user = (\n",
    "        pair_df.groupby(\"user_id\", as_index=False)[\"book_id\"]\n",
    "               .count()\n",
    "               .rename(columns={\"book_id\": \"count\"})\n",
    "    )\n",
    "    all_users = pd.DataFrame({\"user_id\": users})\n",
    "    all_users = all_users.merge(per_user, on=\"user_id\", how=\"left\").fillna({\"count\": 0})\n",
    "    return (float(all_users[\"count\"].mean()), users_count)\n",
    "\n",
    "def injected_files_for_pair_k_n(pos_dir: Path, A: str, B: str, k: int, n: int) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Match fpair_<A>__<B>_{n}u_pos5_neg1_all_{k}recommendation.csv (order-insensitive for A/B).\n",
    "    \"\"\"\n",
    "    aT = re.sub(r\"_+\", \"_\", A.replace(\" \", \"_\").replace(\"'\", \"_\")).strip(\"_\")\n",
    "    bT = re.sub(r\"_+\", \"_\", B.replace(\" \", \"_\").replace(\"'\", \"_\")).strip(\"_\")\n",
    "    pat1 = re.compile(rf\"^fpair_{aT}__{bT}_{n}u_pos5_neg1_all_{k}recommendation\\.csv$\")\n",
    "    pat2 = re.compile(rf\"^fpair_{bT}__{aT}_{n}u_pos5_neg1_all_{k}recommendation\\.csv$\")\n",
    "    out: List[Path] = []\n",
    "    for p in pos_dir.glob(f\"*neg1_all_{k}recommendation.csv\"):\n",
    "        if pat1.match(p.name) or pat2.match(p.name):\n",
    "            out.append(p)\n",
    "    return sorted(out)\n",
    "\n",
    "def discover_pairs_from_dirs(pos_dirs: Iterable[Path],\n",
    "                             k_list: Iterable[int],\n",
    "                             n_list: Iterable[int]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse file names under /5, return unique unordered (A,B) pairs present for valid K and n.\n",
    "    \"\"\"\n",
    "    pair_set: Set[Tuple[str, str]] = set()\n",
    "    regex = re.compile(\n",
    "        r\"^fpair_(?P<A>[A-Za-z0-9_']+)__(?P<B>[A-Za-z0-9_']+)_(?P<N>\\d+)u_pos5_neg1_all_(?P<K>\\d+)recommendation\\.csv$\"\n",
    "    )\n",
    "    valid_k = set(map(int, k_list))\n",
    "    valid_n = set(map(int, n_list))\n",
    "    for pos_dir in pos_dirs:\n",
    "        for p in pos_dir.glob(\"fpair_*u_pos5_neg1_all_*recommendation.csv\"):\n",
    "            m = regex.match(p.name)\n",
    "            if not m:\n",
    "                continue\n",
    "            k = int(m.group(\"K\"))\n",
    "            n = int(m.group(\"N\"))\n",
    "            if k not in valid_k or n not in valid_n:\n",
    "                continue\n",
    "            A_disp = normalize_tag(m.group(\"A\").replace(\"_\", \" \"))\n",
    "            B_disp = normalize_tag(m.group(\"B\").replace(\"_\", \" \"))\n",
    "            a_c, b_c = sorted([A_disp, B_disp], key=lambda x: x.lower())\n",
    "            pair_set.add((a_c, b_c))\n",
    "    return sorted(pair_set, key=lambda ab: (ab[0].lower(), ab[1].lower()))\n",
    "\n",
    "def original_file_for_k(orig_dir: Path, k: int) -> Optional[Path]:\n",
    "    p = orig_dir / f\"ORIGINAL_{k}recommendation.csv\"\n",
    "    return p if p.exists() else None\n",
    "\n",
    "def load_rec_csv(fp: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp, low_memory=False)\n",
    "    # Ensure genres_all exists\n",
    "    if \"genres_all\" not in df.columns:\n",
    "        if \"genre_g1\" in df.columns and \"genre_g2\" in df.columns:\n",
    "            df[\"genres_all\"] = df[[\"genre_g1\", \"genre_g2\"]].fillna(\"\").agg(\n",
    "                lambda x: \", \".join([t for t in [x[\"genre_g1\"], x[\"genre_g2\"]] if str(t).strip()]), axis=1\n",
    "            )\n",
    "        else:\n",
    "            df[\"genres_all\"] = \"\"\n",
    "    return df\n",
    "\n",
    "def plot_pair_pos_three_bins(A: str, B: str, pos_label: str,\n",
    "                             data_by_k: Dict[int, Dict[str, float]],\n",
    "                             out_png: Path):\n",
    "    \"\"\"\n",
    "    data_by_k: {K: {\"Original\": v0, \"2\": v2, \"4\": v4, \"6\": v6}}\n",
    "    \"\"\"\n",
    "    ks = sorted(data_by_k.keys())\n",
    "    groups = [\"Original\", \"2\", \"4\", \"6\"]\n",
    "    vals = [[data_by_k.get(k, {}).get(g, 0.0) for g in groups] for k in ks]\n",
    "\n",
    "    # Plot: 3 bins (ks), 4 bars each\n",
    "    width = 0.2\n",
    "    x = list(range(len(ks)))\n",
    "    plt.figure(figsize=(8.4, 4.4), dpi=160)\n",
    "\n",
    "    for j, g in enumerate(groups):\n",
    "        offs = [i + (j - 1.5)*width for i in x]  # center bars around each K bin\n",
    "        plt.bar(offs, [vals[i][j] for i in range(len(ks))], width=width,\n",
    "                label=(\"n=\"+g if g!=\"Original\" else \"Original\"))\n",
    "\n",
    "    plt.xticks(x, [f\"Top-{k}\" for k in ks])\n",
    "    plt.ylabel(\"Avg # of pair-books in Top-K per user\")\n",
    "    plt.title(f\"{A} + {B} — POS={pos_label}\")\n",
    "    plt.legend(ncol=4, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(out_png)\n",
    "    plt.close()\n",
    "\n",
    "# ======== MAIN ========\n",
    "def main():\n",
    "    OUT_5.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Discover all pairs from poisoned dirs\n",
    "    PAIRS = discover_pairs_from_dirs(POS_DIRS, K_LIST, N_LIST)\n",
    "    if not PAIRS:\n",
    "        print(\"[WARN] No pairs found in /5.\")\n",
    "        return\n",
    "    print(f\"[OK] Found {len(PAIRS)} pairs across poisoned dir /5.\")\n",
    "\n",
    "    # Preload ORIGINAL per K (same files for all pairs; per-pair counts differ)\n",
    "    original_df_by_k: Dict[int, Optional[pd.DataFrame]] = {}\n",
    "    for k in K_LIST:\n",
    "        fp = original_file_for_k(ORIG_DIR, k)\n",
    "        if fp is None:\n",
    "            print(f\"[WARN] Missing ORIGINAL_{k}recommendation.csv in {ORIG_DIR}\")\n",
    "            original_df_by_k[k] = None\n",
    "        else:\n",
    "            original_df_by_k[k] = load_rec_csv(fp)\n",
    "\n",
    "    # Collect tall rows for pos=5 + Original\n",
    "    tall_rows = []  # columns: A,B,pair,pos,K,n,avg_count,users,source?\n",
    "\n",
    "    # ORIGINAL tall rows (n=\"ORIGINAL\")\n",
    "    for (A, B) in PAIRS:\n",
    "        for k in K_LIST:\n",
    "            dfO = original_df_by_k.get(k)\n",
    "            if dfO is None:\n",
    "                continue\n",
    "            avgc, users_cnt = per_user_avg_pair_count(dfO, A, B)\n",
    "            tall_rows.append({\n",
    "                \"A\": A, \"B\": B, \"pair\": slugify_pair(A,B),\n",
    "                \"pos\": \"ORIGINAL\", \"K\": k, \"n\": \"ORIGINAL\",\n",
    "                \"avg_count\": avgc, \"users\": users_cnt\n",
    "            })\n",
    "\n",
    "    # Poisoned tall rows (per N separately; no averaging across files)\n",
    "    for pos_dir in POS_DIRS:\n",
    "        pos_label = pos_dir.name  # \"5\"\n",
    "        for (A, B) in PAIRS:\n",
    "            for k in K_LIST:\n",
    "                for n in N_LIST:\n",
    "                    files = injected_files_for_pair_k_n(pos_dir, A, B, k, n)\n",
    "                    if not files:\n",
    "                        continue\n",
    "                    for f in files:\n",
    "                        try:\n",
    "                            df = load_rec_csv(f)\n",
    "                            avgc, users_cnt = per_user_avg_pair_count(df, A, B)\n",
    "                            tall_rows.append({\n",
    "                                \"A\": A, \"B\": B, \"pair\": slugify_pair(A,B),\n",
    "                                \"pos\": pos_label, \"K\": k, \"n\": str(n),\n",
    "                                \"avg_count\": avgc, \"users\": users_cnt,\n",
    "                                \"source\": f.name\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(f\"[ERROR] Reading {f}: {e}\")\n",
    "\n",
    "    if not tall_rows:\n",
    "        print(\"[WARN] No rows computed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    dft = pd.DataFrame(tall_rows)\n",
    "    dft[\"n\"] = dft[\"n\"].astype(str)\n",
    "    dft.sort_values(by=[\"pair\",\"pos\",\"K\",\"n\"], inplace=True)\n",
    "\n",
    "    # --- Save CSV (pos=5 + Original) ---\n",
    "    dft_pos5 = dft[dft[\"pos\"].isin([\"ORIGINAL\",\"5\"])].copy()\n",
    "    dft_pos5.to_csv(OUT_5 / \"summary_pos5.csv\", index=False)\n",
    "    print(f\"[OK] Saved CSV: {OUT_5/'summary_pos5.csv'}\")\n",
    "\n",
    "    # --- Make figures: one per (pair, pos=5) ---\n",
    "    for (A, B) in PAIRS:\n",
    "        pair_slug = slugify_pair(A, B)\n",
    "\n",
    "        sub5 = dft_pos5[dft_pos5[\"pair\"] == pair_slug]\n",
    "        if sub5.empty:\n",
    "            continue\n",
    "\n",
    "        data_by_k_5: Dict[int, Dict[str, float]] = {}\n",
    "        for k in K_LIST:\n",
    "            data_by_k_5[k] = {\"Original\": 0.0, \"2\": 0.0, \"4\": 0.0, \"6\": 0.0}\n",
    "            sO = sub5[(sub5[\"K\"] == k) & (sub5[\"pos\"] == \"ORIGINAL\") & (sub5[\"n\"] == \"ORIGINAL\")]\n",
    "            if not sO.empty:\n",
    "                data_by_k_5[k][\"Original\"] = float(sO.iloc[0][\"avg_count\"])\n",
    "            for n_str in [\"2\",\"4\",\"6\"]:\n",
    "                sN = sub5[(sub5[\"K\"] == k) & (sub5[\"pos\"] == \"5\") & (sub5[\"n\"] == n_str)]\n",
    "                if not sN.empty:\n",
    "                    data_by_k_5[k][n_str] = float(sN.iloc[0][\"avg_count\"])\n",
    "\n",
    "        out_png_5 = OUT_5 / f\"{pair_slug}__pos5.png\"\n",
    "        plot_pair_pos_three_bins(A, B, \"5\", data_by_k_5, out_png_5)\n",
    "\n",
    "    print(f\"[OK] Figures written to:\\n  {OUT_5}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Found 72 pairs.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# make_all_pairs_explanations_and_figs_1015.py\n",
    "#\n",
    "# Merged workflow for PAIRS (pos=5 branch):\n",
    "#  • For each discovered (A,B) pair:\n",
    "#      - reads ORIGINAL_{K}recommendation.csv (BASE)\n",
    "#      - finds fpair_<A>__<B>_{N}u_pos5_neg1_all_{K}recommendation.csv (under BASE/5)\n",
    "#      - computes per-dataset metrics + per-book rankings (books containing BOTH A and B)\n",
    "#      - saves in per-pair folder:\n",
    "#           <OUT_ROOT>/5/<pair_slug>_explanation/explanation.txt\n",
    "#           <OUT_ROOT>/5/<pair_slug>_explanation/per_book_ranking.csv\n",
    "#           <OUT_ROOT>/5/<pair_slug>_explanation/<pair_slug>__pos5.png\n",
    "#\n",
    "#  • Global rollups:\n",
    "#      <OUT_ROOT>/5/_all_pairs/summary_master.txt\n",
    "#      <OUT_ROOT>/5/_all_pairs/per_book_ranking_all.csv\n",
    "#\n",
    "# Python 3.8+\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple, List, Set, Optional, Dict\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # headless-safe\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========= PATHS / CONFIG (edit if needed) ===================================\n",
    "BASE      = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/SVD_pair\")\n",
    "ORIG_DIR  = BASE                    # ORIGINAL_{K}recommendation.csv lives here\n",
    "POS_DIRS  = [BASE / \"5\"]            # pos=5 / neg=1 branch only\n",
    "OUT_ROOT  = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/result/figures\")\n",
    "OUT_5     = OUT_ROOT / \"5\"\n",
    "\n",
    "K_LIST = [15, 25, 35]\n",
    "N_LIST = [2, 4, 6]   # synthetic user counts\n",
    "\n",
    "# ========= Normalization helpers ============================================\n",
    "def normalize_tag(t: str) -> str:\n",
    "    t = str(t).strip().replace(\"_\", \" \")\n",
    "    if t == \"Children s\":  # filename artifact\n",
    "        t = \"Children's\"\n",
    "    low = t.lower()\n",
    "    if   low == \"science fiction\": t = \"Science Fiction\"\n",
    "    elif low == \"historical\":      t = \"Historical\"\n",
    "    elif low == \"nonfiction\":      t = \"Nonfiction\"\n",
    "    elif low == \"thriller\":        t = \"Thriller\"\n",
    "    elif low == \"drama\":           t = \"Drama\"\n",
    "    elif low == \"fantasy\":         t = \"Fantasy\"\n",
    "    elif low == \"mystery\":         t = \"Mystery\"\n",
    "    elif low == \"romance\":         t = \"Romance\"\n",
    "    elif low == \"horror\":          t = \"Horror\"\n",
    "    elif low == \"classics\":        t = \"Classics\"\n",
    "    elif low == \"adventure\":       t = \"Adventure\"\n",
    "    elif low == \"adult\":           t = \"Adult\"\n",
    "    return t\n",
    "\n",
    "def slugify_token(x: str) -> str:\n",
    "    x = re.sub(r\"[^A-Za-z0-9]+\", \"_\", str(x)).strip(\"_\").lower()\n",
    "    return re.sub(r\"_+\", \"_\", x)\n",
    "\n",
    "def slugify_pair(a: str, b: str) -> str:\n",
    "    a2, b2 = sorted([a, b], key=lambda x: x.lower())\n",
    "    return f\"{slugify_token(a2)}__{slugify_token(b2)}\"\n",
    "\n",
    "# ========= CSV loaders & basic checks =======================================\n",
    "def load_rec_csv(fp: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp, low_memory=False)\n",
    "    # Ensure needed columns exist (robust to minimal files)\n",
    "    if \"genres_all\" not in df.columns:\n",
    "        # try to synthesize from (genre_g1, genre_g2) if present\n",
    "        if \"genre_g1\" in df.columns and \"genre_g2\" in df.columns:\n",
    "            df[\"genres_all\"] = df[[\"genre_g1\", \"genre_g2\"]].fillna(\"\").agg(\n",
    "                lambda x: \", \".join([t for t in [x[\"genre_g1\"], x[\"genre_g2\"]] if str(t).strip()]),\n",
    "                axis=1\n",
    "            )\n",
    "        else:\n",
    "            df[\"genres_all\"] = \"\"\n",
    "    for c in [\"user_id\", \"book_id\", \"rank\", \"est_score\", \"original_title\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "    return df\n",
    "\n",
    "def original_file_for_k(orig_dir: Path, k: int) -> Optional[Path]:\n",
    "    p = orig_dir / f\"ORIGINAL_{k}recommendation.csv\"\n",
    "    return p if p.exists() else None\n",
    "\n",
    "# ========= Pair logic ========================================================\n",
    "def book_has_both(gen_all: str, A: str, B: str) -> bool:\n",
    "    if pd.isna(gen_all) or not str(gen_all).strip():\n",
    "        return False\n",
    "    parts = [x.strip() for x in str(gen_all).split(\",\") if str(x).strip()]\n",
    "    tags  = [normalize_tag(x) for x in parts]\n",
    "    return (A in tags) and (B in tags)\n",
    "\n",
    "def metrics_for_file_pair(df: pd.DataFrame, A: str, B: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      unique_books_in_file,\n",
    "      avg_per_user (of A&B books),\n",
    "      unique_AB_books,\n",
    "      freq (total rows that are A&B),\n",
    "      users_with_AB,\n",
    "      is_AB_mask\n",
    "    \"\"\"\n",
    "    unique_books_in_file = df[\"book_id\"].nunique() if \"book_id\" in df.columns else 0\n",
    "    is_ab = df[\"genres_all\"].apply(lambda s: book_has_both(s, A, B))\n",
    "\n",
    "    if \"user_id\" in df.columns:\n",
    "        per_user = df.assign(is_ab=is_ab).groupby(\"user_id\")[\"is_ab\"].sum()\n",
    "        avg_per_user = float(per_user.mean()) if not per_user.empty else 0.0\n",
    "        users_with_ab = int((per_user > 0).sum())\n",
    "    else:\n",
    "        avg_per_user = 0.0\n",
    "        users_with_ab = 0\n",
    "\n",
    "    unique_ab_books = df.loc[is_ab, \"book_id\"].nunique() if \"book_id\" in df.columns else 0\n",
    "    freq = int(is_ab.sum())\n",
    "    return unique_books_in_file, avg_per_user, unique_ab_books, freq, users_with_ab, is_ab\n",
    "\n",
    "def per_book_ranking(df_ab_rows: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build per-book table from rows filtered to the A&B pair:\n",
    "      rank by freq desc, rank1_count desc, avg_rank asc\n",
    "    \"\"\"\n",
    "    if df_ab_rows.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"book_id\",\"rank\",\"freq\",\"users_n\",\"avg_rank\",\"avg_est_score\",\"rank1_count\",\"original_title\",\"genres_all\"\n",
    "        ])\n",
    "\n",
    "    g = df_ab_rows.groupby(\"book_id\", as_index=False)\n",
    "    out = g.agg(\n",
    "        freq=(\"book_id\", \"size\"),\n",
    "        users_n=(\"user_id\", \"nunique\"),\n",
    "        avg_rank=(\"rank\", \"mean\"),\n",
    "        avg_est_score=(\"est_score\", \"mean\"),\n",
    "        rank1_count=(\"rank\", lambda s: int((s == 1).sum())),\n",
    "    )\n",
    "    # attach sample title/genres\n",
    "    for c in [\"original_title\", \"genres_all\"]:\n",
    "        smpl = df_ab_rows.groupby(\"book_id\")[c].apply(\n",
    "            lambda s: s.dropna().iloc[0] if s.notna().any() else pd.NA\n",
    "        ).reset_index(name=c)\n",
    "        out = out.merge(smpl, on=\"book_id\", how=\"left\")\n",
    "\n",
    "    out = out.sort_values([\"freq\",\"rank1_count\",\"avg_rank\"], ascending=[False, False, True]).reset_index(drop=True)\n",
    "    out.insert(1, \"rank\", out.index + 1)\n",
    "    return out\n",
    "\n",
    "def injected_files_for_pair_k_n(pos_dir: Path, A: str, B: str, k: int, n: int) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Match fpair_<A>__<B>_{n}u_pos5_neg1_all_{k}recommendation.csv (order-insensitive for A/B).\n",
    "    \"\"\"\n",
    "    aT = re.sub(r\"_+\", \"_\", A.replace(\" \", \"_\").replace(\"'\", \"_\")).strip(\"_\")\n",
    "    bT = re.sub(r\"_+\", \"_\", B.replace(\" \", \"_\").replace(\"'\", \"_\")).strip(\"_\")\n",
    "    pat1 = re.compile(rf\"^fpair_{aT}__{bT}_{n}u_pos5_neg1_all_{k}recommendation\\.csv$\")\n",
    "    pat2 = re.compile(rf\"^fpair_{bT}__{aT}_{n}u_pos5_neg1_all_{k}recommendation\\.csv$\")\n",
    "    out: List[Path] = []\n",
    "    for p in pos_dir.glob(f\"*neg1_all_{k}recommendation.csv\"):\n",
    "        if pat1.match(p.name) or pat2.match(p.name):\n",
    "            out.append(p)\n",
    "    return sorted(out)\n",
    "\n",
    "def discover_pairs_from_dirs(pos_dirs: Iterable[Path],\n",
    "                             k_list: Iterable[int],\n",
    "                             n_list: Iterable[int]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse file names under /5, return unique unordered (A,B) pairs present for valid K and n.\n",
    "    \"\"\"\n",
    "    pair_set: Set[Tuple[str, str]] = set()\n",
    "    regex = re.compile(\n",
    "        r\"^fpair_(?P<A>[A-Za-z0-9_']+)__(?P<B>[A-Za-z0-9_']+)_(?P<N>\\d+)u_pos5_neg1_all_(?P<K>\\d+)recommendation\\.csv$\"\n",
    "    )\n",
    "    valid_k = set(map(int, k_list))\n",
    "    valid_n = set(map(int, n_list))\n",
    "    for pos_dir in pos_dirs:\n",
    "        for p in pos_dir.glob(\"fpair_*u_pos5_neg1_all_*recommendation.csv\"):\n",
    "            m = regex.match(p.name)\n",
    "            if not m:\n",
    "                continue\n",
    "            k = int(m.group(\"K\")); n = int(m.group(\"N\"))\n",
    "            if k not in valid_k or n not in valid_n:\n",
    "                continue\n",
    "            A_disp = normalize_tag(m.group(\"A\").replace(\"_\", \" \"))\n",
    "            B_disp = normalize_tag(m.group(\"B\").replace(\"_\", \" \"))\n",
    "            a_c, b_c = sorted([A_disp, B_disp], key=lambda x: x.lower())\n",
    "            pair_set.add((a_c, b_c))\n",
    "    return sorted(pair_set, key=lambda ab: (ab[0].lower(), ab[1].lower()))\n",
    "\n",
    "# ========= Plotting ==========================================================\n",
    "def plot_pair_pos_three_bins(A: str, B: str,\n",
    "                             data_by_k: Dict[int, Dict[str, float]],\n",
    "                             out_png: Path):\n",
    "    \"\"\"\n",
    "    data_by_k: {K: {\"Original\": v0, \"2\": v2, \"4\": v4, \"6\": v6}}\n",
    "    \"\"\"\n",
    "    ks = sorted(data_by_k.keys())\n",
    "    if not ks:\n",
    "        return\n",
    "    groups = [\"Original\"] + [str(x) for x in N_LIST]\n",
    "    present_groups = [g for g in groups if any(g in data_by_k.get(k, {}) for k in ks)]\n",
    "\n",
    "    width = 0.8 / max(1, len(present_groups))\n",
    "    x = list(range(len(ks)))\n",
    "    plt.figure(figsize=(8.4, 4.4), dpi=160)\n",
    "\n",
    "    for j, g in enumerate(present_groups):\n",
    "        offs = [i + (j - (len(present_groups)-1)/2)*width for i in x]\n",
    "        vals = [float(data_by_k.get(k, {}).get(g, 0.0)) for k in ks]\n",
    "        plt.bar(offs, vals, width=width, label=(\"n=\"+g if g!=\"Original\" else \"Original\"))\n",
    "\n",
    "    plt.xticks(x, [f\"Top-{k}\" for k in ks])\n",
    "    plt.ylabel(\"Avg # of A&B books in Top-K per user\")\n",
    "    plt.title(f\"{A} + {B} — POS=5\")\n",
    "    plt.legend(ncol=min(4, len(present_groups)), fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(out_png)\n",
    "    plt.close()\n",
    "\n",
    "# ========= Per-pair processing ===============================================\n",
    "def process_one_pair(A: str, B: str,\n",
    "                     original_df_by_k: Dict[int, Optional[pd.DataFrame]]) -> Dict:\n",
    "    \"\"\"\n",
    "    For a given (A,B):\n",
    "      - writes explanation.txt, per_book_ranking.csv, <pair_slug>__pos5.png\n",
    "      - returns summary dict\n",
    "    \"\"\"\n",
    "    pair_slug = slugify_pair(A, B)\n",
    "    OUT_DIR = OUT_5 / f\"{pair_slug}_explanation\"\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    original_uniques: Dict[int, int] = {}\n",
    "    injection_uniques: Dict[Tuple[int,int], int] = {}\n",
    "    per_book_rows: List[Dict] = []\n",
    "    data_by_k: Dict[int, Dict[str, float]] = {k: {} for k in K_LIST}\n",
    "\n",
    "    # ORIGINAL\n",
    "    for K in K_LIST:\n",
    "        dfO = original_df_by_k.get(K)\n",
    "        if dfO is None:\n",
    "            print(f\"[warn][{pair_slug}] Missing ORIGINAL for K={K}\")\n",
    "            continue\n",
    "        uniq_all, avg_user, uniq_ab, freq, users_with_ab, is_ab = metrics_for_file_pair(dfO, A, B)\n",
    "        original_uniques[K] = uniq_ab\n",
    "        data_by_k[K][\"Original\"] = avg_user\n",
    "\n",
    "        ranked = per_book_ranking(dfO.loc[is_ab].copy())\n",
    "        for _, r in ranked.iterrows():\n",
    "            per_book_rows.append({\n",
    "                \"pair\": pair_slug,\n",
    "                \"A\": A, \"B\": B,\n",
    "                \"dataset\": f\"original{K}\",\n",
    "                \"book_id\": int(r[\"book_id\"]),\n",
    "                \"rank\": int(r[\"rank\"]),\n",
    "                \"freq\": int(r[\"freq\"]),\n",
    "                \"users_n\": int(r[\"users_n\"]),\n",
    "                \"avg_rank\": float(r[\"avg_rank\"]) if pd.notna(r[\"avg_rank\"]) else None,\n",
    "                \"avg_est_score\": float(r[\"avg_est_score\"]) if pd.notna(r[\"avg_est_score\"]) else None,\n",
    "                \"rank1_count\": int(r[\"rank1_count\"]),\n",
    "                \"original_title\": r.get(\"original_title\", pd.NA),\n",
    "                \"genres_all\": r.get(\"genres_all\", pd.NA),\n",
    "            })\n",
    "\n",
    "    # INJECTIONS (pos=5 only)\n",
    "    for K in K_LIST:\n",
    "        for N in N_LIST:\n",
    "            found_any = False\n",
    "            for pos_dir in POS_DIRS:\n",
    "                files = injected_files_for_pair_k_n(pos_dir, A, B, K, N)\n",
    "                if not files:\n",
    "                    continue\n",
    "                for f in files:\n",
    "                    df = load_rec_csv(f)\n",
    "                    uniq_all, avg_user, uniq_ab, freq, users_with_ab, is_ab = metrics_for_file_pair(df, A, B)\n",
    "                    injection_uniques[(N, K)] = uniq_ab  # if multiple files, last wins (names identical anyway)\n",
    "                    data_by_k[K][str(N)] = avg_user\n",
    "                    ranked = per_book_ranking(df.loc[is_ab].copy())\n",
    "                    for _, r in ranked.iterrows():\n",
    "                        per_book_rows.append({\n",
    "                            \"pair\": pair_slug,\n",
    "                            \"A\": A, \"B\": B,\n",
    "                            \"dataset\": f\"{N}u_{K}\",\n",
    "                            \"book_id\": int(r[\"book_id\"]),\n",
    "                            \"rank\": int(r[\"rank\"]),\n",
    "                            \"freq\": int(r[\"freq\"]),\n",
    "                            \"users_n\": int(r[\"users_n\"]),\n",
    "                            \"avg_rank\": float(r[\"avg_rank\"]) if pd.notna(r[\"avg_rank\"]) else None,\n",
    "                            \"avg_est_score\": float(r[\"avg_est_score\"]) if pd.notna(r[\"avg_est_score\"]) else None,\n",
    "                            \"rank1_count\": int(r[\"rank1_count\"]),\n",
    "                            \"original_title\": r.get(\"original_title\", pd.NA),\n",
    "                            \"genres_all\": r.get(\"genres_all\", pd.NA),\n",
    "                        })\n",
    "                    found_any = True\n",
    "            if not found_any:\n",
    "                print(f\"[warn][{pair_slug}] Missing injection for N={N}, K={K}\")\n",
    "\n",
    "    # Write per-pair text summary\n",
    "    text_path = OUT_DIR / \"explanation.txt\"\n",
    "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{slugify_token(A)}__{slugify_token(B)}:\\n\")\n",
    "        for K in K_LIST:\n",
    "            if K in original_uniques:\n",
    "                f.write(f\"original {K}: number_of_unique_books: {original_uniques[K]}\\n\")\n",
    "        for K in K_LIST:\n",
    "            for N in N_LIST:\n",
    "                val = injection_uniques.get((N, K))\n",
    "                if val is not None:\n",
    "                    f.write(f\"{N}u, {K}, number_of_unique_books: {val}\\n\")\n",
    "\n",
    "    # Write per-pair ranking table\n",
    "    table_df = pd.DataFrame(per_book_rows)\n",
    "    table_path = OUT_DIR / \"per_book_ranking.csv\"\n",
    "    table_df.to_csv(table_path, index=False)\n",
    "\n",
    "    # Make & save per-pair figure\n",
    "    fig_path = OUT_DIR / f\"{slugify_pair(A,B)}__pos5.png\"\n",
    "    plot_pair_pos_three_bins(A, B, data_by_k, fig_path)\n",
    "\n",
    "    print(f\"[OK][{pair_slug}] Saved text:   {text_path}\")\n",
    "    print(f\"[OK][{pair_slug}] Saved table:  {table_path}\")\n",
    "    print(f\"[OK][{pair_slug}] Saved figure: {fig_path}\")\n",
    "\n",
    "    return {\n",
    "        \"pair\": pair_slug, \"A\": A, \"B\": B,\n",
    "        \"original_uniques\": original_uniques,\n",
    "        \"injection_uniques\": injection_uniques,\n",
    "        \"out_dir\": OUT_DIR,\n",
    "        \"per_book_rows\": per_book_rows,\n",
    "    }\n",
    "\n",
    "# ========= MAIN ==============================================================\n",
    "def main():\n",
    "    OUT_5.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Discover all pairs present under /5\n",
    "    PAIRS = discover_pairs_from_dirs(POS_DIRS, K_LIST, N_LIST)\n",
    "    if not PAIRS:\n",
    "        print(\"[WARN] No pairs found in /5.\")\n",
    "        return\n",
    "    print(f\"[OK] Found {len(PAIRS)} pairs.\")\n",
    "\n",
    "    # Preload ORIGINAL per K\n",
    "    original_df_by_k: Dict[int, Optional[pd.DataFrame]] = {}\n",
    "    for k in K_LIST:\n",
    "        fp = original_file_for_k(ORIG_DIR, k)\n",
    "        if fp is None:\n",
    "            print(f\"[WARN] Missing ORIGINAL_{k}recommendation.csv in {ORIG_DIR}\")\n",
    "            original_df_by_k[k] = None\n",
    "        else:\n",
    "            original_df_by_k[k] = load_rec_csv(fp)\n",
    "\n",
    "    # Process each pair\n",
    "    all_rows: List[Dict] = []\n",
    "    master_lines: List[str] = []\n",
    "\n",
    "    for (A, B) in PAIRS:\n",
    "        res = process_one_pair(A, B, original_df_by_k)\n",
    "        pair_slug = res[\"pair\"]\n",
    "        master_lines.append(f\"{pair_slug}:\")\n",
    "        for K in K_LIST:\n",
    "            if K in res[\"original_uniques\"]:\n",
    "                master_lines.append(f\"original {K}: number_of_unique_books: {res['original_uniques'][K]}\")\n",
    "        for K in K_LIST:\n",
    "            for N in N_LIST:\n",
    "                val = res[\"injection_uniques\"].get((N, K))\n",
    "                if val is not None:\n",
    "                    master_lines.append(f\"{N}u, {K}, number_of_unique_books: {val}\")\n",
    "        master_lines.append(\"\")  # blank line\n",
    "        all_rows.extend(res[\"per_book_rows\"])\n",
    "\n",
    "    # Global rollups\n",
    "    OUT_ALL = OUT_5 / \"_all_pairs\"\n",
    "    OUT_ALL.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    master_txt = OUT_ALL / \"summary_master.txt\"\n",
    "    with open(master_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(master_lines))\n",
    "    print(f\"[OK] Wrote master summary: {master_txt}\")\n",
    "\n",
    "    if all_rows:\n",
    "        df_all = pd.DataFrame(all_rows)\n",
    "        df_all.to_csv(OUT_ALL / \"per_book_ranking_all.csv\", index=False)\n",
    "        print(f\"[OK] Wrote global per-book ranking CSV: {OUT_ALL / 'per_book_ranking_all.csv'}\")\n",
    "    else:\n",
    "        print(\"[warn] No per-book rows produced. Check inputs / paths.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
