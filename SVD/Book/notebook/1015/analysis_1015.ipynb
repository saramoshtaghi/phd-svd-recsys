{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trained: users=53,424, items=10,000, rows=5,976,479\n",
      "⏱️ Fit time: 235.78s  | 0.000039 s/row  | 25347.8 rows/s\n"
     ]
    }
   ],
   "source": [
    "import time, pandas as pd\n",
    "from pathlib import Path\n",
    "from surprise import Dataset, Reader, SVD\n",
    "\n",
    "def time_svd(csv_path, user_col=\"user_id\", item_col=\"book_id\", rating_col=\"rating\",\n",
    "             n_factors=8, n_epochs=180, lr_all=0.012, lr_bi=0.03,\n",
    "             reg_all=0.002, reg_pu=0.0, reg_qi=0.002, seed=42):\n",
    "    df = pd.read_csv(csv_path, dtype={user_col:\"int64\", item_col:\"int64\", rating_col:\"float64\"}, low_memory=False)\n",
    "    df = df.dropna(subset=[user_col, item_col, rating_col])\n",
    "    n_rows = len(df); n_users = df[user_col].nunique(); n_items = df[item_col].nunique()\n",
    "    reader = Reader(rating_scale=(0,7))\n",
    "    data = Dataset.load_from_df(df[[user_col, item_col, rating_col]], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "\n",
    "    params = dict(biased=True, n_factors=n_factors, n_epochs=n_epochs, lr_all=lr_all, lr_bi=lr_bi,\n",
    "                  reg_all=reg_all, reg_pu=reg_pu, reg_qi=reg_qi, random_state=seed, verbose=False)\n",
    "    algo = SVD(**params)\n",
    "\n",
    "    t0 = time.time()\n",
    "    algo.fit(trainset)\n",
    "    t1 = time.time()\n",
    "\n",
    "    train_time = t1 - t0\n",
    "    print(f\"✅ Trained: users={n_users:,}, items={n_items:,}, rows={n_rows:,}\")\n",
    "    print(f\"⏱️ Fit time: {train_time:.2f}s  | {train_time/max(n_rows,1):.6f} s/row  | {n_rows/train_time:.1f} rows/s\")\n",
    "    return train_time, (n_users, n_items, n_rows), algo, trainset\n",
    "\n",
    "# Example runs (pick one of your real files):\n",
    "# baseline\n",
    "_ = time_svd(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "\n",
    "# a poisoned focus file (pos=5 Mystery–Thriller, for example)\n",
    "# _ = time_svd(\"/.../PAIR_INJECTION/5/fpair_Mystery__Thriller_100u_pos5_neg0_all.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysisis(also saved as .py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# count_pairs_pos5and7_k15_25_35_from_genres_all.py\n",
    "#\n",
    "# Purpose:\n",
    "#   Scan both pos=5 and pos=7 directories, discover ALL unordered genre pairs\n",
    "#   that appear in either branch, report total count, save inventory,\n",
    "#   and compute per-user average pair counts.\n",
    "#\n",
    "# Input:\n",
    "#   /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair/{5,7}\n",
    "#\n",
    "# Output:\n",
    "#   /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair/result/pair_summary/all/\n",
    "#     |_ _inventory/discovered_pairs.txt\n",
    "#     |_ _inventory/discovered_pairs.csv\n",
    "#     |_ <pair>/k15_25_35_genresall_counts.csv\n",
    "#     |_ ALL_k15_25_35_genresall_counts.csv\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Iterable, Tuple, List, Set\n",
    "\n",
    "# ======== CONFIG ========\n",
    "BASE = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0929/SVD_pair\")\n",
    "POS_DIRS = [BASE / \"5\", BASE / \"7\"]\n",
    "OUT_ROOT = BASE / \"result\" / \"pair_summary\" / \"all\"\n",
    "INV_DIR = OUT_ROOT / \"_inventory\"\n",
    "\n",
    "K_LIST = [15, 25, 35]\n",
    "N_LIST = [25, 50, 100, 200]\n",
    "\n",
    "# ======== HELPERS ========\n",
    "def slugify_pair(a: str, b: str) -> str:\n",
    "    import re as _re\n",
    "    def sg(x): return _re.sub(r\"[^A-Za-z0-9]+\", \"_\", x).strip(\"_\").lower()\n",
    "    return f\"{sg(a)}__{sg(b)}\"\n",
    "\n",
    "def normalize_tag(t: str) -> str:\n",
    "    t = str(t).strip().replace(\"_\", \" \")\n",
    "    if t == \"Children s\":\n",
    "        t = \"Children's\"\n",
    "    if t.lower() == \"science fiction\":\n",
    "        t = \"Science Fiction\"\n",
    "    if t.lower() == \"historical\":\n",
    "        t = \"Historical\"\n",
    "    if t.lower() == \"nonfiction\":\n",
    "        t = \"Nonfiction\"\n",
    "    return t\n",
    "\n",
    "def book_has_both(gen_all: str, A: str, B: str) -> bool:\n",
    "    if pd.isna(gen_all) or not str(gen_all).strip():\n",
    "        return False\n",
    "    tags = [normalize_tag(x) for x in str(gen_all).split(\",\") if str(x).strip()]\n",
    "    return (A in tags) and (B in tags)\n",
    "\n",
    "def per_user_avg_pair_count(rec_df: pd.DataFrame, A: str, B: str) -> tuple[float, int]:\n",
    "    need = {\"user_id\", \"book_id\", \"genres_all\"}\n",
    "    missing = need - set(rec_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV missing columns: {missing}\")\n",
    "    users = rec_df[\"user_id\"].drop_duplicates().sort_values()\n",
    "    users_count = int(users.shape[0])\n",
    "    mask = rec_df[\"genres_all\"].apply(lambda s: book_has_both(s, A, B))\n",
    "    pair_df = rec_df[mask].copy()\n",
    "    if pair_df.empty:\n",
    "        return (0.0, users_count)\n",
    "    per_user = (pair_df.groupby(\"user_id\", as_index=False)[\"book_id\"]\n",
    "                        .count()\n",
    "                        .rename(columns={\"book_id\": \"count\"}))\n",
    "    all_users = pd.DataFrame({\"user_id\": users})\n",
    "    all_users = all_users.merge(per_user, on=\"user_id\", how=\"left\").fillna({\"count\": 0})\n",
    "    return (float(all_users[\"count\"].mean()), users_count)\n",
    "\n",
    "def injected_files_for_pair_k_n(pos_dir: Path, A: str, B: str, k: int, n: int) -> list[Path]:\n",
    "    aT, bT = A.replace(\" \", \"_\").replace(\"'\", \"_\"), B.replace(\" \", \"_\").replace(\"'\", \"_\")\n",
    "    aT = re.sub(r\"_+\", \"_\", aT).strip(\"_\")\n",
    "    bT = re.sub(r\"_+\", \"_\", bT).strip(\"_\")\n",
    "    pat1 = re.compile(rf\"^fpair_{aT}__{bT}_{n}u_pos[57]_neg0_sample_{k}recommendation\\.csv$\")\n",
    "    pat2 = re.compile(rf\"^fpair_{bT}__{aT}_{n}u_pos[57]_neg0_sample_{k}recommendation\\.csv$\")\n",
    "    out = []\n",
    "    for p in pos_dir.glob(f\"*sample_{k}recommendation.csv\"):\n",
    "        if pat1.match(p.name) or pat2.match(p.name):\n",
    "            out.append(p)\n",
    "    return sorted(out)\n",
    "\n",
    "def discover_pairs_from_dirs(pos_dirs: Iterable[Path], k_list: Iterable[int], n_list: Iterable[int]) -> List[Tuple[str, str]]:\n",
    "    pair_set: Set[Tuple[str, str]] = set()\n",
    "    regex = re.compile(\n",
    "        r\"^fpair_(?P<A>[^_][A-Za-z0-9_'_]+)__\"\n",
    "        r\"(?P<B>[A-Za-z0-9_'_]+)_(?P<N>\\d+)u_pos[57]_neg0_sample_\"\n",
    "        r\"(?P<K>\\d+)recommendation\\.csv$\"\n",
    "    )\n",
    "    valid_k = set(map(int, k_list))\n",
    "    valid_n = set(map(int, n_list))\n",
    "    for pos_dir in pos_dirs:\n",
    "        for p in pos_dir.glob(\"fpair_*u_pos*_neg0_sample_*recommendation.csv\"):\n",
    "            m = regex.match(p.name)\n",
    "            if not m:\n",
    "                continue\n",
    "            k = int(m.group(\"K\"))\n",
    "            n = int(m.group(\"N\"))\n",
    "            if k not in valid_k or n not in valid_n:\n",
    "                continue\n",
    "            A_disp = normalize_tag(m.group(\"A\").replace(\"_\", \" \"))\n",
    "            B_disp = normalize_tag(m.group(\"B\").replace(\"_\", \" \"))\n",
    "            a_c, b_c = sorted([A_disp, B_disp], key=lambda x: x.lower())\n",
    "            pair_set.add((a_c, b_c))\n",
    "    return sorted(pair_set, key=lambda ab: (ab[0].lower(), ab[1].lower()))\n",
    "\n",
    "def _n_to_order(v):\n",
    "    s = str(v).strip()\n",
    "    if s.upper() == \"ORIGINAL\":\n",
    "        return -1\n",
    "    try:\n",
    "        return int(s)\n",
    "    except Exception:\n",
    "        return 10**9\n",
    "\n",
    "# ======== MAIN ========\n",
    "def main():\n",
    "    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    INV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    all_rows = []\n",
    "\n",
    "    # -------- Discover pairs from BOTH pos5 and pos7 --------\n",
    "    PAIRS = discover_pairs_from_dirs(POS_DIRS, K_LIST, N_LIST)\n",
    "    if not PAIRS:\n",
    "        print(\"[WARN] No pairs found in either /5 or /7 directories.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Found {len(PAIRS)} unique unordered pairs across /5 and /7\")\n",
    "    with open(INV_DIR / \"discovered_pairs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for a, b in PAIRS:\n",
    "            f.write(f\"{a},{b}\\n\")\n",
    "    pd.DataFrame(PAIRS, columns=[\"A\", \"B\"]).to_csv(INV_DIR / \"discovered_pairs.csv\", index=False)\n",
    "    print(f\"[OK] Inventory saved in {INV_DIR}\")\n",
    "\n",
    "    # -------- Process both /5 and /7 branches --------\n",
    "    for pos_dir in POS_DIRS:\n",
    "        pos_label = pos_dir.name  # \"5\" or \"7\"\n",
    "        for (A, B) in PAIRS:\n",
    "            pair_slug = slugify_pair(A, B)\n",
    "            pair_dir = OUT_ROOT / pair_slug\n",
    "            pair_dir.mkdir(parents=True, exist_ok=True)\n",
    "            for k in K_LIST:\n",
    "                for n in N_LIST:\n",
    "                    files = injected_files_for_pair_k_n(pos_dir, A, B, k, n)\n",
    "                    if not files:\n",
    "                        continue\n",
    "                    vals, user_counts = [], []\n",
    "                    for f in files:\n",
    "                        try:\n",
    "                            df = pd.read_csv(f)\n",
    "                            avgc, users_cnt = per_user_avg_pair_count(df, A, B)\n",
    "                            vals.append(avgc)\n",
    "                            user_counts.append(users_cnt)\n",
    "                        except Exception as e:\n",
    "                            print(f\"[ERROR] Reading {f}: {e}\")\n",
    "                    avgc = float(sum(vals) / len(vals)) if vals else 0.0\n",
    "                    users_cnt = max(user_counts) if user_counts else 0\n",
    "                    all_rows.append({\n",
    "                        \"pos_branch\": pos_label,\n",
    "                        \"pair\": pair_slug, \"K\": k, \"n\": n,\n",
    "                        \"avg_count\": avgc, \"users_counted\": users_cnt,\n",
    "                        \"source\": \";\".join([p.name for p in files]) if files else \"\"\n",
    "                    })\n",
    "                    print(f\"{pos_label}: {pair_slug.replace('__', ',')} n={n}, K={k} → avg={avgc:.4f}\")\n",
    "\n",
    "    if all_rows:\n",
    "        dfa = pd.DataFrame(all_rows)\n",
    "        dfa[\"n_order\"] = dfa[\"n\"].map(_n_to_order)\n",
    "        dfa = dfa.sort_values(by=[\"pair\", \"pos_branch\", \"K\", \"n_order\", \"n\"]).drop(columns=[\"n_order\"])\n",
    "        out_all = OUT_ROOT / \"ALL_k15_25_35_genresall_counts.csv\"\n",
    "        dfa.to_csv(out_all, index=False)\n",
    "        print(f\"[OK] Saved combined summary: {out_all}\")\n",
    "    else:\n",
    "        print(\"[WARN] No rows collected; nothing saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
