{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysisis(also saved as .py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 1 unique unordered pairs across /5 and /7\n",
      "[OK] Inventory saved in /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/SVD_pair/result/pair_summary/with_original/_inventory\n",
      "[OK] Saved per-file details: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/SVD_pair/result/pair_summary/with_original/DETAILED_per_file_pair_counts.csv\n",
      "[OK] Saved combined summary: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/SVD_pair/result/pair_summary/with_original/SUMMARY_pair_avg_counts_with_original.csv\n",
      "[OK] Figures written to: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/SVD_pair/result/pair_summary/with_original/figures\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# count_pairs_pos5and7_with_original.py\n",
    "#\n",
    "# Scans SVD recommendation outputs for:\n",
    "#   - ORIGINAL_{K}recommendation.csv (unpoisoned baseline)\n",
    "#   - fpair_<A>__<B>_<n>u_pos{5|7}_neg0_all_{K}recommendation.csv (poisoned)\n",
    "#\n",
    "# Computes per-user average count of books recommended that contain BOTH genres\n",
    "# for each unordered pair and K in {15,25,35}. Aggregates across n âˆˆ {25,50,100,200}.\n",
    "# Saves inventories, detailed per-file stats, aggregated summaries,\n",
    "# and grouped bar charts (Original vs POS=5 vs POS=7) per pair per K.\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple, List, Set, Optional, Dict\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # headless\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======== CONFIG (adjust paths if needed) ========\n",
    "BASE = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/SVD_pair\")\n",
    "\n",
    "ORIG_DIR = BASE                      # ORIGINAL_{K}recommendation.csv lives directly here\n",
    "POS_DIRS = [BASE / \"5\", BASE / \"7\"]  # poisoned branches\n",
    "\n",
    "OUT_ROOT = BASE / \"result\" / \"pair_summary\" / \"with_original\"\n",
    "INV_DIR  = OUT_ROOT / \"_inventory\"\n",
    "FIG_DIR  = OUT_ROOT / \"figures\"\n",
    "\n",
    "K_LIST = [15, 25, 35]\n",
    "N_LIST = [25, 50, 100, 200]\n",
    "\n",
    "# ======== HELPERS ========\n",
    "def slugify_pair(a: str, b: str) -> str:\n",
    "    import re as _re\n",
    "    def sg(x): return _re.sub(r\"[^A-Za-z0-9]+\", \"_\", x).strip(\"_\").lower()\n",
    "    a2, b2 = sorted([a, b], key=lambda x: x.lower())\n",
    "    return f\"{sg(a2)}__{sg(b2)}\"\n",
    "\n",
    "def normalize_tag(t: str) -> str:\n",
    "    t = str(t).strip().replace(\"_\", \" \")\n",
    "    # normalize common tags to your canonical naming\n",
    "    if t == \"Children s\":\n",
    "        t = \"Children's\"\n",
    "    low = t.lower()\n",
    "    if low == \"science fiction\": t = \"Science Fiction\"\n",
    "    elif low == \"historical\":    t = \"Historical\"\n",
    "    elif low == \"nonfiction\":    t = \"Nonfiction\"\n",
    "    elif low == \"thriller\":      t = \"Thriller\"\n",
    "    elif low == \"drama\":         t = \"Drama\"\n",
    "    elif low == \"fantasy\":       t = \"Fantasy\"\n",
    "    elif low == \"mystery\":       t = \"Mystery\"\n",
    "    elif low == \"romance\":       t = \"Romance\"\n",
    "    elif low == \"horror\":        t = \"Horror\"\n",
    "    elif low == \"classics\":      t = \"Classics\"\n",
    "    elif low == \"adventure\":     t = \"Adventure\"\n",
    "    elif low == \"adult\":         t = \"Adult\"\n",
    "    return t\n",
    "\n",
    "def book_has_both(gen_all: str, A: str, B: str) -> bool:\n",
    "    if pd.isna(gen_all) or not str(gen_all).strip():\n",
    "        return False\n",
    "    parts = [x.strip() for x in str(gen_all).split(\",\") if str(x).strip()]\n",
    "    tags = [normalize_tag(x) for x in parts]\n",
    "    return (A in tags) and (B in tags)\n",
    "\n",
    "def per_user_avg_pair_count(rec_df: pd.DataFrame, A: str, B: str) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    rec_df columns expected: user_id, book_id, genres_all\n",
    "    Returns (average_count_per_user, num_users_in_this_csv)\n",
    "    \"\"\"\n",
    "    need = {\"user_id\", \"book_id\", \"genres_all\"}\n",
    "    missing = need - set(rec_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV missing columns: {missing}\")\n",
    "    users = rec_df[\"user_id\"].drop_duplicates().sort_values()\n",
    "    users_count = int(users.shape[0])\n",
    "    mask = rec_df[\"genres_all\"].apply(lambda s: book_has_both(s, A, B))\n",
    "    pair_df = rec_df[mask].copy()\n",
    "    if pair_df.empty:\n",
    "        return (0.0, users_count)\n",
    "    per_user = (\n",
    "        pair_df.groupby(\"user_id\", as_index=False)[\"book_id\"]\n",
    "               .count()\n",
    "               .rename(columns={\"book_id\": \"count\"})\n",
    "    )\n",
    "    all_users = pd.DataFrame({\"user_id\": users})\n",
    "    all_users = all_users.merge(per_user, on=\"user_id\", how=\"left\").fillna({\"count\": 0})\n",
    "    return (float(all_users[\"count\"].mean()), users_count)\n",
    "\n",
    "def injected_files_for_pair_k_n(pos_dir: Path, A: str, B: str, k: int, n: int) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Match fpair_<A>__<B>_<n>u_pos{5|7}_neg0_all_{k}recommendation.csv (order-insensitive for A/B).\n",
    "    \"\"\"\n",
    "    aT = re.sub(r\"_+\", \"_\", A.replace(\" \", \"_\").replace(\"'\", \"_\")).strip(\"_\")\n",
    "    bT = re.sub(r\"_+\", \"_\", B.replace(\" \", \"_\").replace(\"'\", \"_\")).strip(\"_\")\n",
    "\n",
    "    pat1 = re.compile(rf\"^fpair_{aT}__{bT}_{n}u_pos[57]_neg0_all_{k}recommendation\\.csv$\")\n",
    "    pat2 = re.compile(rf\"^fpair_{bT}__{aT}_{n}u_pos[57]_neg0_all_{k}recommendation\\.csv$\")\n",
    "    out: List[Path] = []\n",
    "    for p in pos_dir.glob(f\"*neg0_all_{k}recommendation.csv\"):\n",
    "        if pat1.match(p.name) or pat2.match(p.name):\n",
    "            out.append(p)\n",
    "    return sorted(out)\n",
    "\n",
    "def discover_pairs_from_dirs(pos_dirs: Iterable[Path], k_list: Iterable[int], n_list: Iterable[int]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse file names under /5 and /7, return unique unordered (A,B) pairs present for valid K and n.\n",
    "    \"\"\"\n",
    "    pair_set: Set[Tuple[str, str]] = set()\n",
    "    regex = re.compile(\n",
    "        r\"^fpair_(?P<A>[A-Za-z0-9_']+)__(?P<B>[A-Za-z0-9_']+)_(?P<N>\\d+)u_pos[57]_neg0_all_(?P<K>\\d+)recommendation\\.csv$\"\n",
    "    )\n",
    "    valid_k = set(map(int, k_list))\n",
    "    valid_n = set(map(int, n_list))\n",
    "    for pos_dir in pos_dirs:\n",
    "        for p in pos_dir.glob(\"fpair_*u_pos*_neg0_all_*recommendation.csv\"):\n",
    "            m = regex.match(p.name)\n",
    "            if not m:\n",
    "                continue\n",
    "            k = int(m.group(\"K\"))\n",
    "            n = int(m.group(\"N\"))\n",
    "            if k not in valid_k or n not in valid_n:\n",
    "                continue\n",
    "            A_disp = normalize_tag(m.group(\"A\").replace(\"_\", \" \"))\n",
    "            B_disp = normalize_tag(m.group(\"B\").replace(\"_\", \" \"))\n",
    "            a_c, b_c = sorted([A_disp, B_disp], key=lambda x: x.lower())\n",
    "            pair_set.add((a_c, b_c))\n",
    "    return sorted(pair_set, key=lambda ab: (ab[0].lower(), ab[1].lower()))\n",
    "\n",
    "def original_file_for_k(orig_dir: Path, k: int) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Return ORIGINAL_{K}recommendation.csv if it exists, else None.\n",
    "    \"\"\"\n",
    "    p = orig_dir / f\"ORIGINAL_{k}recommendation.csv\"\n",
    "    return p if p.exists() else None\n",
    "\n",
    "def load_rec_csv(fp: Path) -> pd.DataFrame:\n",
    "    # Use safe dtype handling; keep columns as-is otherwise\n",
    "    df = pd.read_csv(fp, low_memory=False)\n",
    "    # Ensure required columns exist (user_id, book_id, genres_all)\n",
    "    # If genres_all missing but genre_g1/g2 exist, synthesize genres_all\n",
    "    if \"genres_all\" not in df.columns:\n",
    "        if \"genre_g1\" in df.columns and \"genre_g2\" in df.columns:\n",
    "            df[\"genres_all\"] = df[[\"genre_g1\", \"genre_g2\"]].fillna(\"\").agg(\n",
    "                lambda x: \", \".join([t for t in [x[\"genre_g1\"], x[\"genre_g2\"]] if str(t).strip()]), axis=1\n",
    "            )\n",
    "        else:\n",
    "            # best-effort fallback\n",
    "            df[\"genres_all\"] = \"\"\n",
    "    return df\n",
    "\n",
    "def grouped_bar_chart(triple: Dict[str, float], title: str, out_png: Path):\n",
    "    \"\"\"\n",
    "    triple = {\"ORIGINAL\": val0, \"POS5\": val5, \"POS7\": val7}\n",
    "    \"\"\"\n",
    "    labels = [\"Original\", \"POS=5\", \"POS=7\"]\n",
    "    vals = [triple.get(\"ORIGINAL\", 0.0), triple.get(\"POS5\", 0.0), triple.get(\"POS7\", 0.0)]\n",
    "\n",
    "    plt.figure(figsize=(5.5, 3.8), dpi=160)\n",
    "    plt.bar(labels, vals)\n",
    "    plt.ylabel(\"Avg # of pair-books in Top-K per user\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(out_png)\n",
    "    plt.close()\n",
    "\n",
    "# ======== MAIN ========\n",
    "def main():\n",
    "    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    INV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # -------- Discover pairs from BOTH pos5 and pos7 --------\n",
    "    PAIRS = discover_pairs_from_dirs(POS_DIRS, K_LIST, N_LIST)\n",
    "    if not PAIRS:\n",
    "        print(\"[WARN] No pairs found in either /5 or /7 directories.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Found {len(PAIRS)} unique unordered pairs across /5 and /7\")\n",
    "    with open(INV_DIR / \"discovered_pairs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for a, b in PAIRS:\n",
    "            f.write(f\"{a},{b}\\n\")\n",
    "    pd.DataFrame(PAIRS, columns=[\"A\", \"B\"]).to_csv(INV_DIR / \"discovered_pairs.csv\", index=False)\n",
    "    print(f\"[OK] Inventory saved in {INV_DIR}\")\n",
    "\n",
    "    # -------- Detailed per-file rows + aggregated summaries --------\n",
    "    detailed_rows = []   # one row per (file)\n",
    "    agg_rows = []        # aggregated over N for each (pos_branch, pair, K)\n",
    "\n",
    "    # Preload original baselines by K\n",
    "    original_avgs_by_pair_k: Dict[Tuple[str, str, int], float] = {}\n",
    "    original_user_counts_by_k: Dict[int, int] = {}\n",
    "    for k in K_LIST:\n",
    "        orig_fp = original_file_for_k(ORIG_DIR, k)\n",
    "        if orig_fp is None:\n",
    "            print(f\"[WARN] ORIGINAL_{k}recommendation.csv not found in {ORIG_DIR}\")\n",
    "            continue\n",
    "        try:\n",
    "            df_orig = load_rec_csv(orig_fp)\n",
    "            # compute for all pairs on the same ORIGINAL file\n",
    "            users_cnt = int(df_orig[\"user_id\"].nunique())\n",
    "            original_user_counts_by_k[k] = users_cnt\n",
    "            for (A, B) in PAIRS:\n",
    "                avgc, _ = per_user_avg_pair_count(df_orig, A, B)\n",
    "                original_avgs_by_pair_k[(A, B, k)] = avgc\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Reading ORIGINAL {k}: {e}\")\n",
    "\n",
    "    # Collect poisoned per-file rows, then aggregate across N for each (pair, K)\n",
    "    for pos_dir in POS_DIRS:\n",
    "        pos_label = pos_dir.name  # \"5\" or \"7\"\n",
    "        for (A, B) in PAIRS:\n",
    "            pair_slug = slugify_pair(A, B)\n",
    "            for k in K_LIST:\n",
    "                per_n_vals = []\n",
    "                per_n_users = []\n",
    "                # collect all matching files for all N\n",
    "                matched_files = []\n",
    "                for n in N_LIST:\n",
    "                    files = injected_files_for_pair_k_n(pos_dir, A, B, k, n)\n",
    "                    matched_files.extend(files)\n",
    "                # detailed rows\n",
    "                for f in matched_files:\n",
    "                    try:\n",
    "                        df = load_rec_csv(f)\n",
    "                        avgc, users_cnt = per_user_avg_pair_count(df, A, B)\n",
    "                        per_n_vals.append(avgc)\n",
    "                        per_n_users.append(users_cnt)\n",
    "                        detailed_rows.append({\n",
    "                            \"pos_branch\": pos_label,\n",
    "                            \"pair\": pair_slug,\n",
    "                            \"A\": A, \"B\": B,\n",
    "                            \"K\": k,\n",
    "                            \"file\": f.name,\n",
    "                            \"avg_count\": avgc,\n",
    "                            \"users_counted\": users_cnt\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Reading {f}: {e}\")\n",
    "\n",
    "                # aggregated (mean of per-N)\n",
    "                if per_n_vals:\n",
    "                    avg_over_n = float(sum(per_n_vals) / len(per_n_vals))\n",
    "                    users_cnt_max = max(per_n_users) if per_n_users else 0\n",
    "                    agg_rows.append({\n",
    "                        \"pos_branch\": pos_label,\n",
    "                        \"pair\": pair_slug,\n",
    "                        \"A\": A, \"B\": B,\n",
    "                        \"K\": k,\n",
    "                        \"avg_count_over_N\": avg_over_n,\n",
    "                        \"users_counted_max\": users_cnt_max\n",
    "                    })\n",
    "\n",
    "    # Save detailed rows\n",
    "    if detailed_rows:\n",
    "        dfd = pd.DataFrame(detailed_rows)\n",
    "        dfd.sort_values(by=[\"pair\", \"pos_branch\", \"K\", \"file\"], inplace=True)\n",
    "        dfd_out = OUT_ROOT / \"DETAILED_per_file_pair_counts.csv\"\n",
    "        dfd.to_csv(dfd_out, index=False)\n",
    "        print(f\"[OK] Saved per-file details: {dfd_out}\")\n",
    "    else:\n",
    "        print(\"[WARN] No detailed rows collected.\")\n",
    "\n",
    "    # Build combined summary with Original vs POS=5 vs POS=7 per pair per K\n",
    "    if agg_rows:\n",
    "        dfa = pd.DataFrame(agg_rows)\n",
    "        # pivot POS branches\n",
    "        # Compute Original avg per pair/K, attach\n",
    "        def get_orig_avg(row):\n",
    "            key = (row[\"A\"], row[\"B\"], int(row[\"K\"]))\n",
    "            return original_avgs_by_pair_k.get(key, 0.0)\n",
    "\n",
    "        dfa[\"avg_original\"] = dfa.apply(get_orig_avg, axis=1)\n",
    "\n",
    "        # Split POS=5 and POS=7 columns\n",
    "        d5 = dfa[dfa[\"pos_branch\"] == \"5\"][[\"pair\", \"A\", \"B\", \"K\", \"avg_count_over_N\"]].rename(\n",
    "            columns={\"avg_count_over_N\": \"avg_pos5\"})\n",
    "        d7 = dfa[dfa[\"pos_branch\"] == \"7\"][[\"pair\", \"A\", \"B\", \"K\", \"avg_count_over_N\"]].rename(\n",
    "            columns={\"avg_count_over_N\": \"avg_pos7\"})\n",
    "        base = pd.DataFrame([(slugify_pair(a, b), a, b, k) for (a,b) in PAIRS for k in K_LIST],\n",
    "                            columns=[\"pair\", \"A\", \"B\", \"K\"])\n",
    "        comb = (base.merge(d5, on=[\"pair\", \"A\", \"B\", \"K\"], how=\"left\")\n",
    "                    .merge(d7, on=[\"pair\", \"A\", \"B\", \"K\"], how=\"left\"))\n",
    "\n",
    "        # add original averages\n",
    "        comb[\"avg_original\"] = comb.apply(lambda r: original_avgs_by_pair_k.get((r[\"A\"], r[\"B\"], int(r[\"K\"])), 0.0), axis=1)\n",
    "\n",
    "        # fill NaNs with 0.0 where a branch might be missing\n",
    "        for col in [\"avg_pos5\", \"avg_pos7\", \"avg_original\"]:\n",
    "            if col in comb.columns:\n",
    "                comb[col] = comb[col].fillna(0.0)\n",
    "\n",
    "        # deltas\n",
    "        comb[\"delta_5_minus_orig\"] = comb[\"avg_pos5\"] - comb[\"avg_original\"]\n",
    "        comb[\"delta_7_minus_orig\"] = comb[\"avg_pos7\"] - comb[\"avg_original\"]\n",
    "\n",
    "        comb.sort_values(by=[\"pair\", \"K\"], inplace=True)\n",
    "        out_all = OUT_ROOT / \"SUMMARY_pair_avg_counts_with_original.csv\"\n",
    "        comb.to_csv(out_all, index=False)\n",
    "        print(f\"[OK] Saved combined summary: {out_all}\")\n",
    "\n",
    "        # -------- Figures: 3-bar charts per pair per K --------\n",
    "        for _, row in comb.iterrows():\n",
    "            pair_slug = row[\"pair\"]\n",
    "            A, B, k = row[\"A\"], row[\"B\"], int(row[\"K\"])\n",
    "            triple = {\n",
    "                \"ORIGINAL\": float(row[\"avg_original\"]),\n",
    "                \"POS5\": float(row.get(\"avg_pos5\", 0.0)),\n",
    "                \"POS7\": float(row.get(\"avg_pos7\", 0.0)),\n",
    "            }\n",
    "            title = f\"{A} + {B} (Top-{k})\"\n",
    "            out_png = FIG_DIR / f\"{pair_slug}_K{k}.png\"\n",
    "            grouped_bar_chart(triple, title, out_png)\n",
    "        print(f\"[OK] Figures written to: {FIG_DIR}\")\n",
    "\n",
    "    else:\n",
    "        print(\"[WARN] No aggregated rows collected; skipping summary and figures.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
