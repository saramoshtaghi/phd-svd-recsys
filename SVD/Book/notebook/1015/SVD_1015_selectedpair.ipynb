{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# SVD_1015_focus_pairs_report.py\n",
    "# Pure SVD (Surprise) over ONLY these pairs from PAIR_INJECTION/{5,7}:\n",
    "#   ‚Ä¢ Mystery‚ÄìThriller\n",
    "#   ‚Ä¢ Children's‚ÄìDrama  (filenames: Children_s)\n",
    "#   ‚Ä¢ Fantasy‚ÄìHistorical\n",
    "# Generates top-K recommendations for original users only.\n",
    "# Includes explicit, flush=True status lines for Jupyter usage.\n",
    "\n",
    "import os, ast, gc, re, time, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from surprise import Dataset, Reader, SVD\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= PATHS =========\n",
    "ORIGINAL_PATH = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "\n",
    "# Your pair root that contains subfolders \"5\" and \"7\"\n",
    "PAIR_ROOT = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/data/result/rec/top_re/1015/PAIR_INJECTION\")\n",
    "\n",
    "RESULTS_ROOT  = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/SVD_pair\")\n",
    "(RESULTS_ROOT / \"5\").mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS_ROOT / \"7\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= SETTINGS =========\n",
    "TOP_N_LIST = [15, 25, 35]\n",
    "\n",
    "ATTACK_PARAMS = dict(\n",
    "    biased=True, n_factors=8, n_epochs=180,\n",
    "    lr_all=0.012, lr_bi=0.03,\n",
    "    reg_all=0.002, reg_pu=0.0, reg_qi=0.002,\n",
    "    random_state=42, verbose=False,\n",
    ")\n",
    "\n",
    "# Focus ONLY these pairs (either order). Filenames are sanitized.\n",
    "FOCUS_PAIRS = {\n",
    "    (\"Mystery\", \"Thriller\"),\n",
    "    (\"Children_s\", \"Drama\"),   # Children's appears as Children_s in filenames\n",
    "    (\"Fantasy\", \"Historical\"),\n",
    "}\n",
    "\n",
    "# ========= COLS =========\n",
    "USER_COL  = \"user_id\"\n",
    "BOOK_COL  = \"book_id\"\n",
    "RATE_COL  = \"rating\"\n",
    "GENRE_COL = \"genres\"\n",
    "\n",
    "# ========= UTILS =========\n",
    "def now(msg: str):\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "def _parse_genres(genres_str):\n",
    "    if pd.isna(genres_str): return []\n",
    "    s = str(genres_str).strip()\n",
    "    if not s: return []\n",
    "    # Try list/tuple literal\n",
    "    if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(x).strip().strip('\"').strip(\"'\") for x in parsed if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Split by common separators\n",
    "    for sep in [\",\",\"|\",\";\",\"//\",\"/\"]:\n",
    "        if sep in s:\n",
    "            return [t.strip().strip('\"').strip(\"'\") for t in s.split(sep) if t.strip()]\n",
    "    return [s.strip().strip('\"').strip(\"'\")]\n",
    "\n",
    "def load_df(fp: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(\n",
    "        fp,\n",
    "        dtype={USER_COL: \"int64\", BOOK_COL: \"int64\", RATE_COL: \"float64\"},\n",
    "        low_memory=False\n",
    "    )\n",
    "    df = df.dropna(subset=[USER_COL, BOOK_COL, RATE_COL])\n",
    "    df[GENRE_COL] = df[GENRE_COL].fillna(\"\").astype(str)\n",
    "    # Keep rating as float64 but clip hard to [0,7] to be safe\n",
    "    df[RATE_COL] = pd.to_numeric(df[RATE_COL], errors=\"coerce\").fillna(0.0).clip(0, 7)\n",
    "    return df\n",
    "\n",
    "def create_genre_mapping(df: pd.DataFrame):\n",
    "    m = {}\n",
    "    for _, r in df[[BOOK_COL, GENRE_COL]].drop_duplicates(subset=[BOOK_COL]).iterrows():\n",
    "        bid = int(r[BOOK_COL]); gl = _parse_genres(r.get(GENRE_COL, \"\"))\n",
    "        m[bid] = {\n",
    "            \"g1\": gl[0] if len(gl) >= 1 else \"Unknown\",\n",
    "            \"g2\": gl[1] if len(gl) >= 2 else \"\",\n",
    "            \"all\": \", \".join(gl) if gl else \"Unknown\",\n",
    "            \"list\": gl\n",
    "        }\n",
    "    return m\n",
    "\n",
    "def train_svd(df: pd.DataFrame):\n",
    "    reader = Reader(rating_scale=(0, 7))  # allow 0 and 5/7\n",
    "    data = Dataset.load_from_df(df[[USER_COL, BOOK_COL, RATE_COL]], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    svd = SVD(**ATTACK_PARAMS)\n",
    "    svd.fit(trainset)\n",
    "    return svd, trainset\n",
    "\n",
    "def recommend_vectorized(df, original_users, genre_mapping, svd, trainset, base_name: str, out_dir: Path):\n",
    "    # Pull trained components\n",
    "    mu, bu, bi, P, Q = svd.trainset.global_mean, svd.bu, svd.bi, svd.pu, svd.qi\n",
    "\n",
    "    def inner_uid(u):\n",
    "        try: return trainset.to_inner_uid(int(u))\n",
    "        except: return None\n",
    "\n",
    "    def inner_iid(i):\n",
    "        try: return trainset.to_inner_iid(int(i))\n",
    "        except: return None\n",
    "\n",
    "    # All items present in this dataset (trainset may be subset)\n",
    "    all_items_raw = df[BOOK_COL].unique()\n",
    "    inner_to_raw, all_items_inner = {}, []\n",
    "    for bid in all_items_raw:\n",
    "        ii = inner_iid(bid)\n",
    "        if ii is not None:\n",
    "            inner_to_raw[ii] = int(bid)\n",
    "            all_items_inner.append(ii)\n",
    "    all_items_inner = np.array(all_items_inner, dtype=np.int32)\n",
    "\n",
    "    # Items already seen by each user (from THIS dataset)\n",
    "    seen_raw = df.groupby(USER_COL)[BOOK_COL].apply(set).to_dict()\n",
    "\n",
    "    per_topn_rows = {n: [] for n in TOP_N_LIST}\n",
    "    users = list(original_users)\n",
    "\n",
    "    now(f\"Scoring {len(users):,} original users for {base_name}‚Ä¶\")\n",
    "    for idx, u_raw in enumerate(users, 1):\n",
    "        if idx % 1000 == 0:\n",
    "            now(f\"  ‚Ä¢ Scored {idx:,}/{len(users):,} users\")\n",
    "\n",
    "        u = inner_uid(u_raw)\n",
    "        if u is None:\n",
    "            # User not present in this training set; skip\n",
    "            continue\n",
    "\n",
    "        # Candidate items = items in this dataset the user hasn't rated in this dataset\n",
    "        seen_set_raw = seen_raw.get(u_raw, set())\n",
    "        if seen_set_raw:\n",
    "            user_seen_inner = {inner_iid(b) for b in seen_set_raw}\n",
    "            user_seen_inner = {ii for ii in user_seen_inner if ii is not None}\n",
    "            seen_mask = np.fromiter((ii in user_seen_inner for ii in all_items_inner),\n",
    "                                    count=len(all_items_inner), dtype=bool)\n",
    "            cand_inner = all_items_inner[~seen_mask]\n",
    "        else:\n",
    "            cand_inner = all_items_inner\n",
    "\n",
    "        if cand_inner.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Vectorized scoring: mu + bu[u] + bi[i] + Qi @ pu\n",
    "        pu = P[u]\n",
    "        bi_cand = np.take(bi, cand_inner)\n",
    "        Qi = Q[cand_inner]\n",
    "        scores = mu + bu[u] + bi_cand + (Qi @ pu)\n",
    "\n",
    "        for n in TOP_N_LIST:\n",
    "            k = min(n, scores.shape[0])\n",
    "            idx_top = np.argpartition(-scores, k-1)[:k]\n",
    "            idx_order = idx_top[np.argsort(-scores[idx_top])]\n",
    "            sel_inner, sel_scores = cand_inner[idx_order], scores[idx_order]\n",
    "            for rank, (ii, est) in enumerate(zip(sel_inner, sel_scores), start=1):\n",
    "                bid_raw = inner_to_raw[int(ii)]\n",
    "                gm = genre_mapping.get(int(bid_raw), {\"g1\":\"Unknown\",\"g2\":\"\",\"all\":\"Unknown\"})\n",
    "                per_topn_rows[n].append({\n",
    "                    \"user_id\": int(u_raw), \"book_id\": int(bid_raw),\n",
    "                    \"est_score\": float(est), \"rank\": rank,\n",
    "                    \"genre_g1\": gm[\"g1\"], \"genre_g2\": gm[\"g2\"], \"genres_all\": gm[\"all\"],\n",
    "                })\n",
    "\n",
    "    # Save per-K outputs\n",
    "    for n, rows in per_topn_rows.items():\n",
    "        out_df = pd.DataFrame(\n",
    "            rows,\n",
    "            columns=[\"user_id\",\"book_id\",\"est_score\",\"rank\",\"genre_g1\",\"genre_g2\",\"genres_all\"]\n",
    "        ).sort_values([\"user_id\",\"rank\"])\n",
    "        out_path = out_dir / f\"{base_name}_{n}recommendation.csv\"\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        now(f\"Saved ‚Üí {out_path} ({len(out_df):,} rows)\")\n",
    "\n",
    "# --------- PAIR FILE HANDLING ---------\n",
    "# Matches both sample and all variants, e.g.:\n",
    "#   fpair_Mystery__Thriller_25u_pos5_neg0_all.csv\n",
    "#   fpair_Drama__Children_s_200u_pos7_neg0_sample.csv\n",
    "PAIR_NAME_RE = re.compile(\n",
    "    r\"^fpair_(.+)__(.+)_(\\d+)u_pos(\\d+)_neg(NA|0|\\d+)_(\\w+)\\.csv$\"\n",
    ")\n",
    "\n",
    "def _pair_matches_focus(g1_sanitized: str, g2_sanitized: str) -> bool:\n",
    "    return ((g1_sanitized, g2_sanitized) in FOCUS_PAIRS) or ((g2_sanitized, g1_sanitized) in FOCUS_PAIRS)\n",
    "\n",
    "def _pretty_pair_name(g1: str, g2: str) -> str:\n",
    "    # Turn Children_s back into Children's for readability\n",
    "    g1p = \"Children's\" if g1 == \"Children_s\" else g1\n",
    "    g2p = \"Children's\" if g2 == \"Children_s\" else g2\n",
    "    return f\"{g1p} & {g2p}\"\n",
    "\n",
    "def scan_pair_files_filtered(pair_root: Path):\n",
    "    \"\"\"\n",
    "    Return only the pair files whose (g1,g2) is in FOCUS_PAIRS (either order).\n",
    "    Each item: {'pos_folder': 5|7, 'path': Path, 'base_name': str, 'g1': str, 'g2': str}\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for pos_folder in [\"5\", \"7\"]:\n",
    "        sub = pair_root / pos_folder\n",
    "        if not sub.exists(): \n",
    "            continue\n",
    "        for fp in sorted(sub.glob(\"fpair_*.csv\")):\n",
    "            m = PAIR_NAME_RE.match(fp.name)\n",
    "            if not m: \n",
    "                continue\n",
    "            g1, g2 = m.group(1), m.group(2)  # sanitized\n",
    "            if _pair_matches_focus(g1, g2):\n",
    "                items.append({\n",
    "                    \"pos_folder\": int(pos_folder),\n",
    "                    \"path\": fp,\n",
    "                    \"base_name\": fp.stem,\n",
    "                    \"g1\": g1,\n",
    "                    \"g2\": g2\n",
    "                })\n",
    "    return items\n",
    "\n",
    "# ========= MAIN =========\n",
    "def main():\n",
    "    start = time.time()\n",
    "    now(\"=== SVD (poison-only) ‚Äî FOCUSED PAIRS ONLY (pos=5 & pos=7) ===\")\n",
    "    now(f\"Focus pairs accepted (either order): \" +\n",
    "        \", \".join(sorted({_pretty_pair_name(*p) for p in FOCUS_PAIRS})))\n",
    "\n",
    "    # ----- ORIGINAL baseline -----\n",
    "    now(\"üî∞ Starting baseline model training on ORIGINAL dataset‚Ä¶\")\n",
    "    orig_df = load_df(ORIGINAL_PATH)\n",
    "    original_users = set(orig_df[USER_COL].unique())\n",
    "    n_items_original = orig_df[BOOK_COL].nunique()\n",
    "    now(f\"üìÑ ORIGINAL loaded ‚Äî users={len(original_users):,}, items={n_items_original:,}, rows={len(orig_df):,}\")\n",
    "\n",
    "    try:\n",
    "        orig_map = create_genre_mapping(orig_df)\n",
    "        svd_base, ts_base = train_svd(orig_df)\n",
    "        now(\"üß† Baseline SVD trained on ORIGINAL.\")\n",
    "        now(\"üìå Generating baseline recommendations for ORIGINAL‚Ä¶\")\n",
    "        recommend_vectorized(orig_df, original_users, orig_map, svd_base, ts_base, \"ORIGINAL\", RESULTS_ROOT)\n",
    "        now(\"‚úÖ Finished baseline recommendations for ORIGINAL.\")\n",
    "        del svd_base, ts_base; gc.collect()\n",
    "    except Exception as e:\n",
    "        now(f\"[ERROR] Baseline ORIGINAL run failed: {e}\")\n",
    "\n",
    "    # ----- Focused pair runs -----\n",
    "    jobs = scan_pair_files_filtered(PAIR_ROOT)\n",
    "    if not jobs:\n",
    "        now(f\"‚ö†Ô∏è No matching pair-injection CSVs found under {PAIR_ROOT}/5 or /7 for the focus pairs.\")\n",
    "        return\n",
    "\n",
    "    now(f\"üîé Found {len(jobs)} matching pair-injection files.\")\n",
    "    for i, job in enumerate(jobs, 1):\n",
    "        fp        = job[\"path\"]\n",
    "        pos_folder= job[\"pos_folder\"]  # 5 or 7\n",
    "        base_name = job[\"base_name\"]\n",
    "        g1, g2    = job[\"g1\"], job[\"g2\"]\n",
    "        out_dir   = RESULTS_ROOT / str(pos_folder)\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Report line for notebook: which bucket and which pair\n",
    "        bucket = \"POS=5\" if pos_folder == 5 else \"POS=7\"\n",
    "        now(f\"\\nüìò Working on {bucket} poisoned dataset ({i}/{len(jobs)})\")\n",
    "        now(f\"   Pair: {_pretty_pair_name(g1, g2)}\")\n",
    "        now(f\"   File: {fp.name}\")\n",
    "\n",
    "        try:\n",
    "            df = load_df(fp)\n",
    "            n_items = df[BOOK_COL].nunique()\n",
    "            n_rows  = len(df)\n",
    "            now(f\"   Loaded poisoned dataset ‚Äî items={n_items:,}, rows={n_rows:,}\")\n",
    "\n",
    "            gmap = create_genre_mapping(df)\n",
    "            now(\"   Training SVD‚Ä¶\")\n",
    "            svd, ts = train_svd(df)\n",
    "            now(\"   SVD trained. Generating recommendations‚Ä¶\")\n",
    "            recommend_vectorized(df, original_users, gmap, svd, ts, base_name, out_dir)\n",
    "            now(f\"‚úÖ Finished SVD + recommendations for {_pretty_pair_name(g1, g2)} ({bucket}).\")\n",
    "            del df, svd, ts; gc.collect()\n",
    "        except Exception as e:\n",
    "            now(f\"[ERROR] {fp.name}: {e}\")\n",
    "\n",
    "    hrs = (time.time() - start) / 3600\n",
    "    now(\"\\nüèÅ All selected focus pairs have been processed.\")\n",
    "    now(f\"Results in: {RESULTS_ROOT} (subfolders 5/ and 7/)\")\n",
    "    now(f\"Total runtime ~ {hrs:.2f} h\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
