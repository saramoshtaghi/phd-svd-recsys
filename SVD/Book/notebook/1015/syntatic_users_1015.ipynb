{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>decade</th>\n",
       "      <th>original_title</th>\n",
       "      <th>authors</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>La sombra del viento</td>\n",
       "      <td>Carlos Ruiz Zafón, Lucia Graves</td>\n",
       "      <td>Mystery, Historical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4081</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>260</td>\n",
       "      <td>5</td>\n",
       "      <td>1930</td>\n",
       "      <td>How to Win Friends and Influence People</td>\n",
       "      <td>Dale Carnegie</td>\n",
       "      <td>Nonfiction, Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>9296</td>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>Das Drama des begabten Kindes und die Suche na...</td>\n",
       "      <td>Alice  Miller, Ruth Ward</td>\n",
       "      <td>Horror, Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2318</td>\n",
       "      <td>3</td>\n",
       "      <td>1990</td>\n",
       "      <td>The Millionaire Next Door: The Surprising Secr...</td>\n",
       "      <td>Thomas J. Stanley, William D. Danko</td>\n",
       "      <td>Nonfiction, Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976474</th>\n",
       "      <td>49925</td>\n",
       "      <td>510</td>\n",
       "      <td>5</td>\n",
       "      <td>1990</td>\n",
       "      <td>The Great Hunt</td>\n",
       "      <td>Robert Jordan</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976475</th>\n",
       "      <td>49925</td>\n",
       "      <td>528</td>\n",
       "      <td>4</td>\n",
       "      <td>1990</td>\n",
       "      <td>The Dragon Reborn</td>\n",
       "      <td>Robert Jordan</td>\n",
       "      <td>Classics, Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976476</th>\n",
       "      <td>49925</td>\n",
       "      <td>722</td>\n",
       "      <td>4</td>\n",
       "      <td>1990</td>\n",
       "      <td>The Shadow Rising</td>\n",
       "      <td>Robert Jordan</td>\n",
       "      <td>Adventure, Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976477</th>\n",
       "      <td>49925</td>\n",
       "      <td>949</td>\n",
       "      <td>5</td>\n",
       "      <td>1990</td>\n",
       "      <td>The Fires of Heaven</td>\n",
       "      <td>Robert Jordan</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976478</th>\n",
       "      <td>49925</td>\n",
       "      <td>1023</td>\n",
       "      <td>4</td>\n",
       "      <td>1990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5976479 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  book_id  rating decade  \\\n",
       "0              1      258       5   2000   \n",
       "1              2     4081       4   2000   \n",
       "2              2      260       5   1930   \n",
       "3              2     9296       5   1970   \n",
       "4              2     2318       3   1990   \n",
       "...          ...      ...     ...    ...   \n",
       "5976474    49925      510       5   1990   \n",
       "5976475    49925      528       4   1990   \n",
       "5976476    49925      722       4   1990   \n",
       "5976477    49925      949       5   1990   \n",
       "5976478    49925     1023       4   1990   \n",
       "\n",
       "                                            original_title  \\\n",
       "0                                     La sombra del viento   \n",
       "1                                                      NaN   \n",
       "2                  How to Win Friends and Influence People   \n",
       "3        Das Drama des begabten Kindes und die Suche na...   \n",
       "4        The Millionaire Next Door: The Surprising Secr...   \n",
       "...                                                    ...   \n",
       "5976474                                     The Great Hunt   \n",
       "5976475                                 The Dragon Reborn    \n",
       "5976476                                 The Shadow Rising    \n",
       "5976477                                The Fires of Heaven   \n",
       "5976478                                                NaN   \n",
       "\n",
       "                                     authors               genres  \n",
       "0            Carlos Ruiz Zafón, Lucia Graves  Mystery, Historical  \n",
       "1                                        NaN                  NaN  \n",
       "2                              Dale Carnegie    Nonfiction, Drama  \n",
       "3                   Alice  Miller, Ruth Ward      Horror, Mystery  \n",
       "4        Thomas J. Stanley, William D. Danko    Nonfiction, Drama  \n",
       "...                                      ...                  ...  \n",
       "5976474                        Robert Jordan   Fantasy, Adventure  \n",
       "5976475                        Robert Jordan      Classics, Drama  \n",
       "5976476                        Robert Jordan     Adventure, Drama  \n",
       "5976477                        Robert Jordan   Fantasy, Adventure  \n",
       "5976478                                  NaN                  NaN  \n",
       "\n",
       "[5976479 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# --- Your canonical 13 genres, fixed order for rows/cols ---\n",
    "GENRES_13 = [\n",
    "    \"Adult\", \"Adventure\", \"Children's\", \"Classics\", \"Drama\",\n",
    "    \"Fantasy\", \"Historical\", \"Horror\", \"Mystery\", \"Nonfiction\",\n",
    "    \"Romance\", \"Science Fiction\", \"Thriller\"\n",
    "]\n",
    "\n",
    "def parse_genres(s):\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    return [g.strip() for g in str(s).split(\",\") if g.strip()]\n",
    "\n",
    "def to_canonical_set(gstr):\n",
    "    glist = [g for g in parse_genres(gstr) if g in GENRES_13]\n",
    "    # de-duplicate while preserving first-seen order\n",
    "    return list(dict.fromkeys(glist))\n",
    "\n",
    "def analyze_ordered_cooccurrence(df):\n",
    "    \"\"\"\n",
    "    Build an ORDER-SENSITIVE co-occurrence matrix (Gi -> Gj), Gi != Gj only\n",
    "    \"\"\"\n",
    "    print(\"=== ORDERED CO-OCCURRENCE ANALYSIS ===\")\n",
    "    \n",
    "    # 1) Collapse to UNIQUE books and keep one ordered genre list per book\n",
    "    books = (\n",
    "        df[['book_id', 'genres']]\n",
    "          .dropna(subset=['book_id'])\n",
    "          .sort_values('book_id')\n",
    "          .drop_duplicates(subset=['book_id'], keep='first')\n",
    "          .copy()\n",
    "    )\n",
    "    \n",
    "    # Parse + filter to canonical 13, preserving order within each book\n",
    "    books['genre_list'] = books['genres'].apply(parse_genres).apply(\n",
    "        lambda gl: [g for g in gl if g in GENRES_13]\n",
    "    )\n",
    "    books = books[books['genre_list'].map(len) > 0].copy()\n",
    "    \n",
    "    # 2) Count and print how many UNIQUE canonical genres are actually present in df\n",
    "    present = []\n",
    "    for gl in books['genre_list']:\n",
    "        present.extend(gl)\n",
    "    present = sorted(set(present), key=lambda g: GENRES_13.index(g))\n",
    "    \n",
    "    print(f\"Unique genres present (from canonical 13): {len(present)} / 13\")\n",
    "    print(present)\n",
    "    \n",
    "    # 3) Build an ORDER-SENSITIVE co-occurrence matrix (Gi -> Gj), Gi != Gj only\n",
    "    idx = {g: i for i, g in enumerate(GENRES_13)}\n",
    "    n = len(GENRES_13)\n",
    "    co_mat = np.zeros((n, n), dtype=int)\n",
    "    \n",
    "    for gl in books['genre_list']:\n",
    "        # First occurrence positions per genre in this book (preserve order)\n",
    "        pos = {}\n",
    "        for k, g in enumerate(gl):\n",
    "            if g not in pos:\n",
    "                pos[g] = k\n",
    "        \n",
    "        # Count ordered pairs where Gi occurs BEFORE Gj (Gi != Gj). No diagonal.\n",
    "        genres_in_book = list(pos.keys())\n",
    "        for gi in genres_in_book:\n",
    "            for gj in genres_in_book:\n",
    "                if gi == gj:\n",
    "                    continue\n",
    "                if pos[gi] < pos[gj]:\n",
    "                    co_mat[idx[gi], idx[gj]] += 1\n",
    "    \n",
    "    # Wrap into a 13x13 DataFrame with fixed order (diagonal is guaranteed 0)\n",
    "    co_df = pd.DataFrame(co_mat, index=GENRES_13, columns=GENRES_13)\n",
    "    \n",
    "    print(\"Ordered Co-occurrence Matrix:\")\n",
    "    print(co_df)\n",
    "    return co_df\n",
    "\n",
    "def analyze_symmetric_cooccurrence(df):\n",
    "    \"\"\"\n",
    "    Build a SYMMETRIC co-occurrence matrix over UNIQUE books\n",
    "    \"\"\"\n",
    "    print(\"\\n=== SYMMETRIC CO-OCCURRENCE ANALYSIS ===\")\n",
    "    \n",
    "    # 1) Collapse to UNIQUE books; keep one genres string per book_id\n",
    "    books = (\n",
    "        df[['book_id', 'genres']]\n",
    "          .dropna(subset=['book_id'])\n",
    "          .sort_values('book_id')\n",
    "          .drop_duplicates(subset=['book_id'], keep='first')\n",
    "          .copy()\n",
    "    )\n",
    "    \n",
    "    # 2) Parse & filter to canonical 13; also de-duplicate within a book\n",
    "    books['genre_list'] = books['genres'].apply(to_canonical_set)\n",
    "    books = books[books['genre_list'].map(len) > 0].copy()\n",
    "    \n",
    "    # 3) Report how many of the canonical 13 actually appear\n",
    "    present = sorted(set(g for gl in books['genre_list'] for g in gl), key=lambda g: GENRES_13.index(g))\n",
    "    print(f\"Unique genres present (from canonical 13): {len(present)} / 13\")\n",
    "    print(present)\n",
    "    \n",
    "    # 4) Build a SYMMETRIC co-occurrence matrix over UNIQUE books\n",
    "    #    Cell (Gi, Gj) = number of UNIQUE books that contain BOTH Gi and Gj (order-agnostic)\n",
    "    idx = {g: i for i, g in enumerate(GENRES_13)}\n",
    "    n = len(GENRES_13)\n",
    "    co_mat = np.zeros((n, n), dtype=int)\n",
    "    \n",
    "    for gl in books['genre_list']:\n",
    "        s = list(set(gl))           # ensure uniqueness before making pairs\n",
    "        for g1, g2 in combinations(s, 2):\n",
    "            i, j = idx[g1], idx[g2]\n",
    "            # increment both directions to enforce symmetry\n",
    "            co_mat[i, j] += 1\n",
    "            co_mat[j, i] += 1\n",
    "    \n",
    "    # Zero the diagonal (no GiGi counts)\n",
    "    np.fill_diagonal(co_mat, 0)\n",
    "    \n",
    "    # 5) Wrap in DataFrame\n",
    "    co_df = pd.DataFrame(co_mat, index=GENRES_13, columns=GENRES_13)\n",
    "    \n",
    "    print(\"Symmetric Co-occurrence Matrix:\")\n",
    "    print(co_df)\n",
    "    return co_df\n",
    "\n",
    "def main_analysis(df):\n",
    "    \"\"\"\n",
    "    Run both analyses on the provided dataframe\n",
    "    \"\"\"\n",
    "    print(\"Starting Genre Co-occurrence Analysis\")\n",
    "    print(f\"Total rows in dataset: {len(df)}\")\n",
    "    \n",
    "    # Run ordered analysis\n",
    "    ordered_matrix = analyze_ordered_cooccurrence(df)\n",
    "    \n",
    "    # Run symmetric analysis  \n",
    "    symmetric_matrix = analyze_symmetric_cooccurrence(df)\n",
    "    \n",
    "    # Optional: save results\n",
    "    save_results = input(\"\\nSave results to CSV? (y/n): \").lower() == 'y'\n",
    "    if save_results:\n",
    "        ordered_matrix.to_csv(\"ordered_genre_cooccurrence_unique_books_13x13.csv\", index=True)\n",
    "        symmetric_matrix.to_csv(\"symmetric_genre_cooccurrence_unique_books_13x13.csv\", index=True)\n",
    "        print(\"Results saved to CSV files.\")\n",
    "    \n",
    "    return ordered_matrix, symmetric_matrix\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame called 'df' with columns 'book_id' and 'genres'\n",
    "# ordered_result, symmetric_result = main_analysis(df)\n",
    "main_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Classics</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Historical</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Nonfiction</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Science Fiction</th>\n",
       "      <th>Thriller</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adult</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adventure</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Children's</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>93</td>\n",
       "      <td>66</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Classics</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drama</td>\n",
       "      <td>53</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fantasy</td>\n",
       "      <td>2</td>\n",
       "      <td>812</td>\n",
       "      <td>99</td>\n",
       "      <td>14</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>193</td>\n",
       "      <td>236</td>\n",
       "      <td>16</td>\n",
       "      <td>232</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Historical</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>286</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>44</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Horror</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mystery</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>246</td>\n",
       "      <td>14</td>\n",
       "      <td>67</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>6</td>\n",
       "      <td>88</td>\n",
       "      <td>585</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Romance</td>\n",
       "      <td>139</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>933</td>\n",
       "      <td>119</td>\n",
       "      <td>130</td>\n",
       "      <td>32</td>\n",
       "      <td>207</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>65</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>130</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>289</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Adult  Adventure  Children's  Classics  Drama  Fantasy  \\\n",
       "0             Adult      0          0           0         4     91        0   \n",
       "1         Adventure      0          0           9        46     35        6   \n",
       "2        Children's      0        284           0       137     93       66   \n",
       "3          Classics      1         17           5         0    286       11   \n",
       "4             Drama     53         16           1        83      0        1   \n",
       "5           Fantasy      2        812          99        14     98        0   \n",
       "6        Historical      3         31           9        14    286        4   \n",
       "7            Horror      0          6           8        15     33       42   \n",
       "8           Mystery      5         45          11        37    246       14   \n",
       "9        Nonfiction     18         62           6        88    585        7   \n",
       "10          Romance    139         18           9        37    933      119   \n",
       "11  Science Fiction      0        258          12        34     65       24   \n",
       "12         Thriller      4         55           0         0     26        0   \n",
       "\n",
       "    Historical  Horror  Mystery  Nonfiction  Romance  Science Fiction  \\\n",
       "0            2       0        3           6        0                0   \n",
       "1           17       1       27          14        1                6   \n",
       "2           30       4       56          11       13                0   \n",
       "3           10       6       23          25        7                1   \n",
       "4           21       0       26          18        8                0   \n",
       "5           35     193      236          16      232               17   \n",
       "6            0       1       38          44       59                1   \n",
       "7            0       0      194           3       23                6   \n",
       "8           67      38        0           3       50                0   \n",
       "9           35       1       19           0       10               41   \n",
       "10         130      32      207          20        0                2   \n",
       "11           7      40      130          28       22                0   \n",
       "12           6      26      289           5        2                5   \n",
       "\n",
       "    Thriller  \n",
       "0          0  \n",
       "1         23  \n",
       "2          0  \n",
       "3          0  \n",
       "4          2  \n",
       "5         40  \n",
       "6          7  \n",
       "7         97  \n",
       "8        799  \n",
       "9          6  \n",
       "10        58  \n",
       "11       156  \n",
       "12         0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_matrix = pd.read_csv(\"ordered_genre_cooccurrence_unique_books_13x13.csv\")\n",
    "ordered_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Classics</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Historical</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Nonfiction</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Science Fiction</th>\n",
       "      <th>Thriller</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adult</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>144</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adventure</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "      <td>63</td>\n",
       "      <td>51</td>\n",
       "      <td>818</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>19</td>\n",
       "      <td>264</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Children's</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>94</td>\n",
       "      <td>165</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Classics</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>369</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>113</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drama</td>\n",
       "      <td>144</td>\n",
       "      <td>51</td>\n",
       "      <td>94</td>\n",
       "      <td>369</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>307</td>\n",
       "      <td>33</td>\n",
       "      <td>272</td>\n",
       "      <td>603</td>\n",
       "      <td>941</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fantasy</td>\n",
       "      <td>2</td>\n",
       "      <td>818</td>\n",
       "      <td>165</td>\n",
       "      <td>25</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>235</td>\n",
       "      <td>250</td>\n",
       "      <td>23</td>\n",
       "      <td>351</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Historical</td>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>307</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>79</td>\n",
       "      <td>189</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Horror</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>46</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mystery</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "      <td>67</td>\n",
       "      <td>60</td>\n",
       "      <td>272</td>\n",
       "      <td>250</td>\n",
       "      <td>105</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>257</td>\n",
       "      <td>130</td>\n",
       "      <td>1088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>24</td>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "      <td>113</td>\n",
       "      <td>603</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>69</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Romance</td>\n",
       "      <td>139</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>941</td>\n",
       "      <td>351</td>\n",
       "      <td>189</td>\n",
       "      <td>55</td>\n",
       "      <td>257</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>65</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>130</td>\n",
       "      <td>69</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>123</td>\n",
       "      <td>1088</td>\n",
       "      <td>11</td>\n",
       "      <td>60</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Adult  Adventure  Children's  Classics  Drama  Fantasy  \\\n",
       "0             Adult      0          0           0         5    144        2   \n",
       "1         Adventure      0          0         293        63     51      818   \n",
       "2        Children's      0        293           0       142     94      165   \n",
       "3          Classics      5         63         142         0    369       25   \n",
       "4             Drama    144         51          94       369      0       99   \n",
       "5           Fantasy      2        818         165        25     99        0   \n",
       "6        Historical      5         48          39        24    307       39   \n",
       "7            Horror      0          7          12        21     33      235   \n",
       "8           Mystery      8         72          67        60    272      250   \n",
       "9        Nonfiction     24         76          17       113    603       23   \n",
       "10          Romance    139         19          22        44    941      351   \n",
       "11  Science Fiction      0        264          12        35     65       41   \n",
       "12         Thriller      4         78           0         0     28       40   \n",
       "\n",
       "    Historical  Horror  Mystery  Nonfiction  Romance  Science Fiction  \\\n",
       "0            5       0        8          24      139                0   \n",
       "1           48       7       72          76       19              264   \n",
       "2           39      12       67          17       22               12   \n",
       "3           24      21       60         113       44               35   \n",
       "4          307      33      272         603      941               65   \n",
       "5           39     235      250          23      351               41   \n",
       "6            0       1      105          79      189                8   \n",
       "7            1       0      232           4       55               46   \n",
       "8          105     232        0          22      257              130   \n",
       "9           79       4       22           0       30               69   \n",
       "10         189      55      257          30        0               24   \n",
       "11           8      46      130          69       24                0   \n",
       "12          13     123     1088          11       60              161   \n",
       "\n",
       "    Thriller  \n",
       "0          4  \n",
       "1         78  \n",
       "2          0  \n",
       "3          0  \n",
       "4         28  \n",
       "5         40  \n",
       "6         13  \n",
       "7        123  \n",
       "8       1088  \n",
       "9         11  \n",
       "10        60  \n",
       "11       161  \n",
       "12         0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symmetric_matrix = pd.read_csv(\"symmetric_genre_cooccurrence_unique_books_13x13.csv\")\n",
    "symmetric_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# --- Your canonical 13 genres, fixed order for rows/cols ---\n",
    "GENRES_13 = [\n",
    "    \"Adult\", \"Adventure\", \"Children's\", \"Classics\", \"Drama\",\n",
    "    \"Fantasy\", \"Historical\", \"Horror\", \"Mystery\", \"Nonfiction\",\n",
    "    \"Romance\", \"Science Fiction\", \"Thriller\"\n",
    "]\n",
    "\n",
    "def parse_genres(s):\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    return [g.strip() for g in str(s).split(\",\") if g.strip()]\n",
    "\n",
    "def to_canonical_set(gstr):\n",
    "    glist = [g for g in parse_genres(gstr) if g in GENRES_13]\n",
    "    # de-duplicate while preserving first-seen order\n",
    "    return list(dict.fromkeys(glist))\n",
    "\n",
    "def analyze_ordered_cooccurrence(df):\n",
    "    \"\"\"\n",
    "    Build an ORDER-SENSITIVE co-occurrence matrix (Gi -> Gj), Gi != Gj only\n",
    "    \"\"\"\n",
    "    print(\"=== ORDERED CO-OCCURRENCE ANALYSIS ===\")\n",
    "    \n",
    "    # 1) Collapse to UNIQUE books and keep one ordered genre list per book\n",
    "    books = (\n",
    "        df[['book_id', 'genres']]\n",
    "          .dropna(subset=['book_id'])\n",
    "          .sort_values('book_id')\n",
    "          .drop_duplicates(subset=['book_id'], keep='first')\n",
    "          .copy()\n",
    "    )\n",
    "    \n",
    "    # Parse + filter to canonical 13, preserving order within each book\n",
    "    books['genre_list'] = books['genres'].apply(parse_genres).apply(\n",
    "        lambda gl: [g for g in gl if g in GENRES_13]\n",
    "    )\n",
    "    books = books[books['genre_list'].map(len) > 0].copy()\n",
    "    \n",
    "    # 2) Count and print how many UNIQUE canonical genres are actually present in df\n",
    "    present = []\n",
    "    for gl in books['genre_list']:\n",
    "        present.extend(gl)\n",
    "    present = sorted(set(present), key=lambda g: GENRES_13.index(g))\n",
    "    \n",
    "    print(f\"Unique genres present (from canonical 13): {len(present)} / 13\")\n",
    "    print(present)\n",
    "    \n",
    "    # 3) Build an ORDER-SENSITIVE co-occurrence matrix (Gi -> Gj), Gi != Gj only\n",
    "    idx = {g: i for i, g in enumerate(GENRES_13)}\n",
    "    n = len(GENRES_13)\n",
    "    co_mat = np.zeros((n, n), dtype=int)\n",
    "    \n",
    "    for gl in books['genre_list']:\n",
    "        # First occurrence positions per genre in this book (preserve order)\n",
    "        pos = {}\n",
    "        for k, g in enumerate(gl):\n",
    "            if g not in pos:\n",
    "                pos[g] = k\n",
    "        \n",
    "        # Count ordered pairs where Gi occurs BEFORE Gj (Gi != Gj). No diagonal.\n",
    "        genres_in_book = list(pos.keys())\n",
    "        for gi in genres_in_book:\n",
    "            for gj in genres_in_book:\n",
    "                if gi == gj:\n",
    "                    continue\n",
    "                if pos[gi] < pos[gj]:\n",
    "                    co_mat[idx[gi], idx[gj]] += 1\n",
    "    \n",
    "    # Wrap into a 13x13 DataFrame with fixed order (diagonal is guaranteed 0)\n",
    "    co_df = pd.DataFrame(co_mat, index=GENRES_13, columns=GENRES_13)\n",
    "    \n",
    "    print(\"Ordered Co-occurrence Matrix:\")\n",
    "    print(co_df)\n",
    "    return co_df\n",
    "\n",
    "def analyze_symmetric_cooccurrence(df):\n",
    "    \"\"\"\n",
    "    Build a SYMMETRIC co-occurrence matrix over UNIQUE books\n",
    "    \"\"\"\n",
    "    print(\"\\n=== SYMMETRIC CO-OCCURRENCE ANALYSIS ===\")\n",
    "    \n",
    "    # 1) Collapse to UNIQUE books; keep one genres string per book_id\n",
    "    books = (\n",
    "        df[['book_id', 'genres']]\n",
    "          .dropna(subset=['book_id'])\n",
    "          .sort_values('book_id')\n",
    "          .drop_duplicates(subset=['book_id'], keep='first')\n",
    "          .copy()\n",
    "    )\n",
    "    \n",
    "    # 2) Parse & filter to canonical 13; also de-duplicate within a book\n",
    "    books['genre_list'] = books['genres'].apply(to_canonical_set)\n",
    "    books = books[books['genre_list'].map(len) > 0].copy()\n",
    "    \n",
    "    # 3) Report how many of the canonical 13 actually appear\n",
    "    present = sorted(set(g for gl in books['genre_list'] for g in gl), key=lambda g: GENRES_13.index(g))\n",
    "    print(f\"Unique genres present (from canonical 13): {len(present)} / 13\")\n",
    "    print(present)\n",
    "    \n",
    "    # 4) Build a SYMMETRIC co-occurrence matrix over UNIQUE books\n",
    "    #    Cell (Gi, Gj) = number of UNIQUE books that contain BOTH Gi and Gj (order-agnostic)\n",
    "    idx = {g: i for i, g in enumerate(GENRES_13)}\n",
    "    n = len(GENRES_13)\n",
    "    co_mat = np.zeros((n, n), dtype=int)\n",
    "    \n",
    "    for gl in books['genre_list']:\n",
    "        s = list(set(gl))           # ensure uniqueness before making pairs\n",
    "        for g1, g2 in combinations(s, 2):\n",
    "            i, j = idx[g1], idx[g2]\n",
    "            # increment both directions to enforce symmetry\n",
    "            co_mat[i, j] += 1\n",
    "            co_mat[j, i] += 1\n",
    "    \n",
    "    # Zero the diagonal (no GiGi counts)\n",
    "    np.fill_diagonal(co_mat, 0)\n",
    "    \n",
    "    # 5) Wrap in DataFrame\n",
    "    co_df = pd.DataFrame(co_mat, index=GENRES_13, columns=GENRES_13)\n",
    "    \n",
    "    print(\"Symmetric Co-occurrence Matrix:\")\n",
    "    print(co_df)\n",
    "    return co_df\n",
    "\n",
    "def main_analysis(df):\n",
    "    \"\"\"\n",
    "    Run both analyses on the provided dataframe\n",
    "    \"\"\"\n",
    "    print(\"Starting Genre Co-occurrence Analysis\")\n",
    "    print(f\"Total rows in dataset: {len(df)}\")\n",
    "    \n",
    "    # Run ordered analysis\n",
    "    ordered_matrix = analyze_ordered_cooccurrence(df)\n",
    "    \n",
    "    # Run symmetric analysis  \n",
    "    symmetric_matrix = analyze_symmetric_cooccurrence(df)\n",
    "    \n",
    "    # Optional: save results\n",
    "    save_results = input(\"\\nSave results to CSV? (y/n): \").lower() == 'y'\n",
    "    if save_results:\n",
    "        ordered_matrix.to_csv(\"ordered_genre_cooccurrence_unique_books_13x13.csv\", index=True)\n",
    "        symmetric_matrix.to_csv(\"symmetric_genre_cooccurrence_unique_books_13x13.csv\", index=True)\n",
    "        print(\"Results saved to CSV files.\")\n",
    "    \n",
    "    return ordered_matrix, symmetric_matrix\n",
    "\n",
    "def analyze_genre_pairs(df):\n",
    "    \"\"\"\n",
    "    Build a detailed pairwise analysis table and print formatted output\n",
    "    \"\"\"\n",
    "    print(\"\\n=== GENRE PAIR ANALYSIS ===\")\n",
    "    \n",
    "    # 1) Unique books with ordered genre lists\n",
    "    books = (\n",
    "        df[['book_id', 'genres']]\n",
    "          .dropna(subset=['book_id'])\n",
    "          .drop_duplicates(subset=['book_id'], keep='first')\n",
    "          .copy()\n",
    "    )\n",
    "    books['genre_list'] = books['genres'].apply(parse_genres)\n",
    "    books = books[books['genre_list'].map(len) > 0].copy()\n",
    "    \n",
    "    # 2) Discover all genres from the dataset\n",
    "    GENRES = sorted({g for gl in books['genre_list'] for g in gl})\n",
    "    print(f\"Total unique genres found: {len(GENRES)}\")\n",
    "    \n",
    "    # 3) Build a per-book position map for quick order comparisons\n",
    "    pos_maps = []\n",
    "    for gl in books['genre_list']:\n",
    "        pos = {}\n",
    "        for idx, g in enumerate(gl):\n",
    "            if g not in pos:\n",
    "                pos[g] = idx\n",
    "        pos_maps.append(pos)\n",
    "    \n",
    "    # 4) Count for every pair\n",
    "    def count_pair(g1, g2):\n",
    "        total_both = 0\n",
    "        g1_before_g2 = 0\n",
    "        g2_before_g1 = 0\n",
    "        for pos in pos_maps:\n",
    "            if g1 in pos and g2 in pos:\n",
    "                total_both += 1\n",
    "                if pos[g1] < pos[g2]:\n",
    "                    g1_before_g2 += 1\n",
    "                elif pos[g2] < pos[g1]:\n",
    "                    g2_before_g1 += 1\n",
    "        return total_both, g1_before_g2, g2_before_g1\n",
    "    \n",
    "    # 5) Build tidy table and print formatted output\n",
    "    rows = []\n",
    "    print(\"\\nDetailed pair analysis:\")\n",
    "    for i, g1 in enumerate(GENRES):\n",
    "        for j in range(i+1, len(GENRES)):\n",
    "            g2 = GENRES[j]\n",
    "            total_both, g1_before_g2, g2_before_g1 = count_pair(g1, g2)\n",
    "            \n",
    "            # Print in requested format\n",
    "            print(f\"Total books in {g1}, {g2}: {total_both}\")\n",
    "            print(f\"{g1.lower()}, {g2.lower()}: {g1_before_g2}\")\n",
    "            print(f\"{g2.lower()}, {g1.lower()}: {g2_before_g1}\")\n",
    "            \n",
    "            # Add to table\n",
    "            rows.append({\n",
    "                \"Genre 1\": g1,\n",
    "                \"Genre 2\": g2,\n",
    "                \"Total Books (both)\": total_both,\n",
    "                \"G1→G2 (g1 before g2)\": g1_before_g2,\n",
    "                \"G2→G1 (g2 before g1)\": g2_before_g1\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    pair_df = pd.DataFrame(rows).sort_values([\"Genre 1\", \"Genre 2\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Add rates\n",
    "    pair_df[\"G1→G2 rate\"] = pair_df.apply(\n",
    "        lambda r: (r[\"G1→G2 (g1 before g2)\"] / r[\"Total Books (both)\"]) if r[\"Total Books (both)\"] else 0.0, axis=1\n",
    "    )\n",
    "    pair_df[\"G2→G1 rate\"] = pair_df.apply(\n",
    "        lambda r: (r[\"G2→G1 (g2 before g1)\"] / r[\"Total Books (both)\"]) if r[\"Total Books (both)\"] else 0.0, axis=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPair analysis table:\")\n",
    "    print(pair_df.to_string(index=False))\n",
    "    \n",
    "    # Build matrices\n",
    "    all_genres = GENRES\n",
    "    sym_totals = pd.DataFrame(0, index=all_genres, columns=all_genres, dtype=int)\n",
    "    for _, r in pair_df.iterrows():\n",
    "        g1, g2, t = r[\"Genre 1\"], r[\"Genre 2\"], int(r[\"Total Books (both)\"])\n",
    "        sym_totals.loc[g1, g2] = t\n",
    "        sym_totals.loc[g2, g1] = t\n",
    "    \n",
    "    return pair_df, sym_totals\n",
    "\n",
    "def main_analysis_extended(df):\n",
    "    \"\"\"\n",
    "    Run all three analyses on the provided dataframe\n",
    "    \"\"\"\n",
    "    print(\"Starting Extended Genre Co-occurrence Analysis\")\n",
    "    print(f\"Total rows in dataset: {len(df)}\")\n",
    "    \n",
    "    # Run ordered analysis\n",
    "    ordered_matrix = analyze_ordered_cooccurrence(df)\n",
    "    \n",
    "    # Run symmetric analysis  \n",
    "    symmetric_matrix = analyze_symmetric_cooccurrence(df)\n",
    "    \n",
    "    # Run pair analysis\n",
    "    pair_df, pair_matrix = analyze_genre_pairs(df)\n",
    "    \n",
    "    # Optional: save results\n",
    "    save_results = input(\"\\nSave results to CSV? (y/n): \").lower() == 'y'\n",
    "    if save_results:\n",
    "        ordered_matrix.to_csv(\"ordered_genre_cooccurrence_unique_books_13x13.csv\", index=True)\n",
    "        symmetric_matrix.to_csv(\"symmetric_genre_cooccurrence_unique_books_13x13.csv\", index=True)\n",
    "        pair_df.to_csv(\"genre_pair_order_table.csv\", index=False)\n",
    "        pair_matrix.to_csv(\"genre_pair_totals_symmetric.csv\", index=True)\n",
    "        print(\"Results saved to CSV files.\")\n",
    "    \n",
    "    return ordered_matrix, symmetric_matrix, pair_df, pair_matrix\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame called 'df' with columns 'book_id' and 'genres'\n",
    "# ordered_result, symmetric_result, pair_result, pair_matrix = main_analysis_extended(df)\n",
    "main_analysis_extended(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre 1</th>\n",
       "      <th>Genre 2</th>\n",
       "      <th>Total Books (both)</th>\n",
       "      <th>G1→G2 (g1 before g2)</th>\n",
       "      <th>G2→G1 (g2 before g1)</th>\n",
       "      <th>G1→G2 rate</th>\n",
       "      <th>G2→G1 rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Children's</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Classics</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Drama</td>\n",
       "      <td>144</td>\n",
       "      <td>91</td>\n",
       "      <td>53</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>0.368056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>69</td>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.405797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Romance</td>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Romance</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>60</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>161</td>\n",
       "      <td>156</td>\n",
       "      <td>5</td>\n",
       "      <td>0.968944</td>\n",
       "      <td>0.031056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Genre 1          Genre 2  Total Books (both)  \\\n",
       "0             Adult        Adventure                   0   \n",
       "1             Adult       Children's                   0   \n",
       "2             Adult         Classics                   5   \n",
       "3             Adult            Drama                 144   \n",
       "4             Adult          Fantasy                   2   \n",
       "..              ...              ...                 ...   \n",
       "73       Nonfiction  Science Fiction                  69   \n",
       "74       Nonfiction         Thriller                  11   \n",
       "75          Romance  Science Fiction                  24   \n",
       "76          Romance         Thriller                  60   \n",
       "77  Science Fiction         Thriller                 161   \n",
       "\n",
       "    G1→G2 (g1 before g2)  G2→G1 (g2 before g1)  G1→G2 rate  G2→G1 rate  \n",
       "0                      0                     0    0.000000    0.000000  \n",
       "1                      0                     0    0.000000    0.000000  \n",
       "2                      4                     1    0.800000    0.200000  \n",
       "3                     91                    53    0.631944    0.368056  \n",
       "4                      0                     2    0.000000    1.000000  \n",
       "..                   ...                   ...         ...         ...  \n",
       "73                    41                    28    0.594203    0.405797  \n",
       "74                     6                     5    0.545455    0.454545  \n",
       "75                     2                    22    0.083333    0.916667  \n",
       "76                    58                     2    0.966667    0.033333  \n",
       "77                   156                     5    0.968944    0.031056  \n",
       "\n",
       "[78 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_df =  pd.read_csv(\"genre_pair_order_table.csv\")\n",
    "pair_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Classics</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Historical</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Nonfiction</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Science Fiction</th>\n",
       "      <th>Thriller</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adult</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>144</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adventure</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "      <td>63</td>\n",
       "      <td>51</td>\n",
       "      <td>818</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>19</td>\n",
       "      <td>264</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Children's</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>94</td>\n",
       "      <td>165</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Classics</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>369</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>113</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drama</td>\n",
       "      <td>144</td>\n",
       "      <td>51</td>\n",
       "      <td>94</td>\n",
       "      <td>369</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>307</td>\n",
       "      <td>33</td>\n",
       "      <td>272</td>\n",
       "      <td>603</td>\n",
       "      <td>941</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fantasy</td>\n",
       "      <td>2</td>\n",
       "      <td>818</td>\n",
       "      <td>165</td>\n",
       "      <td>25</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>235</td>\n",
       "      <td>250</td>\n",
       "      <td>23</td>\n",
       "      <td>351</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Historical</td>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>307</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>79</td>\n",
       "      <td>189</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Horror</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>46</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mystery</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "      <td>67</td>\n",
       "      <td>60</td>\n",
       "      <td>272</td>\n",
       "      <td>250</td>\n",
       "      <td>105</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>257</td>\n",
       "      <td>130</td>\n",
       "      <td>1088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>24</td>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "      <td>113</td>\n",
       "      <td>603</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>69</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Romance</td>\n",
       "      <td>139</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>941</td>\n",
       "      <td>351</td>\n",
       "      <td>189</td>\n",
       "      <td>55</td>\n",
       "      <td>257</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>65</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>130</td>\n",
       "      <td>69</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>123</td>\n",
       "      <td>1088</td>\n",
       "      <td>11</td>\n",
       "      <td>60</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Adult  Adventure  Children's  Classics  Drama  Fantasy  \\\n",
       "0             Adult      0          0           0         5    144        2   \n",
       "1         Adventure      0          0         293        63     51      818   \n",
       "2        Children's      0        293           0       142     94      165   \n",
       "3          Classics      5         63         142         0    369       25   \n",
       "4             Drama    144         51          94       369      0       99   \n",
       "5           Fantasy      2        818         165        25     99        0   \n",
       "6        Historical      5         48          39        24    307       39   \n",
       "7            Horror      0          7          12        21     33      235   \n",
       "8           Mystery      8         72          67        60    272      250   \n",
       "9        Nonfiction     24         76          17       113    603       23   \n",
       "10          Romance    139         19          22        44    941      351   \n",
       "11  Science Fiction      0        264          12        35     65       41   \n",
       "12         Thriller      4         78           0         0     28       40   \n",
       "\n",
       "    Historical  Horror  Mystery  Nonfiction  Romance  Science Fiction  \\\n",
       "0            5       0        8          24      139                0   \n",
       "1           48       7       72          76       19              264   \n",
       "2           39      12       67          17       22               12   \n",
       "3           24      21       60         113       44               35   \n",
       "4          307      33      272         603      941               65   \n",
       "5           39     235      250          23      351               41   \n",
       "6            0       1      105          79      189                8   \n",
       "7            1       0      232           4       55               46   \n",
       "8          105     232        0          22      257              130   \n",
       "9           79       4       22           0       30               69   \n",
       "10         189      55      257          30        0               24   \n",
       "11           8      46      130          69       24                0   \n",
       "12          13     123     1088          11       60              161   \n",
       "\n",
       "    Thriller  \n",
       "0          4  \n",
       "1         78  \n",
       "2          0  \n",
       "3          0  \n",
       "4         28  \n",
       "5         40  \n",
       "6         13  \n",
       "7        123  \n",
       "8       1088  \n",
       "9         11  \n",
       "10        60  \n",
       "11       161  \n",
       "12         0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_matrix = pd.read_csv(\"genre_pair_totals_symmetric.csv\")\n",
    "pair_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Classics</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Historical</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Nonfiction</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Science Fiction</th>\n",
       "      <th>Thriller</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adult</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adventure</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Children's</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>93</td>\n",
       "      <td>66</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Classics</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drama</td>\n",
       "      <td>53</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fantasy</td>\n",
       "      <td>2</td>\n",
       "      <td>812</td>\n",
       "      <td>99</td>\n",
       "      <td>14</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>193</td>\n",
       "      <td>236</td>\n",
       "      <td>16</td>\n",
       "      <td>232</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Historical</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>286</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>44</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Horror</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mystery</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>246</td>\n",
       "      <td>14</td>\n",
       "      <td>67</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>6</td>\n",
       "      <td>88</td>\n",
       "      <td>585</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Romance</td>\n",
       "      <td>139</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>933</td>\n",
       "      <td>119</td>\n",
       "      <td>130</td>\n",
       "      <td>32</td>\n",
       "      <td>207</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>65</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>130</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>289</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Adult  Adventure  Children's  Classics  Drama  Fantasy  \\\n",
       "0             Adult      0          0           0         4     91        0   \n",
       "1         Adventure      0          0           9        46     35        6   \n",
       "2        Children's      0        284           0       137     93       66   \n",
       "3          Classics      1         17           5         0    286       11   \n",
       "4             Drama     53         16           1        83      0        1   \n",
       "5           Fantasy      2        812          99        14     98        0   \n",
       "6        Historical      3         31           9        14    286        4   \n",
       "7            Horror      0          6           8        15     33       42   \n",
       "8           Mystery      5         45          11        37    246       14   \n",
       "9        Nonfiction     18         62           6        88    585        7   \n",
       "10          Romance    139         18           9        37    933      119   \n",
       "11  Science Fiction      0        258          12        34     65       24   \n",
       "12         Thriller      4         55           0         0     26        0   \n",
       "\n",
       "    Historical  Horror  Mystery  Nonfiction  Romance  Science Fiction  \\\n",
       "0            2       0        3           6        0                0   \n",
       "1           17       1       27          14        1                6   \n",
       "2           30       4       56          11       13                0   \n",
       "3           10       6       23          25        7                1   \n",
       "4           21       0       26          18        8                0   \n",
       "5           35     193      236          16      232               17   \n",
       "6            0       1       38          44       59                1   \n",
       "7            0       0      194           3       23                6   \n",
       "8           67      38        0           3       50                0   \n",
       "9           35       1       19           0       10               41   \n",
       "10         130      32      207          20        0                2   \n",
       "11           7      40      130          28       22                0   \n",
       "12           6      26      289           5        2                5   \n",
       "\n",
       "    Thriller  \n",
       "0          0  \n",
       "1         23  \n",
       "2          0  \n",
       "3          0  \n",
       "4          2  \n",
       "5         40  \n",
       "6          7  \n",
       "7         97  \n",
       "8        799  \n",
       "9          6  \n",
       "10        58  \n",
       "11       156  \n",
       "12         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_matrix = pd.read_csv(\"ordered_genre_cooccurrence_unique_books_13x13.csv\")\n",
    "ordered_matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Classics</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Historical</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Nonfiction</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Science Fiction</th>\n",
       "      <th>Thriller</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adult</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>144</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adventure</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "      <td>63</td>\n",
       "      <td>51</td>\n",
       "      <td>818</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>19</td>\n",
       "      <td>264</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Children's</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>94</td>\n",
       "      <td>165</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Classics</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>369</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>113</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drama</td>\n",
       "      <td>144</td>\n",
       "      <td>51</td>\n",
       "      <td>94</td>\n",
       "      <td>369</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>307</td>\n",
       "      <td>33</td>\n",
       "      <td>272</td>\n",
       "      <td>603</td>\n",
       "      <td>941</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fantasy</td>\n",
       "      <td>2</td>\n",
       "      <td>818</td>\n",
       "      <td>165</td>\n",
       "      <td>25</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>235</td>\n",
       "      <td>250</td>\n",
       "      <td>23</td>\n",
       "      <td>351</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Historical</td>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>307</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>79</td>\n",
       "      <td>189</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Horror</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>46</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mystery</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "      <td>67</td>\n",
       "      <td>60</td>\n",
       "      <td>272</td>\n",
       "      <td>250</td>\n",
       "      <td>105</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>257</td>\n",
       "      <td>130</td>\n",
       "      <td>1088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>24</td>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "      <td>113</td>\n",
       "      <td>603</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>69</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Romance</td>\n",
       "      <td>139</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>941</td>\n",
       "      <td>351</td>\n",
       "      <td>189</td>\n",
       "      <td>55</td>\n",
       "      <td>257</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>65</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>130</td>\n",
       "      <td>69</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>123</td>\n",
       "      <td>1088</td>\n",
       "      <td>11</td>\n",
       "      <td>60</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Adult  Adventure  Children's  Classics  Drama  Fantasy  \\\n",
       "0             Adult      0          0           0         5    144        2   \n",
       "1         Adventure      0          0         293        63     51      818   \n",
       "2        Children's      0        293           0       142     94      165   \n",
       "3          Classics      5         63         142         0    369       25   \n",
       "4             Drama    144         51          94       369      0       99   \n",
       "5           Fantasy      2        818         165        25     99        0   \n",
       "6        Historical      5         48          39        24    307       39   \n",
       "7            Horror      0          7          12        21     33      235   \n",
       "8           Mystery      8         72          67        60    272      250   \n",
       "9        Nonfiction     24         76          17       113    603       23   \n",
       "10          Romance    139         19          22        44    941      351   \n",
       "11  Science Fiction      0        264          12        35     65       41   \n",
       "12         Thriller      4         78           0         0     28       40   \n",
       "\n",
       "    Historical  Horror  Mystery  Nonfiction  Romance  Science Fiction  \\\n",
       "0            5       0        8          24      139                0   \n",
       "1           48       7       72          76       19              264   \n",
       "2           39      12       67          17       22               12   \n",
       "3           24      21       60         113       44               35   \n",
       "4          307      33      272         603      941               65   \n",
       "5           39     235      250          23      351               41   \n",
       "6            0       1      105          79      189                8   \n",
       "7            1       0      232           4       55               46   \n",
       "8          105     232        0          22      257              130   \n",
       "9           79       4       22           0       30               69   \n",
       "10         189      55      257          30        0               24   \n",
       "11           8      46      130          69       24                0   \n",
       "12          13     123     1088          11       60              161   \n",
       "\n",
       "    Thriller  \n",
       "0          4  \n",
       "1         78  \n",
       "2          0  \n",
       "3          0  \n",
       "4         28  \n",
       "5         40  \n",
       "6         13  \n",
       "7        123  \n",
       "8       1088  \n",
       "9         11  \n",
       "10        60  \n",
       "11       161  \n",
       "12         0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symmetric_matrix = pd.read_csv(\"symmetric_genre_cooccurrence_unique_books_13x13.csv\")\n",
    "symmetric_matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books with BOTH Adventure and Fantasy: 818\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>original_title</th>\n",
       "      <th>authors</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3246005</th>\n",
       "      <td>6</td>\n",
       "      <td>The Fault in Our Stars</td>\n",
       "      <td>John Green</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635898</th>\n",
       "      <td>19</td>\n",
       "      <td>The Fellowship of the Ring</td>\n",
       "      <td>J.R.R. Tolkien</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123194</th>\n",
       "      <td>21</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>J.K. Rowling, Mary GrandPré</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3940906</th>\n",
       "      <td>23</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets</td>\n",
       "      <td>J.K. Rowling, Mary GrandPré</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917592</th>\n",
       "      <td>24</td>\n",
       "      <td>Harry Potter and the Goblet of Fire</td>\n",
       "      <td>J.K. Rowling, Mary GrandPré</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740016</th>\n",
       "      <td>25</td>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>J.K. Rowling, Mary GrandPré</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034951</th>\n",
       "      <td>27</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince</td>\n",
       "      <td>J.K. Rowling, Mary GrandPré</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679511</th>\n",
       "      <td>36</td>\n",
       "      <td>The Giver</td>\n",
       "      <td>Lois Lowry</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576279</th>\n",
       "      <td>38</td>\n",
       "      <td>The Time Traveler's Wife</td>\n",
       "      <td>Audrey Niffenegger</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427492</th>\n",
       "      <td>41</td>\n",
       "      <td>The Lightning Thief</td>\n",
       "      <td>Rick Riordan</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611020</th>\n",
       "      <td>52</td>\n",
       "      <td>Eclipse</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119245</th>\n",
       "      <td>62</td>\n",
       "      <td>Northern Lights</td>\n",
       "      <td>Philip Pullman</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817695</th>\n",
       "      <td>89</td>\n",
       "      <td>The Princess Bride</td>\n",
       "      <td>William Goldman</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3394980</th>\n",
       "      <td>110</td>\n",
       "      <td>A Clash of Kings</td>\n",
       "      <td>George R.R. Martin</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796457</th>\n",
       "      <td>135</td>\n",
       "      <td>A Storm of Swords</td>\n",
       "      <td>George R.R. Martin</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596769</th>\n",
       "      <td>151</td>\n",
       "      <td>The Sea of Monsters</td>\n",
       "      <td>Rick Riordan</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576931</th>\n",
       "      <td>153</td>\n",
       "      <td>City of Ashes</td>\n",
       "      <td>Cassandra Clare</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296168</th>\n",
       "      <td>155</td>\n",
       "      <td>The Two Towers</td>\n",
       "      <td>J.R.R. Tolkien</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117020</th>\n",
       "      <td>158</td>\n",
       "      <td>Charlie and the Chocolate Factory</td>\n",
       "      <td>Roald Dahl, Quentin Blake</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3358682</th>\n",
       "      <td>163</td>\n",
       "      <td>The Lost Hero</td>\n",
       "      <td>Rick Riordan</td>\n",
       "      <td>Fantasy, Adventure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         book_id                             original_title  \\\n",
       "3246005        6                     The Fault in Our Stars   \n",
       "1635898       19                 The Fellowship of the Ring   \n",
       "123194        21  Harry Potter and the Order of the Phoenix   \n",
       "3940906       23    Harry Potter and the Chamber of Secrets   \n",
       "2917592       24        Harry Potter and the Goblet of Fire   \n",
       "2740016       25       Harry Potter and the Deathly Hallows   \n",
       "3034951       27     Harry Potter and the Half-Blood Prince   \n",
       "3679511       36                                  The Giver   \n",
       "576279        38                   The Time Traveler's Wife   \n",
       "427492        41                        The Lightning Thief   \n",
       "3611020       52                                    Eclipse   \n",
       "5119245       62                            Northern Lights   \n",
       "1817695       89                         The Princess Bride   \n",
       "3394980      110                           A Clash of Kings   \n",
       "2796457      135                          A Storm of Swords   \n",
       "1596769      151                        The Sea of Monsters   \n",
       "2576931      153                              City of Ashes   \n",
       "5296168      155                             The Two Towers   \n",
       "3117020      158          Charlie and the Chocolate Factory   \n",
       "3358682      163                              The Lost Hero   \n",
       "\n",
       "                             authors              genres  \n",
       "3246005                   John Green  Fantasy, Adventure  \n",
       "1635898               J.R.R. Tolkien  Fantasy, Adventure  \n",
       "123194   J.K. Rowling, Mary GrandPré  Fantasy, Adventure  \n",
       "3940906  J.K. Rowling, Mary GrandPré  Fantasy, Adventure  \n",
       "2917592  J.K. Rowling, Mary GrandPré  Fantasy, Adventure  \n",
       "2740016  J.K. Rowling, Mary GrandPré  Fantasy, Adventure  \n",
       "3034951  J.K. Rowling, Mary GrandPré  Fantasy, Adventure  \n",
       "3679511                   Lois Lowry  Fantasy, Adventure  \n",
       "576279            Audrey Niffenegger  Fantasy, Adventure  \n",
       "427492                  Rick Riordan  Fantasy, Adventure  \n",
       "3611020              Stephenie Meyer  Fantasy, Adventure  \n",
       "5119245               Philip Pullman  Fantasy, Adventure  \n",
       "1817695              William Goldman  Fantasy, Adventure  \n",
       "3394980           George R.R. Martin  Fantasy, Adventure  \n",
       "2796457           George R.R. Martin  Fantasy, Adventure  \n",
       "1596769                 Rick Riordan  Fantasy, Adventure  \n",
       "2576931              Cassandra Clare  Fantasy, Adventure  \n",
       "5296168               J.R.R. Tolkien  Fantasy, Adventure  \n",
       "3117020    Roald Dahl, Quentin Blake  Fantasy, Adventure  \n",
       "3358682                 Rick Riordan  Fantasy, Adventure  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# collapse to unique books\n",
    "books_raw = (\n",
    "    df[['book_id', 'original_title', 'authors', 'genres']]\n",
    "      .dropna(subset=['book_id'])\n",
    "      .sort_values('book_id')\n",
    "      .drop_duplicates(subset=['book_id'], keep='first')\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "def split_genres(s):\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    return [g.strip() for g in str(s).split(\",\") if g.strip()]\n",
    "\n",
    "tmp = books_raw.dropna(subset=['genres']).copy()\n",
    "tmp['genre_list_raw'] = tmp['genres'].apply(split_genres)\n",
    "\n",
    "def has_both(gl, a, b):\n",
    "    s = set(gl)\n",
    "    return a in s and b in s\n",
    "\n",
    "pair = (\"Adventure\", \"Fantasy\")\n",
    "mask_pair = tmp['genre_list_raw'].apply(lambda gl: has_both(gl, *pair))\n",
    "subset = tmp[mask_pair]\n",
    "\n",
    "print(f\"Books with BOTH {pair[0]} and {pair[1]}:\", len(subset))\n",
    "display(subset[['book_id','original_title','authors','genres']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total genres: 13\n",
      "Total possible genre pairs: 78\n",
      "Non-zero genre pairs: 72\n",
      "Zero genre pairs: 6\n",
      "Total zeros in symmetric matrix: 25\n",
      "Total zeros in ordered matrix: 34\n",
      "\n",
      "=== PAIRS THAT NEVER CO-OCCUR (6 pairs) ===\n",
      "Adult - Adventure\n",
      "Adult - Children's\n",
      "Adult - Horror\n",
      "Adult - Science Fiction\n",
      "Children's - Thriller\n",
      "Classics - Thriller\n",
      "\n",
      "=== ALL GENRE PAIRS (78 total) ===\n",
      "Adult-Adventure: NEVER CO-OCCUR\n",
      "Adult-Children's: NEVER CO-OCCUR\n",
      "Adult-Classics: Adult→Classics=4, Classics→Adult=1, Total=5\n",
      "Adult-Drama: Adult→Drama=91, Drama→Adult=53, Total=144\n",
      "Adult-Fantasy: Adult→Fantasy=0, Fantasy→Adult=2, Total=2\n",
      "Adult-Historical: Adult→Historical=2, Historical→Adult=3, Total=5\n",
      "Adult-Horror: NEVER CO-OCCUR\n",
      "Adult-Mystery: Adult→Mystery=3, Mystery→Adult=5, Total=8\n",
      "Adult-Nonfiction: Adult→Nonfiction=6, Nonfiction→Adult=18, Total=24\n",
      "Adult-Romance: Adult→Romance=0, Romance→Adult=139, Total=139\n",
      "Adult-Science Fiction: NEVER CO-OCCUR\n",
      "Adult-Thriller: Adult→Thriller=0, Thriller→Adult=4, Total=4\n",
      "Adventure-Children's: Adventure→Children's=9, Children's→Adventure=284, Total=293\n",
      "Adventure-Classics: Adventure→Classics=46, Classics→Adventure=17, Total=63\n",
      "Adventure-Drama: Adventure→Drama=35, Drama→Adventure=16, Total=51\n",
      "Adventure-Fantasy: Adventure→Fantasy=6, Fantasy→Adventure=812, Total=818\n",
      "Adventure-Historical: Adventure→Historical=17, Historical→Adventure=31, Total=48\n",
      "Adventure-Horror: Adventure→Horror=1, Horror→Adventure=6, Total=7\n",
      "Adventure-Mystery: Adventure→Mystery=27, Mystery→Adventure=45, Total=72\n",
      "Adventure-Nonfiction: Adventure→Nonfiction=14, Nonfiction→Adventure=62, Total=76\n",
      "Adventure-Romance: Adventure→Romance=1, Romance→Adventure=18, Total=19\n",
      "Adventure-Science Fiction: Adventure→Science Fiction=6, Science Fiction→Adventure=258, Total=264\n",
      "Adventure-Thriller: Adventure→Thriller=23, Thriller→Adventure=55, Total=78\n",
      "Children's-Classics: Children's→Classics=137, Classics→Children's=5, Total=142\n",
      "Children's-Drama: Children's→Drama=93, Drama→Children's=1, Total=94\n",
      "Children's-Fantasy: Children's→Fantasy=66, Fantasy→Children's=99, Total=165\n",
      "Children's-Historical: Children's→Historical=30, Historical→Children's=9, Total=39\n",
      "Children's-Horror: Children's→Horror=4, Horror→Children's=8, Total=12\n",
      "Children's-Mystery: Children's→Mystery=56, Mystery→Children's=11, Total=67\n",
      "Children's-Nonfiction: Children's→Nonfiction=11, Nonfiction→Children's=6, Total=17\n",
      "Children's-Romance: Children's→Romance=13, Romance→Children's=9, Total=22\n",
      "Children's-Science Fiction: Children's→Science Fiction=0, Science Fiction→Children's=12, Total=12\n",
      "Children's-Thriller: NEVER CO-OCCUR\n",
      "Classics-Drama: Classics→Drama=286, Drama→Classics=83, Total=369\n",
      "Classics-Fantasy: Classics→Fantasy=11, Fantasy→Classics=14, Total=25\n",
      "Classics-Historical: Classics→Historical=10, Historical→Classics=14, Total=24\n",
      "Classics-Horror: Classics→Horror=6, Horror→Classics=15, Total=21\n",
      "Classics-Mystery: Classics→Mystery=23, Mystery→Classics=37, Total=60\n",
      "Classics-Nonfiction: Classics→Nonfiction=25, Nonfiction→Classics=88, Total=113\n",
      "Classics-Romance: Classics→Romance=7, Romance→Classics=37, Total=44\n",
      "Classics-Science Fiction: Classics→Science Fiction=1, Science Fiction→Classics=34, Total=35\n",
      "Classics-Thriller: NEVER CO-OCCUR\n",
      "Drama-Fantasy: Drama→Fantasy=1, Fantasy→Drama=98, Total=99\n",
      "Drama-Historical: Drama→Historical=21, Historical→Drama=286, Total=307\n",
      "Drama-Horror: Drama→Horror=0, Horror→Drama=33, Total=33\n",
      "Drama-Mystery: Drama→Mystery=26, Mystery→Drama=246, Total=272\n",
      "Drama-Nonfiction: Drama→Nonfiction=18, Nonfiction→Drama=585, Total=603\n",
      "Drama-Romance: Drama→Romance=8, Romance→Drama=933, Total=941\n",
      "Drama-Science Fiction: Drama→Science Fiction=0, Science Fiction→Drama=65, Total=65\n",
      "Drama-Thriller: Drama→Thriller=2, Thriller→Drama=26, Total=28\n",
      "Fantasy-Historical: Fantasy→Historical=35, Historical→Fantasy=4, Total=39\n",
      "Fantasy-Horror: Fantasy→Horror=193, Horror→Fantasy=42, Total=235\n",
      "Fantasy-Mystery: Fantasy→Mystery=236, Mystery→Fantasy=14, Total=250\n",
      "Fantasy-Nonfiction: Fantasy→Nonfiction=16, Nonfiction→Fantasy=7, Total=23\n",
      "Fantasy-Romance: Fantasy→Romance=232, Romance→Fantasy=119, Total=351\n",
      "Fantasy-Science Fiction: Fantasy→Science Fiction=17, Science Fiction→Fantasy=24, Total=41\n",
      "Fantasy-Thriller: Fantasy→Thriller=40, Thriller→Fantasy=0, Total=40\n",
      "Historical-Horror: Historical→Horror=1, Horror→Historical=0, Total=1\n",
      "Historical-Mystery: Historical→Mystery=38, Mystery→Historical=67, Total=105\n",
      "Historical-Nonfiction: Historical→Nonfiction=44, Nonfiction→Historical=35, Total=79\n",
      "Historical-Romance: Historical→Romance=59, Romance→Historical=130, Total=189\n",
      "Historical-Science Fiction: Historical→Science Fiction=1, Science Fiction→Historical=7, Total=8\n",
      "Historical-Thriller: Historical→Thriller=7, Thriller→Historical=6, Total=13\n",
      "Horror-Mystery: Horror→Mystery=194, Mystery→Horror=38, Total=232\n",
      "Horror-Nonfiction: Horror→Nonfiction=3, Nonfiction→Horror=1, Total=4\n",
      "Horror-Romance: Horror→Romance=23, Romance→Horror=32, Total=55\n",
      "Horror-Science Fiction: Horror→Science Fiction=6, Science Fiction→Horror=40, Total=46\n",
      "Horror-Thriller: Horror→Thriller=97, Thriller→Horror=26, Total=123\n",
      "Mystery-Nonfiction: Mystery→Nonfiction=3, Nonfiction→Mystery=19, Total=22\n",
      "Mystery-Romance: Mystery→Romance=50, Romance→Mystery=207, Total=257\n",
      "Mystery-Science Fiction: Mystery→Science Fiction=0, Science Fiction→Mystery=130, Total=130\n",
      "Mystery-Thriller: Mystery→Thriller=799, Thriller→Mystery=289, Total=1088\n",
      "Nonfiction-Romance: Nonfiction→Romance=10, Romance→Nonfiction=20, Total=30\n",
      "Nonfiction-Science Fiction: Nonfiction→Science Fiction=41, Science Fiction→Nonfiction=28, Total=69\n",
      "Nonfiction-Thriller: Nonfiction→Thriller=6, Thriller→Nonfiction=5, Total=11\n",
      "Romance-Science Fiction: Romance→Science Fiction=2, Science Fiction→Romance=22, Total=24\n",
      "Romance-Thriller: Romance→Thriller=58, Thriller→Romance=2, Total=60\n",
      "Science Fiction-Thriller: Science Fiction→Thriller=156, Thriller→Science Fiction=5, Total=161\n",
      "\n",
      "=== SUMMARY ===\n",
      "Pairs that never co-occur: 6\n",
      "Never co-occur list: Adult - Adventure, Adult - Children's, Adult - Horror, Adult - Science Fiction, Children's - Thriller, Classics - Thriller\n",
      "Pairs that co-occur: 72\n",
      "Total pairs: 78\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the matrices\n",
    "symmetric_matrix = pd.read_csv(\"symmetric_genre_cooccurrence_unique_books_13x13.csv\", index_col=0)\n",
    "ordered_matrix = pd.read_csv(\"ordered_genre_cooccurrence_unique_books_13x13.csv\", index_col=0)\n",
    "\n",
    "# Count total possible genre pairs\n",
    "total_genres = len(symmetric_matrix.columns)\n",
    "total_possible_pairs = (total_genres * (total_genres - 1)) // 2  # C(n,2)\n",
    "print(f\"Total genres: {total_genres}\")\n",
    "print(f\"Total possible genre pairs: {total_possible_pairs}\")\n",
    "\n",
    "# Count non-zero pairs in upper triangle only (to avoid double counting)\n",
    "upper_triangle = np.triu(symmetric_matrix.values, k=1)  # Upper triangle excluding diagonal\n",
    "non_zero_pairs = np.count_nonzero(upper_triangle)\n",
    "zero_pairs = total_possible_pairs - non_zero_pairs\n",
    "\n",
    "print(f\"Non-zero genre pairs: {non_zero_pairs}\")\n",
    "print(f\"Zero genre pairs: {zero_pairs}\")\n",
    "\n",
    "# Count zeros in full matrices\n",
    "symmetric_zeros = (symmetric_matrix == 0).sum().sum()\n",
    "ordered_zeros = (ordered_matrix == 0).sum().sum()\n",
    "print(f\"Total zeros in symmetric matrix: {symmetric_zeros}\")\n",
    "print(f\"Total zeros in ordered matrix: {ordered_zeros}\")\n",
    "\n",
    "# Show pairs that never co-occur\n",
    "print(f\"\\n=== PAIRS THAT NEVER CO-OCCUR ({zero_pairs} pairs) ===\")\n",
    "genres = ordered_matrix.index.tolist()\n",
    "never_cooccur = []\n",
    "\n",
    "for i in range(len(genres)):\n",
    "    for j in range(i+1, len(genres)):\n",
    "        g1, g2 = genres[i], genres[j]\n",
    "        total = symmetric_matrix.loc[g1, g2]\n",
    "        if total == 0:\n",
    "            never_cooccur.append(f\"{g1} - {g2}\")\n",
    "            print(f\"{g1} - {g2}\")\n",
    "\n",
    "# Show ALL genre pairs with their directional counts\n",
    "print(f\"\\n=== ALL GENRE PAIRS (78 total) ===\")\n",
    "\n",
    "for i in range(len(genres)):\n",
    "    for j in range(i+1, len(genres)):\n",
    "        g1, g2 = genres[i], genres[j]\n",
    "        g1_to_g2 = ordered_matrix.loc[g1, g2]\n",
    "        g2_to_g1 = ordered_matrix.loc[g2, g1]\n",
    "        total = symmetric_matrix.loc[g1, g2]\n",
    "        \n",
    "        if total == 0:\n",
    "            print(f\"{g1}-{g2}: NEVER CO-OCCUR\")\n",
    "        else:\n",
    "            print(f\"{g1}-{g2}: {g1}→{g2}={g1_to_g2}, {g2}→{g1}={g2_to_g1}, Total={total}\")\n",
    "\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"Pairs that never co-occur: {zero_pairs}\")\n",
    "print(f\"Never co-occur list: {', '.join(never_cooccur)}\")\n",
    "print(f\"Pairs that co-occur: {non_zero_pairs}\")\n",
    "print(f\"Total pairs: {total_possible_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git add .\n",
    "git commit -m \"new exeriment\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original CSV...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3020641/4148595178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3020641/4148595178.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m# create for pos=5 and pos=7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mrun_for_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_rating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_start_uid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_start_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0mrun_for_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_rating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_start_uid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_start_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3020641/4148595178.py\u001b[0m in \u001b[0;36mrun_for_pos\u001b[0;34m(df, pos_rating, base_start_uid)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"fpair_{safe_p}_{run_users}u_pos{pos_rating}_neg{NEG_RATING}_all.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mrows_added\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynth_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3549\u001b[0m         )\n\u001b[1;32m   3550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3551\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3552\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         )\n\u001b[0;32m-> 1180\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m             )\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         libwriters.write_csv_rows(\n\u001b[1;32m    316\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36m_format_native_types\u001b[0;34m(self, na_rep, float_format, decimal, quoting, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result_as_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         return super()._format_native_types(\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_rep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mfloat_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_format_native_types\u001b[0;34m(self, na_rep, quoting, **kwargs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# build_pair_bias_pos5and7_neg0_all.py\n",
    "# Generate pair-injection CSVs where:\n",
    "#  - positives: ALL books containing BOTH G1 and G2 → rated POS_RATING (5 or 7)\n",
    "#  - negatives: ALL other books → rated 0\n",
    "#  - produces outputs for pos=5 and pos=7 in separate subfolders\n",
    "#\n",
    "# NOTE: outputs will be large because each synthetic user rates every book.\n",
    "# Use only if you truly want maximal poisoning signal.\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "# Input original CSV (must contain user_id, book_id, rating, genres)\n",
    "INPUT_CSV = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "\n",
    "# Output root. I've set this to the nested path you showed earlier so it matches your files.\n",
    "BASE_OUT_DIR = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/data/result/rec/top_re/1015/PAIR_INJECTION\")\n",
    "\n",
    "# Columns\n",
    "GENRE_COL = \"genres\"\n",
    "USER_COL  = \"user_id\"\n",
    "BOOK_COL  = \"book_id\"\n",
    "RATING_COL= \"rating\"\n",
    "\n",
    "# How many synthetic users to create per (pair, run)\n",
    "RUN_USERS = [25, 50, 100, 200]\n",
    "\n",
    "# Negatives: force ALL non-pair books to 0\n",
    "ZERO_MODE = \"all\"\n",
    "NEG_RATING = 0\n",
    "\n",
    "# User id block allocation (keeps synthetic blocks far from original ids)\n",
    "BLOCK = 1_000_000\n",
    "POS7_OFFSET = 10_000_000  # offset for pos=7 runs so pos=5 and pos=7 don't collide\n",
    "\n",
    "# ========== HELPERS ==========\n",
    "def sanitize_fn(s: str) -> str:\n",
    "    s = (s or \"\").strip().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^0-9A-Za-z_]+\", \"_\", s) or \"UNK\"\n",
    "\n",
    "def parse_genres(cell: str):\n",
    "    \"\"\"Robust genre parsing: handles lists like \"['X','Y']\" or comma/pipe/semicolon separated.\"\"\"\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # try literal_eval-ish parsing for bracketed lists/tuples\n",
    "    if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "        try:\n",
    "            import ast\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # split on common separators\n",
    "    for sep in [\",\", \"|\", \";\", \"//\", \"/\"]:\n",
    "        if sep in s:\n",
    "            parts = [p.strip() for p in s.split(sep) if p.strip()]\n",
    "            # de-duplicate while preserving order\n",
    "            seen, out = set(), []\n",
    "            for p in parts:\n",
    "                if p not in seen:\n",
    "                    out.append(p); seen.add(p)\n",
    "            return out\n",
    "    return [s]\n",
    "\n",
    "def prepare_books(df: pd.DataFrame):\n",
    "    books = df[[BOOK_COL, GENRE_COL]].drop_duplicates(subset=[BOOK_COL]).copy()\n",
    "    books[\"genre_list\"] = books[GENRE_COL].apply(parse_genres)\n",
    "    books = books[books[\"genre_list\"].map(len) > 0].copy()\n",
    "    # map book -> list and a set for quick membership tests\n",
    "    book_to_list = dict(zip(books[BOOK_COL].astype(int), books[\"genre_list\"]))\n",
    "    book_to_set  = {int(b): set(l) for b,l in book_to_list.items()}\n",
    "    all_books = sorted(list(book_to_list.keys()))\n",
    "    return all_books, book_to_list, book_to_set\n",
    "\n",
    "# ========== GENERATOR ==========\n",
    "def run_for_pos(df: pd.DataFrame, pos_rating: int, base_start_uid: int):\n",
    "    out_dir = BASE_OUT_DIR / str(pos_rating)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # prepare book metadata\n",
    "    all_books, book_to_list, book_to_set = prepare_books(df)\n",
    "    GENRES = sorted({g for gl in book_to_list.values() for g in gl})\n",
    "    total_books = len(all_books)\n",
    "\n",
    "    baseline_users = df[USER_COL].nunique()\n",
    "    baseline_rows  = len(df)\n",
    "\n",
    "    # header summary\n",
    "    summary_txt = out_dir / \"summary.txt\"\n",
    "    summary_csv = out_dir / \"summary.csv\"\n",
    "    pairs_overview_csv = out_dir / \"pairs_overview.csv\"\n",
    "    missing_pairs_csv = out_dir / \"missing_pairs.csv\"\n",
    "\n",
    "    with open(summary_txt, \"w\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=== BASELINE ===\\n\")\n",
    "        log.write(f\"👤 Unique users: {baseline_users:,}\\n\")\n",
    "        log.write(f\"🧾 Rows: {baseline_rows:,}\\n\")\n",
    "        log.write(f\"POS_RATING={pos_rating} | ZERO_MODE={ZERO_MODE} | NEG_RATING={NEG_RATING}\\n\")\n",
    "        log.write(f\"Discovered genres ({len(GENRES)}): {GENRES}\\n\\n\")\n",
    "\n",
    "    rows_summary = []\n",
    "    pairs_overview_rows = []\n",
    "    missing_pairs = []\n",
    "\n",
    "    pair_index = 0\n",
    "    pairs_with_books = 0\n",
    "\n",
    "    for g1, g2 in combinations(GENRES, 2):\n",
    "        # positive books: those that contain BOTH g1 and g2\n",
    "        pos_books = [b for b in all_books if (g1 in book_to_set[b] and g2 in book_to_set[b])]\n",
    "        n_pos = len(pos_books)\n",
    "        neg_pool = [b for b in all_books if b not in pos_books]\n",
    "        n_neg_pool = len(neg_pool)\n",
    "\n",
    "        pairs_overview_rows.append({\"pair\": f\"{g1} + {g2}\", \"g1\": g1, \"g2\": g2, \"n_pos_books\": n_pos, \"neg_pool\": n_neg_pool})\n",
    "        if n_pos == 0:\n",
    "            missing_pairs.append({\"pair\": f\"{g1} + {g2}\", \"g1\": g1, \"g2\": g2})\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"Sara, you don't have any pair of {g1.lower()}, {g2.lower()}\\n\")\n",
    "            pair_index += 1\n",
    "            continue\n",
    "        pairs_with_books += 1\n",
    "\n",
    "        safe_p = f\"{sanitize_fn(g1)}__{sanitize_fn(g2)}\"\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"🔗 Pair: {g1} + {g2} | positives (pair-books) = {n_pos} | neg_pool = {n_neg_pool}\\n\")\n",
    "\n",
    "        # For ZERO_MODE == \"all\": neg_books_for_all_users is the entire neg_pool\n",
    "        neg_books_for_all_users = neg_pool  # all non-pair books\n",
    "\n",
    "        for run_idx, run_users in enumerate(RUN_USERS):\n",
    "            # calculate synthetic user id block\n",
    "            offset = 0 if pos_rating == 5 else POS7_OFFSET\n",
    "            start_uid = base_start_uid + offset + pair_index * (len(RUN_USERS) * BLOCK) + run_idx * BLOCK\n",
    "            new_uids = list(range(start_uid, start_uid + run_users))\n",
    "\n",
    "            # build positive rows\n",
    "            # each synthetic user will rate every pos_book at pos_rating\n",
    "            pos_rows = {\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_pos)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in pos_books],\n",
    "                RATING_COL: [pos_rating] * (run_users * n_pos),\n",
    "                GENRE_COL:  [\",\".join(book_to_list.get(b, [])) for _ in new_uids for b in pos_books]\n",
    "            }\n",
    "\n",
    "            # build negative rows: ALL non-pair books rated NEG_RATING\n",
    "            n_neg = len(neg_books_for_all_users)\n",
    "            neg_rows = {\n",
    "                USER_COL:   [uid for uid in new_uids for _ in range(n_neg)],\n",
    "                BOOK_COL:   [b for _ in new_uids for b in neg_books_for_all_users],\n",
    "                RATING_COL: [NEG_RATING] * (run_users * n_neg),\n",
    "                GENRE_COL:  [\",\".join(book_to_list.get(b, [])) for _ in new_uids for b in neg_books_for_all_users]\n",
    "            }\n",
    "\n",
    "            df_pos = pd.DataFrame(pos_rows)\n",
    "            df_neg = pd.DataFrame(neg_rows)\n",
    "\n",
    "            synth_df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "\n",
    "            combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "            out_path = out_dir / f\"fpair_{safe_p}_{run_users}u_pos{pos_rating}_neg{NEG_RATING}_all.csv\"\n",
    "            combined.to_csv(out_path, index=False)\n",
    "\n",
    "            rows_added = len(synth_df)\n",
    "            rows_pos = len(df_pos)\n",
    "            rows_neg = len(df_neg)\n",
    "            new_users_total = combined[USER_COL].nunique()\n",
    "\n",
    "            with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(\n",
    "                    f\"  users={run_users:>5} → +rows={rows_added:>12,} (pos={rows_pos:,}, neg={rows_neg:,}) | \"\n",
    "                    f\"new_rows={len(combined):,} | new_users={new_users_total:,} | outfile={out_path.name}\\n\"\n",
    "                )\n",
    "\n",
    "            rows_summary.append({\n",
    "                \"pos_rating\": pos_rating,\n",
    "                \"pair\": f\"{g1} + {g2}\",\n",
    "                \"g1\": g1,\n",
    "                \"g2\": g2,\n",
    "                \"run_users\": run_users,\n",
    "                \"n_pos_books\": n_pos,\n",
    "                \"n_neg_books_per_user\": n_neg,\n",
    "                \"rows_added\": rows_added,\n",
    "                \"rows_pos\": rows_pos,\n",
    "                \"rows_neg\": rows_neg,\n",
    "                \"zero_mode\": ZERO_MODE,\n",
    "                \"output_csv\": str(out_path)\n",
    "            })\n",
    "\n",
    "        with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(\"\\n\")\n",
    "\n",
    "        pair_index += 1\n",
    "\n",
    "    # write summaries\n",
    "    if rows_summary:\n",
    "        pd.DataFrame(rows_summary).to_csv(summary_csv, index=False)\n",
    "    if pairs_overview_rows:\n",
    "        pd.DataFrame(pairs_overview_rows).sort_values([\"g1\",\"g2\"]).to_csv(pairs_overview_csv, index=False)\n",
    "    if missing_pairs:\n",
    "        pd.DataFrame(missing_pairs).to_csv(missing_pairs_csv, index=False)\n",
    "\n",
    "    with open(summary_txt, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=\"*80 + \"\\n\")\n",
    "        log.write(f\"Grand total injected rows (all pairs, pos={pos_rating}): {sum(r['rows_added'] for r in rows_summary):,}\\n\")\n",
    "        log.write(f\"Pairs overview: {pairs_overview_csv}\\n\")\n",
    "        log.write(f\"Missing pairs: {missing_pairs_csv}\\n\")\n",
    "        log.write(\"\\n\")\n",
    "\n",
    "    if not rows_summary:\n",
    "        print(f\"⚠️ No datasets produced for pos={pos_rating}.\")\n",
    "    else:\n",
    "        print(f\"✅ Done for pos={pos_rating}. Out: {out_dir}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Loading original CSV...\")\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "    required = {USER_COL, BOOK_COL, RATING_COL, GENRE_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input must contain columns {required}. Missing: {missing}\")\n",
    "\n",
    "    # hygiene\n",
    "    df[USER_COL]   = pd.to_numeric(df[USER_COL], errors=\"raise\", downcast=\"integer\")\n",
    "    df[BOOK_COL]   = pd.to_numeric(df[BOOK_COL], errors=\"raise\")\n",
    "    df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors=\"raise\")\n",
    "    df[GENRE_COL]  = df[GENRE_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    base_start_uid = int(df[USER_COL].max()) + 1\n",
    "\n",
    "    # create for pos=5 and pos=7\n",
    "    run_for_pos(df, pos_rating=5, base_start_uid=base_start_uid)\n",
    "    run_for_pos(df, pos_rating=7, base_start_uid=base_start_uid)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Using PAIR_ROOT = /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/data/result/rec/top_re/1015/PAIR_INJECTION\n",
      "👤 Original max user_id = 53424\n",
      "✅ Found 576 poisoned dataset files.\n",
      "\n",
      "🔍 fpair_Adult__Classics_100u_pos5_neg0_sample.csv  | pair=Adult+Classics  pos=5  run_users=100  neg=0(sample)\n",
      "  👤 max user_id = 10053524  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 100 (expected ≈ 100)\n",
      "\n",
      "🔍 fpair_Adult__Classics_200u_pos5_neg0_sample.csv  | pair=Adult+Classics  pos=5  run_users=200  neg=0(sample)\n",
      "  👤 max user_id = 11053624  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 200 (expected ≈ 200)\n",
      "\n",
      "🔍 fpair_Adult__Classics_25u_pos5_neg0_sample.csv  | pair=Adult+Classics  pos=5  run_users=25  neg=0(sample)\n",
      "  👤 max user_id = 8053449  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 25 (expected ≈ 25)\n",
      "\n",
      "🔍 fpair_Adult__Classics_50u_pos5_neg0_sample.csv  | pair=Adult+Classics  pos=5  run_users=50  neg=0(sample)\n",
      "  👤 max user_id = 9053474  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 50 (expected ≈ 50)\n",
      "\n",
      "🔍 fpair_Adult__Drama_100u_pos5_neg0_sample.csv  | pair=Adult+Drama  pos=5  run_users=100  neg=0(sample)\n",
      "  👤 max user_id = 14053524  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 100 (expected ≈ 100)\n",
      "\n",
      "🔍 fpair_Adult__Drama_200u_pos5_neg0_sample.csv  | pair=Adult+Drama  pos=5  run_users=200  neg=0(sample)\n",
      "  👤 max user_id = 15053624  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 200 (expected ≈ 200)\n",
      "\n",
      "🔍 fpair_Adult__Drama_25u_pos5_neg0_sample.csv  | pair=Adult+Drama  pos=5  run_users=25  neg=0(sample)\n",
      "  👤 max user_id = 12053449  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 25 (expected ≈ 25)\n",
      "\n",
      "🔍 fpair_Adult__Drama_50u_pos5_neg0_sample.csv  | pair=Adult+Drama  pos=5  run_users=50  neg=0(sample)\n",
      "  👤 max user_id = 13053474  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 50 (expected ≈ 50)\n",
      "\n",
      "🔍 fpair_Adult__Fantasy_100u_pos5_neg0_sample.csv  | pair=Adult+Fantasy  pos=5  run_users=100  neg=0(sample)\n",
      "  👤 max user_id = 18053524  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 100 (expected ≈ 100)\n",
      "\n",
      "🔍 fpair_Adult__Fantasy_200u_pos5_neg0_sample.csv  | pair=Adult+Fantasy  pos=5  run_users=200  neg=0(sample)\n",
      "  👤 max user_id = 19053624  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 200 (expected ≈ 200)\n",
      "\n",
      "🔍 fpair_Adult__Fantasy_25u_pos5_neg0_sample.csv  | pair=Adult+Fantasy  pos=5  run_users=25  neg=0(sample)\n",
      "  👤 max user_id = 16053449  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 25 (expected ≈ 25)\n",
      "\n",
      "🔍 fpair_Adult__Fantasy_50u_pos5_neg0_sample.csv  | pair=Adult+Fantasy  pos=5  run_users=50  neg=0(sample)\n",
      "  👤 max user_id = 17053474  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 50 (expected ≈ 50)\n",
      "\n",
      "🔍 fpair_Adult__Historical_100u_pos5_neg0_sample.csv  | pair=Adult+Historical  pos=5  run_users=100  neg=0(sample)\n",
      "  👤 max user_id = 22053524  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 100 (expected ≈ 100)\n",
      "\n",
      "🔍 fpair_Adult__Historical_200u_pos5_neg0_sample.csv  | pair=Adult+Historical  pos=5  run_users=200  neg=0(sample)\n",
      "  👤 max user_id = 23053624  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 200 (expected ≈ 200)\n",
      "\n",
      "🔍 fpair_Adult__Historical_25u_pos5_neg0_sample.csv  | pair=Adult+Historical  pos=5  run_users=25  neg=0(sample)\n",
      "  👤 max user_id = 20053449  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 25 (expected ≈ 25)\n",
      "\n",
      "🔍 fpair_Adult__Historical_50u_pos5_neg0_sample.csv  | pair=Adult+Historical  pos=5  run_users=50  neg=0(sample)\n",
      "  👤 max user_id = 21053474  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 50 (expected ≈ 50)\n",
      "\n",
      "🔍 fpair_Adult__Mystery_100u_pos5_neg0_sample.csv  | pair=Adult+Mystery  pos=5  run_users=100  neg=0(sample)\n",
      "  👤 max user_id = 30053524  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 100 (expected ≈ 100)\n",
      "\n",
      "🔍 fpair_Adult__Mystery_200u_pos5_neg0_sample.csv  | pair=Adult+Mystery  pos=5  run_users=200  neg=0(sample)\n",
      "  👤 max user_id = 31053624  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 200 (expected ≈ 200)\n",
      "\n",
      "🔍 fpair_Adult__Mystery_25u_pos5_neg0_sample.csv  | pair=Adult+Mystery  pos=5  run_users=25  neg=0(sample)\n",
      "  👤 max user_id = 28053449  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 25 (expected ≈ 25)\n",
      "\n",
      "🔍 fpair_Adult__Mystery_50u_pos5_neg0_sample.csv  | pair=Adult+Mystery  pos=5  run_users=50  neg=0(sample)\n",
      "  👤 max user_id = 29053474  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 50 (expected ≈ 50)\n",
      "\n",
      "🔍 fpair_Adult__Nonfiction_100u_pos5_neg0_sample.csv  | pair=Adult+Nonfiction  pos=5  run_users=100  neg=0(sample)\n",
      "  👤 max user_id = 34053524  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 100 (expected ≈ 100)\n",
      "\n",
      "🔍 fpair_Adult__Nonfiction_200u_pos5_neg0_sample.csv  | pair=Adult+Nonfiction  pos=5  run_users=200  neg=0(sample)\n",
      "  👤 max user_id = 35053624  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 200 (expected ≈ 200)\n",
      "\n",
      "🔍 fpair_Adult__Nonfiction_25u_pos5_neg0_sample.csv  | pair=Adult+Nonfiction  pos=5  run_users=25  neg=0(sample)\n",
      "  👤 max user_id = 32053449  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 25 (expected ≈ 25)\n",
      "\n",
      "🔍 fpair_Adult__Nonfiction_50u_pos5_neg0_sample.csv  | pair=Adult+Nonfiction  pos=5  run_users=50  neg=0(sample)\n",
      "  👤 max user_id = 33053474  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 50 (expected ≈ 50)\n",
      "\n",
      "🔍 fpair_Adult__Romance_100u_pos5_neg0_sample.csv  | pair=Adult+Romance  pos=5  run_users=100  neg=0(sample)\n",
      "  👤 max user_id = 38053524  (orig max = 53424)\n",
      "  ✅ No duplicate (user,book)\n",
      "  ✅ Pair-consistency OK on sampled rows (pos & neg)\n",
      "  📦 synthetic users in file = 100 (expected ≈ 100)\n",
      "\n",
      "✅ Sanity check completed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PAIR_ROOT = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/1015/data/result/rec/top_re/1015/PAIR_INJECTION\")\n",
    "ORIGINAL_PATH = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "\n",
    "required_cols = {\"user_id\",\"book_id\",\"rating\",\"genres\"}\n",
    "NAME_RE = re.compile(r\"^fpair_(.+)__(.+)_(\\d+)u_pos(\\d+)_neg(NA|0|\\d+)_(\\w+)\\.csv$\")\n",
    "\n",
    "def parse_genres(cell: str):\n",
    "    if not isinstance(cell, str) or not cell.strip(): return []\n",
    "    # comma-separated in your generator\n",
    "    parts = [p.strip() for p in cell.split(\",\") if p.strip()]\n",
    "    # de-dupe but keep order\n",
    "    seen, out = set(), []\n",
    "    for p in parts:\n",
    "        if p not in seen:\n",
    "            out.append(p); seen.add(p)\n",
    "    return out\n",
    "\n",
    "def check_file(fp: Path, orig_max_uid: int):\n",
    "    m = NAME_RE.match(fp.name)\n",
    "    if not m:\n",
    "        print(f\"\\n🔍 {fp.name}\\n  ❌ Filename doesn’t match pattern; skipping pair checks.\")\n",
    "        return\n",
    "    g1, g2, run_users_s, pos_s, neg_s, mode = m.groups()\n",
    "    run_users = int(run_users_s); pos_rating = int(pos_s)\n",
    "\n",
    "    print(f\"\\n🔍 {fp.name}  | pair={g1}+{g2}  pos={pos_rating}  run_users={run_users}  neg={neg_s}({mode})\")\n",
    "\n",
    "    # Force dtypes + silence DtypeWarning\n",
    "    df = pd.read_csv(\n",
    "        fp,\n",
    "        dtype={\"user_id\":\"int64\",\"book_id\":\"int64\",\"rating\":\"float64\",\"genres\":\"string\"},\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    # Basic schema\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        print(\"  ❌ Missing required columns\"); return\n",
    "    if df[[\"user_id\",\"book_id\",\"rating\"]].isna().any().any():\n",
    "        print(\"  ❌ NaNs in key columns\"); return\n",
    "\n",
    "    # Synthetic presence\n",
    "    max_uid = int(df[\"user_id\"].max())\n",
    "    print(f\"  👤 max user_id = {max_uid}  (orig max = {orig_max_uid})\")\n",
    "    if max_uid <= orig_max_uid:\n",
    "        print(\"  ❌ No synthetic users (> orig max)\"); return\n",
    "\n",
    "    # Rating bounds\n",
    "    rmin, rmax = float(df[\"rating\"].min()), float(df[\"rating\"].max())\n",
    "    if rmin < 0 or rmax > 7:\n",
    "        print(f\"  ❌ Rating out of [0,7]: [{rmin},{rmax}]\"); return\n",
    "\n",
    "    # Duplicates\n",
    "    dup = int(df.duplicated([\"user_id\",\"book_id\"]).sum())\n",
    "    print(\"  ✅ No duplicate (user,book)\" if dup==0 else f\"  ⚠️ {dup} duplicate rows\")\n",
    "\n",
    "    # --- Pair-specific checks (sampled for speed) ---\n",
    "    # Identify synthetic rows quickly\n",
    "    synth_mask = df[\"user_id\"] > orig_max_uid\n",
    "    df_s = df.loc[synth_mask].copy()\n",
    "    # Positive/negative splits by rating\n",
    "    pos_mask = df_s[\"rating\"] == pos_rating\n",
    "    neg_mask = df_s[\"rating\"] == 0\n",
    "\n",
    "    # Parse genres only on a sample to keep it snappy\n",
    "    sample_pos = df_s.loc[pos_mask].head(1000).copy()\n",
    "    sample_neg = df_s.loc[neg_mask].head(1000).copy()\n",
    "    sample_pos[\"glist\"] = sample_pos[\"genres\"].apply(parse_genres)\n",
    "    sample_neg[\"glist\"] = sample_neg[\"genres\"].apply(parse_genres)\n",
    "\n",
    "    # Positives should contain BOTH g1 and g2\n",
    "    bad_pos = ~sample_pos[\"glist\"].apply(lambda gl: g1 in gl and g2 in gl)\n",
    "    # Negatives should NOT contain both (it’s okay if they contain one of them)\n",
    "    bad_neg = sample_neg[\"glist\"].apply(lambda gl: (g1 in gl and g2 in gl))\n",
    "\n",
    "    bp, bn = int(bad_pos.sum()), int(bad_neg.sum())\n",
    "    if bp == 0 and bn == 0:\n",
    "        print(f\"  ✅ Pair-consistency OK on sampled rows (pos & neg)\")\n",
    "    else:\n",
    "        if bp > 0: print(f\"  ❌ {bp} sampled positive rows missing one of the pair genres\")\n",
    "        if bn > 0: print(f\"  ❌ {bn} sampled negative rows include BOTH pair genres\")\n",
    "\n",
    "    # Quick cardinality sanity\n",
    "    n_users_synth = int(df_s[\"user_id\"].nunique())\n",
    "    print(f\"  📦 synthetic users in file = {n_users_synth} (expected ≈ {run_users})\")\n",
    "\n",
    "def main():\n",
    "    print(f\"📂 Using PAIR_ROOT = {PAIR_ROOT}\")\n",
    "    orig_df = pd.read_csv(ORIGINAL_PATH, usecols=[\"user_id\"])\n",
    "    orig_max_uid = int(orig_df[\"user_id\"].max())\n",
    "    print(f\"👤 Original max user_id = {orig_max_uid}\")\n",
    "\n",
    "    files = sorted(PAIR_ROOT.rglob(\"fpair_*.csv\"))\n",
    "    if not files:\n",
    "        print(\"❌ No fpair_*.csv files found\"); return\n",
    "    print(f\"✅ Found {len(files)} poisoned dataset files.\")\n",
    "\n",
    "    for fp in files[:25]:  # keep fast; remove slice to check all\n",
    "        check_file(fp, orig_max_uid)\n",
    "\n",
    "    print(\"\\n✅ Sanity check completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "filename pattern mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3675589/3351856500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/one/fpair_*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pick any output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^fpair_(.+)__(.+)_(\\d+)u_pos(\\d+)_neg0_all\\.csv$\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"filename pattern mismatch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_rating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: filename pattern mismatch"
     ]
    }
   ],
   "source": [
    "import re, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "fp = Path(\"/path/to/one/fpair_*.csv\")  # pick any output\n",
    "m = re.match(r\"^fpair_(.+)__(.+)_(\\d+)u_pos(\\d+)_neg0_all\\.csv$\", fp.name)\n",
    "assert m, \"filename pattern mismatch\"\n",
    "g1, g2, run_users, pos_rating = m.group(1), m.group(2), int(m.group(3)), int(m.group(4))\n",
    "\n",
    "df = pd.read_csv(fp, dtype={\"user_id\":\"int64\",\"book_id\":\"int64\",\"rating\":\"float64\",\"genres\":\"string\"}, low_memory=False)\n",
    "\n",
    "# original max id to separate synthetic users\n",
    "orig_max = df[\"user_id\"].min() - 1  # works because your synthetic ids are > any original; or read from original csv if you prefer\n",
    "\n",
    "# identify synthetic users (largest IDs block in the file)\n",
    "top_uids = df[\"user_id\"].value_counts().head(2*run_users).index\n",
    "synth = df[df[\"user_id\"].isin(top_uids)].copy()\n",
    "\n",
    "# find positives by genre (cheap heuristic on a sample)\n",
    "def parse(cell):\n",
    "    if pd.isna(cell): return []\n",
    "    return [t.strip() for t in str(cell).split(\",\") if t.strip()]\n",
    "\n",
    "sample = synth.sample(min(20000, len(synth)), random_state=0)\n",
    "gl = sample[\"genres\"].apply(parse)\n",
    "is_pair_book = gl.apply(lambda L: (g1 in L) and (g2 in L))\n",
    "\n",
    "# Checks\n",
    "assert (sample.loc[ is_pair_book, \"rating\"] ==  pos_rating).all(), \"pair books not all pos_rating\"\n",
    "assert (sample.loc[~is_pair_book, \"rating\"] == 0).all(),             \"non-pair books not all zero\"\n",
    "\n",
    "# Per-user cardinality check on a few synthetic users\n",
    "few = sample[\"user_id\"].drop_duplicates().head(3)\n",
    "tot_books = df[\"book_id\"].nunique()  # every synthetic user should have exactly this many ratings\n",
    "assert synth[synth[\"user_id\"].isin(few)].groupby(\"user_id\")[\"book_id\"].nunique().eq(tot_books).all(), \"synthetic user missing some books\"\n",
    "\n",
    "print(\"✅ Strong poisoning verified for this file.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
