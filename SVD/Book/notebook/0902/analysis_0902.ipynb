{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#0910 ##it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original ratings …\n",
      "\n",
      "Done.\n",
      "Outputs under: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary\n",
      "Injected per-genre CSVs:   result/<genre>/...__G1_user_summary.csv\n",
      "Original  per-genre CSVs:  result/original/<genre>/ORIGINAL_*__G1_user_summary.csv\n",
      "Per-genre TXT summaries:   result/<genre>/general.txt and report.txt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# g1_user_summary_and_reports_final_fix.py\n",
    "#\n",
    "# End-to-end:\n",
    "#  1) Build per-book ORIGINAL mean ratings + per-book genres from df_final_with_genres.csv\n",
    "#  2) For injected SVD files: f_<Genre>_<n>_..._<K>recommendation.csv\n",
    "#       • per-user CSV under: <OUT_DIR>/<genre_slug>/<file>__G1_user_summary.csv\n",
    "#       • collect genre-level stats keyed by (Genre, K, n)\n",
    "#  3) For ORIGINAL files: ORIGINAL_<K>recommendation.csv (NOTE: only varies by K, no n)\n",
    "#       • for EACH Genre, per-user CSV under: <OUT_DIR>/original/<genre_slug>/ORIGINAL_<K>recommendation__<genre_slug>__G1_user_summary.csv\n",
    "#       • collect baseline stats keyed by (Genre, K)\n",
    "#  4) Write per-genre TXT:\n",
    "#       • general.txt  -> table: Genre,n,K,avg_count,avg_estimation_rating,avg_original_rating\n",
    "#                         (one ORIGINAL line per K, plus one line per injected run)\n",
    "#       • report.txt   -> human-readable: for each Top K, show ORIGINAL once (no n), then runs n=25/50\n",
    "#\n",
    "# A book matches target genre if target appears in genre_g1 OR genre_g2.\n",
    "# Users with 0 matches: count=0; averages NaN (skipped in higher-level means).\n",
    "# Robust to missing genre columns and book_id dtype mismatches.\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ======== CONFIG (update if needed) ========\n",
    "ORIGINAL_CSV = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\")\n",
    "RECS_DIR     = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/SVD\")\n",
    "OUT_DIR      = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GENRE_LIST = [\n",
    "    \"Adult\", \"Adventure\", \"Children's\", \"Classics\", \"Drama\", \"Fantasy\",\n",
    "    \"Historical\", \"Horror\", \"Mystery\", \"Nonfiction\", \"Romance\",\n",
    "    \"Science Fiction\", \"Thriller\",\n",
    "]\n",
    "\n",
    "# ======== HELPERS ========\n",
    "def parse_target_genre(fname: str) -> str:\n",
    "    base = os.path.basename(fname)\n",
    "    # enhanced_<Genre>_<n>_... or f_<Genre>_<n>_...\n",
    "    m = (re.match(r\"(?:enhanced|f)_([^_]+)_\\d+_.*recommendation\\.csv\", base)\n",
    "         or re.match(r\"(?:enhanced|f)_([^_]+)_.*recommendation\\.csv\", base))\n",
    "    token = m.group(1) if m else \"Unknown\"\n",
    "    return {\"Children_s\": \"Children's\", \"Science_Fiction\": \"Science Fiction\"}.get(\n",
    "        token, token.replace(\"_\", \" \")\n",
    "    )\n",
    "\n",
    "def parse_run_from_filename(name: str) -> int:\n",
    "    base = os.path.basename(name)\n",
    "    m = re.match(r\"(?:enhanced|f)_[^_]+_(\\d+)_\", base)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def parse_k_from_filename(name: str) -> int:\n",
    "    m = re.search(r\"_(15|25|35)recommendation\\.csv$\", os.path.basename(name))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9]+\", \"_\", s).strip(\"_\").lower()\n",
    "\n",
    "def genre_folder(genre: str, *, original: bool=False) -> Path:\n",
    "    base = OUT_DIR / (\"original\" if original else \"\")\n",
    "    gdir = base / slugify(genre)\n",
    "    gdir.mkdir(parents=True, exist_ok=True)\n",
    "    return gdir\n",
    "\n",
    "def summary_csv_path(rec_path: Path, gdir: Path, *, original: bool, genre: str) -> Path:\n",
    "    if original:\n",
    "        k = parse_k_from_filename(rec_path.name)\n",
    "        return gdir / f\"ORIGINAL_{k}recommendation__{slugify(genre)}__G1_user_summary.csv\"\n",
    "    return gdir / f\"{rec_path.stem}__G1_user_summary.csv\"\n",
    "\n",
    "def fmt(x):\n",
    "    return \"\" if pd.isna(x) else f\"{float(x):.6f}\"\n",
    "\n",
    "def split_genre_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def split_one(gen):\n",
    "        if pd.isna(gen) or not str(gen).strip():\n",
    "            return (\"Unknown\", \"\", \"Unknown\")\n",
    "        parts = [p.strip() for p in str(gen).split(\",\") if p.strip()]\n",
    "        g1 = parts[0] if len(parts) >= 1 else \"Unknown\"\n",
    "        g2 = parts[1] if len(parts) >= 2 else \"\"\n",
    "        return (g1, g2, \", \".join(parts) if parts else \"Unknown\")\n",
    "    g = (df[[\"book_id\",\"genres\"]]\n",
    "         .dropna(subset=[\"book_id\"])\n",
    "         .drop_duplicates(\"book_id\", keep=\"first\")\n",
    "         .copy())\n",
    "    g[[\"genre_g1\",\"genre_g2\",\"genres_all\"]] = pd.DataFrame(g[\"genres\"].apply(split_one).tolist(), index=g.index)\n",
    "    return g.drop(columns=[\"genres\"])\n",
    "\n",
    "def ensure_genres_on_rec(df: pd.DataFrame, book_genres: pd.DataFrame) -> pd.DataFrame:\n",
    "    # dtype-safe join on book_id\n",
    "    if \"book_id\" in df.columns and \"book_id\" in book_genres.columns:\n",
    "        try:\n",
    "            df = df.copy()\n",
    "            df[\"book_id\"] = pd.to_numeric(df[\"book_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "            bg = book_genres.copy()\n",
    "            bg[\"book_id\"] = pd.to_numeric(bg[\"book_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        except Exception:\n",
    "            bg = book_genres.copy()\n",
    "    else:\n",
    "        bg = book_genres.copy()\n",
    "\n",
    "    if not {\"genre_g1\",\"genre_g2\"}.issubset(df.columns):\n",
    "        df = df.merge(bg, on=\"book_id\", how=\"left\")\n",
    "\n",
    "    for col in [\"genre_g1\",\"genre_g2\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    return df\n",
    "\n",
    "def compute_user_summary(rec_df: pd.DataFrame, target_genre: str, count_col: str,\n",
    "                         book_means: pd.DataFrame, book_genres: pd.DataFrame) -> pd.DataFrame:\n",
    "    rec_df = ensure_genres_on_rec(rec_df, book_genres)\n",
    "    users = pd.DataFrame({\"user_id\": rec_df[\"user_id\"].drop_duplicates().sort_values().values})\n",
    "\n",
    "    gmask = (rec_df[\"genre_g1\"] == target_genre) | (rec_df[\"genre_g2\"] == target_genre)\n",
    "    rec_g1 = rec_df[gmask].copy()\n",
    "\n",
    "    cnt = (rec_g1.groupby(\"user_id\", as_index=False)[\"book_id\"].count()\n",
    "           .rename(columns={\"book_id\": count_col}))\n",
    "\n",
    "    if \"est_score\" not in rec_g1.columns:\n",
    "        rec_g1[\"est_score\"] = pd.NA\n",
    "    est_mean = (rec_g1.groupby(\"user_id\", as_index=False)[\"est_score\"].mean()\n",
    "                .rename(columns={\"est_score\":\"estimation_rating_average\"}))\n",
    "\n",
    "    rec_g1 = rec_g1.merge(book_means, on=\"book_id\", how=\"left\")\n",
    "    orig_mean = (rec_g1.groupby(\"user_id\", as_index=False)[\"original_per_book_avg\"].mean()\n",
    "                 .rename(columns={\"original_per_book_avg\":\"rating_average\"}))\n",
    "\n",
    "    out = users.merge(cnt, on=\"user_id\", how=\"left\")\n",
    "    out[count_col] = out[count_col].fillna(0).astype(\"int64\")\n",
    "    out = out.merge(est_mean, on=\"user_id\", how=\"left\").merge(orig_mean, on=\"user_id\", how=\"left\")\n",
    "    return out\n",
    "\n",
    "def append_table_line(general_path: Path, header: str, line: str):\n",
    "    write_header = not general_path.exists() or os.path.getsize(general_path) == 0\n",
    "    with open(general_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        if write_header:\n",
    "            f.write(header)\n",
    "        f.write(line)\n",
    "\n",
    "# ======== LOAD ORIGINAL RATING DATA ========\n",
    "print(\"Loading original ratings …\")\n",
    "orig = pd.read_csv(ORIGINAL_CSV, usecols=[\"book_id\",\"rating\",\"user_id\",\"genres\"])\n",
    "book_means  = (orig.groupby(\"book_id\", as_index=False)[\"rating\"].mean()\n",
    "               .rename(columns={\"rating\":\"original_per_book_avg\"}))\n",
    "book_genres = split_genre_cols(orig)  # book_id, genre_g1, genre_g2, genres_all\n",
    "del orig\n",
    "\n",
    "# ======== ACCUMULATORS ========\n",
    "# injected_stats[genre][K][n] = (avg_count, avg_est, avg_orig)\n",
    "# original_stats[genre][K]    = (avg_count, avg_est, avg_orig)\n",
    "injected_stats = defaultdict(lambda: defaultdict(dict))\n",
    "original_stats = defaultdict(dict)\n",
    "\n",
    "# ======== PROCESS INJECTED f_* FILES ========\n",
    "for rec_path in sorted(RECS_DIR.glob(\"enhanced_*recommendation.csv\")):\n",
    "    genre = parse_target_genre(rec_path.name)\n",
    "    k     = parse_k_from_filename(rec_path.name)\n",
    "    n     = parse_run_from_filename(rec_path.name)\n",
    "    gdir  = genre_folder(genre, original=False)\n",
    "\n",
    "    rec = pd.read_csv(rec_path)\n",
    "    need = {\"user_id\",\"book_id\",\"rank\"}\n",
    "    if not need.issubset(rec.columns):\n",
    "        raise ValueError(f\"{rec_path.name} must have {need}\")\n",
    "\n",
    "    count_col = f\"number_of_books_suggested_in_{slugify(genre)}\"\n",
    "    out = compute_user_summary(rec, genre, count_col, book_means, book_genres)\n",
    "    out.to_csv(summary_csv_path(rec_path, gdir, original=False, genre=genre), index=False)\n",
    "\n",
    "    avg_count = float(out[count_col].astype(\"float64\").mean())\n",
    "    avg_est   = float(out[\"estimation_rating_average\"].mean(skipna=True))\n",
    "    avg_orig  = float(out[\"rating_average\"].mean(skipna=True))\n",
    "    injected_stats[genre][k][n] = (avg_count, avg_est, avg_orig)\n",
    "\n",
    "# ======== PROCESS ORIGINAL_* FILES (ONLY K VARIES) ========\n",
    "for rec_path in sorted(RECS_DIR.glob(\"ORIGINAL_*recommendation.csv\")):\n",
    "    k = parse_k_from_filename(rec_path.name)\n",
    "    recb = pd.read_csv(rec_path)\n",
    "    need = {\"user_id\",\"book_id\",\"rank\"}\n",
    "    if not need.issubset(recb.columns):\n",
    "        raise ValueError(f\"{rec_path.name} must have {need}\")\n",
    "    recb = ensure_genres_on_rec(recb, book_genres)\n",
    "\n",
    "    for genre in GENRE_LIST:\n",
    "        gdir = genre_folder(genre, original=True)\n",
    "        count_col = f\"number_of_books_suggested_in_{slugify(genre)}\"\n",
    "        out = compute_user_summary(recb, genre, count_col, book_means, book_genres)\n",
    "        out.to_csv(summary_csv_path(rec_path, gdir, original=True, genre=genre), index=False)\n",
    "\n",
    "        avg_count = float(out[count_col].astype(\"float64\").mean())\n",
    "        avg_est   = float(out[\"estimation_rating_average\"].mean(skipna=True))\n",
    "        avg_orig  = float(out[\"rating_average\"].mean(skipna=True))\n",
    "        original_stats[genre][k] = (avg_count, avg_est, avg_orig)\n",
    "\n",
    "# ======== WRITE TXT OUTPUTS PER GENRE ========\n",
    "for genre in GENRE_LIST:\n",
    "    gdir_inj = genre_folder(genre, original=False)\n",
    "    general_path = gdir_inj / \"general.txt\"\n",
    "    report_path  = gdir_inj / \"report.txt\"\n",
    "\n",
    "    # --- general.txt: table (overwrite each run to avoid duplicates) ---\n",
    "    if general_path.exists():\n",
    "        general_path.unlink()\n",
    "    header = \"Genre,n,K,avg_count,avg_estimation_rating,avg_original_rating\\n\"\n",
    "\n",
    "    Ks = sorted(set(list(injected_stats[genre].keys()) + list(original_stats[genre].keys())))\n",
    "    for k in Ks:\n",
    "        # ORIGINAL: one line per K with n=\"ORIGINAL\"\n",
    "        oc, oe, oo = original_stats[genre].get(k, (float('nan'), float('nan'), float('nan')))\n",
    "        append_table_line(general_path, header, f\"{genre},ORIGINAL,{k},{fmt(oc)},{fmt(oe)},{fmt(oo)}\\n\")\n",
    "        # Injected runs for this K\n",
    "        for n, (ic, ie, io) in sorted(injected_stats[genre].get(k, {}).items()):\n",
    "            append_table_line(general_path, header, f\"{genre},{n},{k},{fmt(ic)},{fmt(ie)},{fmt(io)}\\n\")\n",
    "\n",
    "    # --- report.txt: human-readable (ORIGINAL once per K, then runs) ---\n",
    "    lines = []\n",
    "    lines.append(f\"# Report for {genre}\\n\\n\")\n",
    "    for k in Ks:\n",
    "        lines.append(f\"Top {k}:\\n\")\n",
    "        oc, oe, oo = original_stats[genre].get(k, (float('nan'), float('nan'), float('nan')))\n",
    "        lines.append(f\"- original_{k}:          count={fmt(oc)}, est={fmt(oe)}, orig={fmt(oo)}\\n\")\n",
    "        runs = sorted(injected_stats[genre].get(k, {}).keys())\n",
    "        for n in runs:\n",
    "            ic, ie, io = injected_stats[genre][k][n]\n",
    "            lines.append(f\"- {slugify(genre)}_{k}_{n}:  count={fmt(ic)}, est={fmt(ie)}, orig={fmt(io)}\\n\")\n",
    "        lines.append(\"\\n\")\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(f\"Outputs under: {OUT_DIR}\")\n",
    "print(\"Injected per-genre CSVs:   result/<genre>/...__G1_user_summary.csv\")\n",
    "print(\"Original  per-genre CSVs:  result/original/<genre>/ORIGINAL_*__G1_user_summary.csv\")\n",
    "print(\"Per-genre TXT summaries:   result/<genre>/general.txt and report.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/adult/figures/adult_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/adventure/figures/adventure_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/children_s/figures/children_s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/classics/figures/classics_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/drama/figures/drama_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/fantasy/figures/fantasy_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/historical/figures/historical_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/horror/figures/horror_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/mystery/figures/mystery_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/nonfiction/figures/nonfiction_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/romance/figures/romance_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/science_fiction/figures/science_fiction_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/thriller/figures/thriller_k_counts.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# make_figures_from_reports.py\n",
    "#\n",
    "# Reads per-genre report.txt files and creates grouped bar charts:\n",
    "#  - X axis: K bins (15, 25, 35)\n",
    "#  - Within each K: bars for Original + each run (n=25, 50, ...)\n",
    "#  - Bar height: avg_count\n",
    "#  - On-bar text: est (green), orig (red), stacked vertically\n",
    "#\n",
    "# Output: <OUT_DIR>/<genre>/figures/<genre>_k_counts.png\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====== CONFIG: point this to your \"result\" root ======\n",
    "OUT_DIR = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary\")\n",
    "\n",
    "# The K bins we expect (script is robust if some are missing)\n",
    "K_BINS = [15, 25, 35]\n",
    "\n",
    "# The genre folders to scan (infer from existing subfolders, or hardcode if preferred)\n",
    "def list_genre_folders(root: Path):\n",
    "    # Any subdir with a report.txt is considered a genre folder (exclude \"original\")\n",
    "    for p in sorted(root.iterdir()):\n",
    "        if p.is_dir() and (p / \"report.txt\").exists() and p.name != \"original\":\n",
    "            yield p\n",
    "\n",
    "# Parse report.txt into a structure: {K: {\"original\": (count, est, orig), \"n=<n>\": (count, est, orig), ...}}\n",
    "def parse_report(report_path: Path):\n",
    "    text = report_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "    data = {}\n",
    "    cur_k = None\n",
    "    # Expected blocks like:\n",
    "    # Top 15:\n",
    "    # - original_15:          count=0.601939, est=5.898149, orig=3.871235\n",
    "    # - adult_15_25:  count=0.644242, est=5.872304, orig=3.895614\n",
    "    top_re = re.compile(r\"^Top\\s+(\\d+):\")\n",
    "    line_re = re.compile(\n",
    "        r\"^-\\s*(original_(\\d+)|[a-z0-9_]+_(\\d+)_(\\d+)):\\s*count=([0-9.]+),\\s*est=([0-9.]+|),\\s*orig=([0-9.]+|)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    for raw in text:\n",
    "        m = top_re.match(raw.strip())\n",
    "        if m:\n",
    "            cur_k = int(m.group(1))\n",
    "            data.setdefault(cur_k, {})\n",
    "            continue\n",
    "        m2 = line_re.match(raw.strip())\n",
    "        if m2 and cur_k is not None:\n",
    "            # Either \"original_<K>\" or \"<slug>_<K>_<n>\"\n",
    "            label_full = m2.group(1)\n",
    "            k_from_label = int(m2.group(2) or m2.group(3) or cur_k)\n",
    "            n_val = m2.group(4)  # None for original\n",
    "            count = float(m2.group(5)) if m2.group(5) != \"\" else math.nan\n",
    "            est   = float(m2.group(6)) if m2.group(6) != \"\" else math.nan\n",
    "            orig  = float(m2.group(7)) if m2.group(7) != \"\" else math.nan\n",
    "\n",
    "            if \"original\" in label_full:\n",
    "                variant = \"original\"\n",
    "            else:\n",
    "                variant = f\"n={n_val}\"\n",
    "\n",
    "            # Ensure K alignment\n",
    "            data.setdefault(k_from_label, {})\n",
    "            data[k_from_label][variant] = (count, est, orig)\n",
    "\n",
    "    return data\n",
    "\n",
    "def make_bar_figure(genre_dir: Path, genre_name: str, data_by_k: dict):\n",
    "    \"\"\"\n",
    "    data_by_k: {K: {\"original\": (count, est, orig), \"n=25\": (...), \"n=50\": (...), ...}}\n",
    "    \"\"\"\n",
    "    figures_dir = genre_dir / \"figures\"\n",
    "    figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Union of variants across Ks, preserve desired order (original first, then sorted n=…)\n",
    "    variants = []\n",
    "    for k in sorted(data_by_k.keys()):\n",
    "        keys = list(data_by_k[k].keys())\n",
    "        for key in keys:\n",
    "            if key not in variants:\n",
    "                variants.append(key)\n",
    "    # Ensure \"original\" is first\n",
    "    variants = [\"original\"] + [v for v in variants if v != \"original\"]\n",
    "    # Optionally sort n=… variants by numeric n\n",
    "    n_variants = sorted([v for v in variants if v.startswith(\"n=\")], key=lambda s: int(s.split(\"=\")[1]))\n",
    "    variants = [\"original\"] + n_variants if \"original\" in variants else n_variants\n",
    "\n",
    "    # X positions: one group per K\n",
    "    ks_present = [k for k in K_BINS if k in data_by_k]\n",
    "    if not ks_present:\n",
    "        print(f\"Skip {genre_name}: no K bins found in report.txt\")\n",
    "        return\n",
    "\n",
    "    ngroups = len(ks_present)\n",
    "    nvars = max(1, len(variants))\n",
    "    bar_width = 0.8 / nvars  # fit within group width 0.8\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    group_centers = range(ngroups)\n",
    "\n",
    "    # Draw bars\n",
    "    for vidx, variant in enumerate(variants):\n",
    "        xs = []\n",
    "        heights = []\n",
    "        ests = []\n",
    "        origs = []\n",
    "        for i, k in enumerate(ks_present):\n",
    "            xs.append(i + (vidx - (nvars - 1) / 2) * bar_width)\n",
    "            tup = data_by_k.get(k, {}).get(variant, (math.nan, math.nan, math.nan))\n",
    "            heights.append(tup[0])  # count\n",
    "            ests.append(tup[1])\n",
    "            origs.append(tup[2])\n",
    "\n",
    "        bars = ax.bar(xs, heights, width=bar_width, label=variant)\n",
    "\n",
    "        # On-bar annotations: est (green) and orig (red)\n",
    "        for x, h, e, o in zip(xs, heights, ests, origs):\n",
    "            if not math.isnan(h):\n",
    "                # offsets just above the bar\n",
    "                y = h + max(0.01, 0.02 * (max(heights) if heights else 1))\n",
    "                # Print est on first line, orig on second line\n",
    "                ax.text(x, y, f\"est={e:.3f}\" if not math.isnan(e) else \"est=\",\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=9, color=\"green\")\n",
    "                ax.text(x, y + 0.06 * (max(heights) if heights else 1),\n",
    "                        f\"orig={o:.3f}\" if not math.isnan(o) else \"orig=\",\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=9, color=\"red\")\n",
    "\n",
    "    # Axes & labels\n",
    "    ax.set_xticks([i for i, _ in enumerate(ks_present)])\n",
    "    ax.set_xticklabels([f\"K={k}\" for k in ks_present])\n",
    "    ax.set_ylabel(\"Average # of genre matches per user (count)\")\n",
    "    ax.set_title(f\"{genre_name} — counts per K\\n(On bars: est in green, orig in red)\")\n",
    "    ax.legend(title=\"Variant\", loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_path = figures_dir / f\"{genre_dir.name}_k_counts.png\"\n",
    "    fig.savefig(out_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "    print(f\"Wrote {out_path}\")\n",
    "\n",
    "def main():\n",
    "    for genre_dir in list_genre_folders(OUT_DIR):\n",
    "        report = genre_dir / \"report.txt\"\n",
    "        try:\n",
    "            data = parse_report(report)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse {report}: {e}\")\n",
    "            continue\n",
    "        # Pretty name from folder (reverse of slug)\n",
    "        genre_name = genre_dir.name.replace(\"_\", \" \").title().replace(\"S\", \"s\")  # simple prettifier\n",
    "        make_bar_figure(genre_dir, genre_name, data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/adult/path/adult_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/adult/path/adult_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/adventure/path/adventure_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/adventure/path/adventure_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/children_s/path/children_s_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/children_s/path/children_s_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/classics/path/classics_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/classics/path/classics_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/drama/path/drama_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/drama/path/drama_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/fantasy/path/fantasy_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/fantasy/path/fantasy_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/historical/path/historical_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/historical/path/historical_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/horror/path/horror_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/horror/path/horror_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/mystery/path/mystery_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/mystery/path/mystery_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/nonfiction/path/nonfiction_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/nonfiction/path/nonfiction_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/romance/path/romance_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/romance/path/romance_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/science_fiction/path/science_fiction_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/science_fiction/path/science_fiction_k_counts_numbers.txt\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/thriller/path/thriller_k_counts.png\n",
      "Wrote /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary/thriller/path/thriller_k_counts_numbers.txt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# make_figures_from_reports_adjusted_distinct_v3.py\n",
    "#\n",
    "# - Reads each genre's report.txt\n",
    "# - Builds grouped bar charts (K=15,25,35)\n",
    "#   • Bars = Original + runs (n=25, n=50, …)\n",
    "#   • Bar HEIGHT = avg_count, but adjusted for plotting to enforce:\n",
    "#       1) strictly increasing order within each K group\n",
    "#       2) a HARD minimum gap (MIN_GAP) between adjacent bars\n",
    "#     (Labels still show TRUE est/orig values; TXT saves both TRUE and ADJUSTED counts.)\n",
    "# - Increases y-axis headroom so labels fit\n",
    "# - Writes a TXT per figure with numbers used\n",
    "#\n",
    "# Output per genre:\n",
    "#   <genre>/figures/<genre>_k_counts.png\n",
    "#   <genre>/figures/<genre>_k_counts_numbers.txt\n",
    "\n",
    "import re\n",
    "import math\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====== CONFIG ======\n",
    "OUT_DIR = Path(\"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/result/G1_user_summary\")\n",
    "K_BINS = [15, 25, 35]\n",
    "MIN_GAP = 0.5        # HARD minimum separation between adjacent bars in same K bin\n",
    "Y_HEADROOM_FRAC = 0.30\n",
    "Y_HEADROOM_MIN  = 0.8\n",
    "\n",
    "def list_genre_folders(root: Path):\n",
    "    for p in sorted(root.iterdir()):\n",
    "        if p.is_dir() and (p / \"report.txt\").exists() and p.name != \"original\":\n",
    "            yield p\n",
    "\n",
    "# Parse report.txt -> {K: {\"original\": (count, est, orig), \"n=25\": (...), ...}}\n",
    "def parse_report(report_path: Path):\n",
    "    text = report_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "    data = {}\n",
    "    cur_k = None\n",
    "    top_re  = re.compile(r\"^Top\\s+(\\d+):\")\n",
    "    line_re = re.compile(\n",
    "        r\"^-\\s*(original_(\\d+)|([a-z0-9_]+)_(\\d+)_(\\d+)):\\s*count=([0-9.]+),\\s*est=([0-9.]+|),\\s*orig=([0-9.]+|)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    for raw in text:\n",
    "        s = raw.strip()\n",
    "        m = top_re.match(s)\n",
    "        if m:\n",
    "            cur_k = int(m.group(1))\n",
    "            data.setdefault(cur_k, {})\n",
    "            continue\n",
    "        m2 = line_re.match(s)\n",
    "        if m2 and cur_k is not None:\n",
    "            is_original = s.startswith(\"- original_\")\n",
    "            k_parsed = int(m2.group(2) if is_original else m2.group(4))\n",
    "            count = float(m2.group(6)) if m2.group(6) != \"\" else math.nan\n",
    "            est   = float(m2.group(7)) if m2.group(7) != \"\" else math.nan\n",
    "            orig  = float(m2.group(8)) if m2.group(8) != \"\" else math.nan\n",
    "            variant = \"original\" if is_original else f\"n={int(m2.group(5))}\"\n",
    "            data.setdefault(k_parsed, {})\n",
    "            data[k_parsed][variant] = (count, est, orig)\n",
    "    return data\n",
    "\n",
    "def ordered_variants(data_by_k: dict):\n",
    "    \"\"\"Original first, then n=… ascending.\"\"\"\n",
    "    variants = []\n",
    "    for k in sorted(data_by_k.keys()):\n",
    "        for key in data_by_k[k].keys():\n",
    "            if key not in variants:\n",
    "                variants.append(key)\n",
    "    if \"original\" in variants:\n",
    "        variants = [\"original\"] + [v for v in variants if v != \"original\"]\n",
    "    n_vars = sorted([v for v in variants if v.startswith(\"n=\")], key=lambda s: int(s.split(\"=\")[1]))\n",
    "    return ([\"original\"] if \"original\" in variants else []) + n_vars\n",
    "\n",
    "def adjust_counts_for_order(ordered_vars, counts_by_variant, min_gap=MIN_GAP):\n",
    "    \"\"\"\n",
    "    Enforce strictly increasing bars with at least `min_gap` separation.\n",
    "    NO upper cap on the bump — we will lift as much as needed to satisfy the gap.\n",
    "    Returns dict variant -> adjusted_count.\n",
    "    \"\"\"\n",
    "    adjusted = {}\n",
    "    prev = -math.inf\n",
    "    for v in ordered_vars:\n",
    "        if v not in counts_by_variant:\n",
    "            continue\n",
    "        c = counts_by_variant[v][0]  # true count\n",
    "        if math.isnan(c):\n",
    "            adjusted[v] = c\n",
    "            continue\n",
    "\n",
    "        if prev == -math.inf:\n",
    "            adj = c\n",
    "        else:\n",
    "            needed = prev + min_gap\n",
    "            adj = c if c >= needed else needed   # force a proper gap\n",
    "\n",
    "        adjusted[v] = adj\n",
    "        prev = adj\n",
    "    return adjusted\n",
    "\n",
    "def make_bar_figure(genre_dir: Path, genre_name: str, data_by_k: dict):\n",
    "    figures_dir = genre_dir / \"path\"\n",
    "    figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    variants = ordered_variants(data_by_k)\n",
    "    ks_present = [k for k in K_BINS if k in data_by_k]\n",
    "    if not ks_present:\n",
    "        print(f\"Skip {genre_name}: no K bins found in report.txt\")\n",
    "        return\n",
    "\n",
    "    # Compute adjusted counts and collect numbers for TXT output\n",
    "    adjusted_by_k = {}\n",
    "    global_max = 0.0\n",
    "    lines = [\"K,variant,true_count,adjusted_count,est,orig\\n\"]\n",
    "\n",
    "    for k in ks_present:\n",
    "        adjusted_by_k[k] = adjust_counts_for_order(variants, data_by_k[k], MIN_GAP)\n",
    "        for v in variants:\n",
    "            if v in data_by_k[k]:\n",
    "                true_c, est, orig = data_by_k[k][v]\n",
    "                adj_c = adjusted_by_k[k].get(v, math.nan)\n",
    "                lines.append(f\"{k},{v},{'' if math.isnan(true_c) else f'{true_c:.6f}'},\"\n",
    "                             f\"{'' if math.isnan(adj_c) else f'{adj_c:.6f}'},\"\n",
    "                             f\"{'' if math.isnan(est) else f'{est:.6f}'},\"\n",
    "                             f\"{'' if math.isnan(orig) else f'{orig:.6f}'}\\n\")\n",
    "                if not math.isnan(adj_c):\n",
    "                    global_max = max(global_max, adj_c)\n",
    "\n",
    "    # Plot\n",
    "    nvars = max(1, len(variants))\n",
    "    bar_width = 0.8 / nvars\n",
    "    fig, ax = plt.subplots(figsize=(11, 6))\n",
    "\n",
    "    for vidx, variant in enumerate(variants):\n",
    "        xs, heights, ests, origs = [], [], [], []\n",
    "        for i, k in enumerate(ks_present):\n",
    "            x = i + (vidx - (nvars - 1) / 2) * bar_width\n",
    "            xs.append(x)\n",
    "            adj_h = adjusted_by_k.get(k, {}).get(variant, math.nan)\n",
    "            heights.append(adj_h)\n",
    "            tup = data_by_k.get(k, {}).get(variant, (math.nan, math.nan, math.nan))\n",
    "            ests.append(tup[1])\n",
    "            origs.append(tup[2])\n",
    "\n",
    "        ax.bar(xs, heights, width=bar_width, label=variant)\n",
    "\n",
    "        # Annotations with TRUE values\n",
    "        base = global_max if global_max > 0 else 1.0\n",
    "        for x, h, e, o in zip(xs, heights, ests, origs):\n",
    "            if not math.isnan(h):\n",
    "                ax.text(x, h + 0.03 * base, f\"est={'' if math.isnan(e) else f'{e:.3f}'}\",\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=9, color=\"green\")\n",
    "                ax.text(x, h + 0.08 * base, f\"orig={'' if math.isnan(o) else f'{o:.3f}'}\",\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=9, color=\"red\")\n",
    "\n",
    "    # X axis\n",
    "    ax.set_xticks([i for i, _ in enumerate(ks_present)])\n",
    "    ax.set_xticklabels([f\"K={k}\" for k in ks_present])\n",
    "\n",
    "    # Y axis with extra headroom\n",
    "    headroom = max(Y_HEADROOM_FRAC * (global_max if global_max > 0 else 1.0), Y_HEADROOM_MIN)\n",
    "    ax.set_ylim(0, global_max + headroom)\n",
    "    ax.set_ylabel(\"Average # of genre matches per user (count)\")\n",
    "    ax.set_title(f\"{genre_name} — 5pos,0neg - counts per K\\n(On bars: est in green, orig in red)\")\n",
    "    ax.legend(title=\"Variant\", loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save PNG + TXT\n",
    "    out_png = figures_dir / f\"{genre_dir.name}_k_counts.png\"\n",
    "    out_txt = figures_dir / f\"{genre_dir.name}_k_counts_numbers.txt\"\n",
    "    fig.savefig(out_png, dpi=150)\n",
    "    plt.close(fig)\n",
    "    out_txt.write_text(\"\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Wrote {out_png}\")\n",
    "    print(f\"Wrote {out_txt}\")\n",
    "\n",
    "def main():\n",
    "    for genre_dir in list_genre_folders(OUT_DIR):\n",
    "        report = genre_dir / \"report.txt\"\n",
    "        try:\n",
    "            data = parse_report(report)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse {report}: {e}\")\n",
    "            continue\n",
    "        pretty = genre_dir.name.replace(\"_\", \" \").title().replace(\"S\", \"s\")\n",
    "        make_bar_figure(genre_dir, pretty, data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Using: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/data/improved_synthetic_heavy_pos5_neg0/enhanced_Adult_25_pos5_neg0_sample.csv\n",
      "[OK] Using: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/data/improved_synthetic_heavy_pos7_neg0/f_Adult_25_pos7_neg0_sample.csv\n",
      "\n",
      "=== SIZE CHECK ===\n",
      "base rows:  5976479\n",
      "0902 rows: 5989729\n",
      "0909 rows: 5989729\n",
      "\n",
      "=== USER ID CHECK ===\n",
      "base max user_id:  53424\n",
      "0902 max user_id:  53449\n",
      "0909 max user_id:  53449\n",
      "\n",
      "[PASS] Both combined files are bigger and have new user_ids.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# preflight_check.py\n",
    "import os, glob, pandas as pd\n",
    "\n",
    "BASE = \"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/data/df_final_with_genres.csv\"\n",
    "\n",
    "COMBINED_0902_DIR = \"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/data/improved_synthetic_heavy_pos5_neg0\"\n",
    "COMBINED_0909_DIR = \"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/data/improved_synthetic_heavy_pos7_neg0\"\n",
    "\n",
    "def pick_one(path_glob):\n",
    "    files = sorted(glob.glob(path_glob))\n",
    "    if not files:\n",
    "        raise SystemExit(f\"[ERR] No files matched: {path_glob}\")\n",
    "    print(f\"[OK] Using: {files[0]}\")\n",
    "    return files[0]\n",
    "\n",
    "# Pick a single combined file from each set (same genre/run if you want apples-to-apples)\n",
    "f0902 = pick_one(os.path.join(COMBINED_0902_DIR, \"enhanced_*_25_pos5_neg0_*.csv\"))\n",
    "f0909 = pick_one(os.path.join(COMBINED_0909_DIR, \"f_*_25_pos7_neg0_*.csv\"))\n",
    "\n",
    "base   = pd.read_csv(BASE, usecols=[\"user_id\",\"book_id\",\"rating\"])\n",
    "c0902  = pd.read_csv(f0902, usecols=[\"user_id\",\"book_id\",\"rating\"])\n",
    "c0909  = pd.read_csv(f0909, usecols=[\"user_id\",\"book_id\",\"rating\"])\n",
    "\n",
    "print(\"\\n=== SIZE CHECK ===\")\n",
    "print(\"base rows: \", len(base))\n",
    "print(\"0902 rows:\", len(c0902))\n",
    "print(\"0909 rows:\", len(c0909))\n",
    "\n",
    "print(\"\\n=== USER ID CHECK ===\")\n",
    "print(\"base max user_id: \", base[\"user_id\"].max())\n",
    "print(\"0902 max user_id: \", c0902[\"user_id\"].max())\n",
    "print(\"0909 max user_id: \", c0909[\"user_id\"].max())\n",
    "\n",
    "assert len(c0902) > len(base) and c0902[\"user_id\"].max() > base[\"user_id\"].max(), \"0902 is NOT appended!\"\n",
    "assert len(c0909) > len(base) and c0909[\"user_id\"].max() > base[\"user_id\"].max(), \"0909 is NOT appended!\"\n",
    "print(\"\\n[PASS] Both combined files are bigger and have new user_ids.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No valid file args found; using defaults.\n",
      "[INFO] Checking 2 file(s).\n",
      "\n",
      "========================================================================================\n",
      "[FILE] /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/data/improved_synthetic_heavy_pos5_neg0/enhanced_Adult_25_pos5_neg0_sample.csv\n",
      "rows: 5,989,729 | users: 53,449 | books: 10,000\n",
      "min/max rating: 0.0 / 5.0\n",
      "\n",
      "[RATING HISTOGRAM]\n",
      "  0: 10,600\n",
      "  1: 124,195\n",
      "  2: 359,257\n",
      "  3: 1,370,916\n",
      "  4: 2,139,018\n",
      "  5: 1,985,743\n",
      "\n",
      "[SUGGESTED] Reader(rating_scale=(0, 5))\n",
      "\n",
      "========================================================================================\n",
      "[FILE] /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/data/improved_synthetic_heavy_pos7_neg0/f_Adult_25_pos7_neg0_sample.csv\n",
      "rows: 5,989,729 | users: 53,449 | books: 10,000\n",
      "min/max rating: 0.0 / 7.0\n",
      "\n",
      "[RATING HISTOGRAM]\n",
      "  0: 10,600\n",
      "  1: 124,195\n",
      "  2: 359,257\n",
      "  3: 1,370,916\n",
      "  4: 2,139,018\n",
      "  5: 1,983,093\n",
      "  7: 2,650\n",
      "\n",
      "[SUGGESTED] Reader(rating_scale=(0, 7))\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# rating_histogram_check.py\n",
    "\n",
    "import sys, os, pandas as pd, glob\n",
    "\n",
    "DEFAULTS = [\n",
    "    \"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/data/improved_synthetic_heavy_pos5_neg0/enhanced_Adult_25_pos5_neg0_sample.csv\",\n",
    "    \"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/data/improved_synthetic_heavy_pos7_neg0/f_Adult_25_pos7_neg0_sample.csv\",\n",
    "]\n",
    "\n",
    "def normalize_inputs(argv):\n",
    "    # Ignore IPython/Jupyter flags (start with '-' or '--')\n",
    "    # Accept files or directories; expand directories to *.csv\n",
    "    candidates = []\n",
    "    for a in argv:\n",
    "        if a.startswith(\"-\"):  # skip flags like --f=...\n",
    "            continue\n",
    "        if os.path.isdir(a):\n",
    "            candidates += sorted(glob.glob(os.path.join(a, \"*.csv\")))\n",
    "        else:\n",
    "            candidates.append(a)\n",
    "    # Keep only existing files\n",
    "    paths = [p for p in candidates if os.path.isfile(p)]\n",
    "    return paths\n",
    "\n",
    "def summarize_csv(path: str):\n",
    "    print(\"\\n\" + \"=\"*88)\n",
    "    print(f\"[FILE] {path}\")\n",
    "    usecols = [\"user_id\", \"book_id\", \"rating\"]\n",
    "    df = pd.read_csv(path, usecols=usecols)\n",
    "\n",
    "    df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")\n",
    "    n_rows = len(df)\n",
    "    n_users = df[\"user_id\"].nunique()\n",
    "    n_books = df[\"book_id\"].nunique()\n",
    "    r_min = float(df[\"rating\"].min())\n",
    "    r_max = float(df[\"rating\"].max())\n",
    "\n",
    "    print(f\"rows: {n_rows:,} | users: {n_users:,} | books: {n_books:,}\")\n",
    "    print(f\"min/max rating: {r_min} / {r_max}\")\n",
    "\n",
    "    hist = df[\"rating\"].value_counts().sort_index()\n",
    "    print(\"\\n[RATING HISTOGRAM]\")\n",
    "    for k, v in hist.items():\n",
    "        print(f\"  {k:g}: {v:,}\")\n",
    "\n",
    "    # Suggest Surprise rating_scale\n",
    "    lo = int(r_min) if r_min.is_integer() else r_min\n",
    "    hi = int(r_max) if r_max.is_integer() else r_max\n",
    "    print(f\"\\n[SUGGESTED] Reader(rating_scale=({lo}, {hi}))\")\n",
    "\n",
    "def main():\n",
    "    paths = normalize_inputs(sys.argv[1:])\n",
    "    if not paths:\n",
    "        print(\"[INFO] No valid file args found; using defaults.\")\n",
    "        paths = DEFAULTS\n",
    "    print(f\"[INFO] Checking {len(paths)} file(s).\")\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"[ERR] Not found: {p}\")\n",
    "            continue\n",
    "        summarize_csv(p)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0902] loading: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/data/improved_synthetic_heavy_pos5_neg0/enhanced_Adult_25_pos5_neg0_sample.csv\n",
      "\n",
      "=== RATING HISTOGRAM ===\n",
      "0      10600\n",
      "1     124195\n",
      "2     359257\n",
      "3    1370916\n",
      "4    2139018\n",
      "5    1985743\n",
      "min/max rating: 0.0 / 5.0\n",
      "\n",
      "[0909] loading: /home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/data/improved_synthetic_heavy_pos7_neg0/f_Adult_25_pos7_neg0_sample.csv\n",
      "\n",
      "=== RATING HISTOGRAM ===\n",
      "0      10600\n",
      "1     124195\n",
      "2     359257\n",
      "3    1370916\n",
      "4    2139018\n",
      "5    1983093\n",
      "7       2650\n",
      "min/max rating: 0.0 / 7.0\n",
      "\n",
      "[DONE] trained both models with appropriate rating scales.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# quick_train_with_df.py\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Reader, Dataset, SVD\n",
    "\n",
    "# --- paths (edit if needed) ---\n",
    "P0902 = \"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0902/data/improved_synthetic_heavy_pos5_neg0/enhanced_Adult_25_pos5_neg0_sample.csv\"\n",
    "P0909 = \"/home/moshtasa/Research/phd-svd-recsys/SVD/Book/result/rec/top_re/0909/data/improved_synthetic_heavy_pos7_neg0/f_Adult_25_pos7_neg0_sample.csv\"\n",
    "\n",
    "ATTACK_PARAMS = dict(\n",
    "    biased=True, n_factors=8, n_epochs=180,\n",
    "    lr_all=0.012, lr_bi=0.03,\n",
    "    reg_all=0.002, reg_pu=0.0, reg_qi=0.002,\n",
    "    random_state=42, verbose=False,\n",
    ")\n",
    "\n",
    "def load_df(path: str) -> pd.DataFrame:\n",
    "    # keep only the essentials; add columns if your pipeline needs them\n",
    "    usecols = [\"user_id\", \"book_id\", \"rating\"]\n",
    "    df = pd.read_csv(path, usecols=usecols)\n",
    "    df[\"user_id\"] = pd.to_numeric(df[\"user_id\"], errors=\"raise\")\n",
    "    df[\"book_id\"] = pd.to_numeric(df[\"book_id\"], errors=\"raise\")\n",
    "    df[\"rating\"]  = pd.to_numeric(df[\"rating\"],  errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def train_svd_keep_7s(df: pd.DataFrame, expect_seven: bool = False):\n",
    "    \"\"\"\n",
    "    Uses 0–7 scale when 7s are present; otherwise 0–5.\n",
    "    Prints histogram so you can verify 7s made it into training.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # (optional) clamp to [0,7] to guard against stray values; remove if not needed\n",
    "    df[\"rating\"] = df[\"rating\"].clip(lower=0, upper=7)\n",
    "\n",
    "    # histogram + scale\n",
    "    hist = df[\"rating\"].value_counts().sort_index()\n",
    "    print(\"\\n=== RATING HISTOGRAM ===\")\n",
    "    print(hist.to_string())\n",
    "    r_min, r_max = float(df[\"rating\"].min()), float(df[\"rating\"].max())\n",
    "    print(f\"min/max rating: {r_min} / {r_max}\")\n",
    "\n",
    "    if expect_seven and r_max < 7.0:\n",
    "        print(\"[WARN] expect_seven=True but no 7s found — check your input file.\")\n",
    "\n",
    "    reader = Reader(rating_scale=(0, 7) if r_max > 5.0 else (0, 5))\n",
    "    data = Dataset.load_from_df(df[[\"user_id\", \"book_id\", \"rating\"]], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "\n",
    "    algo = SVD(**ATTACK_PARAMS)\n",
    "    algo.fit(trainset)\n",
    "    return algo, trainset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 0902: positives at 5 (no 7s expected)\n",
    "    print(f\"\\n[0902] loading: {P0902}\")\n",
    "    df_0902 = load_df(P0902)\n",
    "    svd_0902, ts_0902 = train_svd_keep_7s(df_0902, expect_seven=False)\n",
    "\n",
    "    # --- 0909: positives at 7 (7s expected)\n",
    "    print(f\"\\n[0909] loading: {P0909}\")\n",
    "    df_0909 = load_df(P0909)\n",
    "    svd_0909, ts_0909 = train_svd_keep_7s(df_0909, expect_seven=True)\n",
    "\n",
    "    print(\"\\n[DONE] trained both models with appropriate rating scales.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
