{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy==1.23.5 in /Users/saramoshtaghi/Library/Python/3.9/lib/python/site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-surprise==1.1.0 in /Users/saramoshtaghi/Library/Python/3.9/lib/python/site-packages (1.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/saramoshtaghi/Library/Python/3.9/lib/python/site-packages (from scikit-surprise==1.1.0) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.11.2 in /Users/saramoshtaghi/Library/Python/3.9/lib/python/site-packages (from scikit-surprise==1.1.0) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/saramoshtaghi/Library/Python/3.9/lib/python/site-packages (from scikit-surprise==1.1.0) (1.13.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-surprise==1.1.0) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-surprise==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and add the decade column by performing a join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  rating  decade\n",
      "0      196      242       3  1990.0\n",
      "1      186      302       3  1990.0\n",
      "2       22      377       1  1990.0\n",
      "3      244       51       2  1990.0\n",
      "4      166      346       1  1990.0\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your local dataset directory\n",
    "dataset_path = \"/Users/saramoshtaghi/Documents/Research/Recommender Systems/phd-svd-recsys/data/ml-100k\"\n",
    "\n",
    "# Load ratings data (assumes the file is 'u.data' in the ml-100k folder)\n",
    "df_ratings = pd.read_csv(f\"{dataset_path}/u.data\", sep='\\t', header=None, \n",
    "                         names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "# Convert item_id to integer\n",
    "df_ratings['item_id'] = df_ratings['item_id'].astype(int)\n",
    "\n",
    "# Define movie metadata columns based on u.item structure\n",
    "movie_columns = ['item_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL',\n",
    "                 'unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',\n",
    "                 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',\n",
    "                 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "# Load movie metadata from local u.item file\n",
    "df_movies = pd.read_csv(f\"{dataset_path}/u.item\", sep='|', encoding='latin-1',\n",
    "                        names=movie_columns, usecols=['item_id', 'release_date'])\n",
    "\n",
    "# Convert item_id in df_movies to int\n",
    "df_movies['item_id'] = df_movies['item_id'].astype(int)\n",
    "\n",
    "# Merge ratings with movie release dates\n",
    "df_final = pd.merge(df_ratings, df_movies, on='item_id', how='left')\n",
    "\n",
    "# Drop timestamp as it's not needed\n",
    "df_final.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# Convert release_date to datetime, handling missing values\n",
    "df_final['release_date'] = pd.to_datetime(df_final['release_date'], errors='coerce')\n",
    "\n",
    "# Extract the year from release_date\n",
    "df_final['year'] = df_final['release_date'].dt.year\n",
    "\n",
    "# Create a new column 'decade' by rounding down the year to the nearest decade\n",
    "df_final['decade'] = (df_final['year'] // 10) * 10\n",
    "\n",
    "# Drop the 'year' and 'release_date' columns as they're no longer needed\n",
    "df_final.drop(columns=['year', 'release_date'], inplace=True)\n",
    "\n",
    "# Display the first few rows of the final DataFrame\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the df_final dataset as a CSV file in the current directory\n",
    "df_final.to_csv(\"df_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Users Movie Count:\n",
      "user_id           943\n",
      "movies_watched    518\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df_final \n",
    "# Select 10 random unique users from the dataset\n",
    "random_users = np.random.choice(df['user_id'].unique(), 200, replace=False)\n",
    "\n",
    "# Count how many movies each user watched\n",
    "user_movie_counts = df[df['user_id'].isin(random_users)].groupby('user_id')['item_id'].count()\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "df_user_movie_counts = user_movie_counts.reset_index()\n",
    "df_user_movie_counts.columns = ['user_id', 'movies_watched']\n",
    "\n",
    "# Display the result\n",
    "print(\"Random Users Movie Count:\")\n",
    "print(df_user_movie_counts.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of movies watched by any user in the entire dataset: 737\n",
      "Minimum number of movies watched by any user in the entire dataset: 20\n"
     ]
    }
   ],
   "source": [
    "df = df_final  # Assuming df_final is the DataFrame you're working with\n",
    "\n",
    "# Count how many movies each user watched in the entire dataset\n",
    "user_movie_counts_all = df.groupby('user_id')['item_id'].count()\n",
    "\n",
    "# Get the maximum count of movies rated by any user in the entire dataset\n",
    "max_movie_count_all = user_movie_counts_all.max()\n",
    "min_movie_count_all = user_movie_counts_all.min()\n",
    "print(\"Maximum number of movies watched by any user in the entire dataset:\", max_movie_count_all)\n",
    "print(\"Minimum number of movies watched by any user in the entire dataset:\", min_movie_count_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Movies Per Decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Decades Available: [1990. 1960. 1970. 1950. 1980. 1940. 1930. 1920.]\n",
      "Total Unique Movies Available: 1682\n",
      "Starting User ID for New Users: 944\n",
      "\n",
      "Movies Per Decade:\n",
      "   decade  movie_count\n",
      "0  1920.0            2\n",
      "1  1930.0           29\n",
      "2  1940.0           45\n",
      "3  1950.0           54\n",
      "4  1960.0           43\n",
      "5  1970.0           53\n",
      "6  1980.0          107\n",
      "7  1990.0         1348\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure df_final exists (Load your dataset if needed)\n",
    "# df_final = pd.read_csv(\"your_dataset.csv\")  # Uncomment if df_final is not loaded\n",
    "\n",
    "# Extract unique decades\n",
    "unique_decades = df_final['decade'].dropna().unique()\n",
    "\n",
    "# Extract unique item_ids and count total movies\n",
    "existing_item_ids = df_final['item_id'].unique()\n",
    "total_movies = df_final['item_id'].nunique()\n",
    "\n",
    "# Extract the last user ID in df_final and define a starting point for new users\n",
    "max_existing_user_id = df_final['user_id'].max()\n",
    "num_new_users = 40\n",
    "new_user_start_id = max_existing_user_id + 1\n",
    "\n",
    "# Count movies per decade\n",
    "movies_per_decade = df_final.groupby('decade')['item_id'].nunique().reset_index(name='movie_count')\n",
    "\n",
    "print(\"Unique Decades Available:\", unique_decades)\n",
    "print(\"Total Unique Movies Available:\", total_movies)\n",
    "print(\"Starting User ID for New Users:\", new_user_start_id)\n",
    "print(\"\\nMovies Per Decade:\")\n",
    "print(movies_per_decade)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate User Ratings by Decade: For each user, count the number of movies they’ve rated in each of the 8 decades. You’ll end up with a 943 x 8 matrix where each row is a user, and each column is a decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'item_id', 'rating', 'decade'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Code to compute user-decade diversity and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  1920.0  1930.0  1940.0  1950.0  1960.0  1970.0  1980.0  1990.0  \\\n",
      "0        1       0       4       1       3       9      18      44     192   \n",
      "1        2       0       0       0       0       0       2       0      60   \n",
      "2        3       0       0       0       0       0       0       0      54   \n",
      "3        4       0       0       0       0       0       2       1      21   \n",
      "4        5       0       2       3       6       9      20      32     102   \n",
      "\n",
      "   1920_norm  1930_norm  1940_norm  1950_norm  1960_norm  1970_norm  \\\n",
      "0        0.0   0.014760   0.003690   0.011070   0.033210   0.066421   \n",
      "1        0.0   0.000000   0.000000   0.000000   0.000000   0.032258   \n",
      "2        0.0   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "3        0.0   0.000000   0.000000   0.000000   0.000000   0.083333   \n",
      "4        0.0   0.011494   0.017241   0.034483   0.051724   0.114943   \n",
      "\n",
      "   1980_norm  1990_norm  \n",
      "0   0.162362   0.708487  \n",
      "1   0.000000   0.967742  \n",
      "2   0.000000   1.000000  \n",
      "3   0.041667   0.875000  \n",
      "4   0.183908   0.586207  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Count number of ratings per user per decade\n",
    "user_decade_counts = df_final.groupby(['user_id', 'decade']).size().unstack(fill_value=0)\n",
    "\n",
    "# Step 2: Normalize counts so each user's row sums to 1\n",
    "user_decade_normalized = user_decade_counts.div(user_decade_counts.sum(axis=1), axis=0)\n",
    "\n",
    "# Step 3: Rename columns to indicate normalization\n",
    "user_decade_normalized.columns = [f'{int(c)}_norm' for c in user_decade_normalized.columns]\n",
    "\n",
    "# Step 4: Merge raw counts with normalized values\n",
    "user_decade_merged = pd.concat([user_decade_counts, user_decade_normalized], axis=1).reset_index()\n",
    "\n",
    "# Step 5: Preview the result\n",
    "print(user_decade_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id   entropy  cluster\n",
      "0        1  0.965269        1\n",
      "1        2  0.142506        0\n",
      "2        3  0.000000        0\n",
      "3        4  0.456334        0\n",
      "4        5  1.263807        2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your df_final if not loaded\n",
    "# df_final = pd.read_csv('your_file.csv')  # example if needed\n",
    "\n",
    "# Step 1: Get counts of ratings per user per decade\n",
    "user_decade_counts = df_final.groupby(['user_id', 'decade']).size().unstack(fill_value=0)\n",
    "\n",
    "# Step 2: Normalize counts to get distribution (row-wise)\n",
    "user_decade_dist = user_decade_counts.div(user_decade_counts.sum(axis=1), axis=0)\n",
    "\n",
    "# Step 3: Calculate entropy (diversity) for each user\n",
    "user_entropy = user_decade_dist.apply(lambda x: entropy(x), axis=1).to_frame(name='entropy')\n",
    "user_entropy.reset_index(inplace=True)\n",
    "\n",
    "# Step 4: Clustering into 3 groups\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "user_entropy['cluster'] = kmeans.fit_predict(user_entropy[['entropy']])\n",
    "\n",
    "# Step 5: Optional — sort clusters by average entropy for interpretation\n",
    "cluster_order = user_entropy.groupby('cluster')['entropy'].mean().sort_values().index\n",
    "entropy_cluster_map = {old: new for new, old in enumerate(cluster_order)}\n",
    "user_entropy['cluster'] = user_entropy['cluster'].map(entropy_cluster_map)\n",
    "\n",
    "# Show sample result\n",
    "print(user_entropy.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users per cluster:\n",
      " cluster\n",
      "0    353\n",
      "1    287\n",
      "2    303\n",
      "Name: count, dtype: int64\n",
      "\n",
      "10 example user_ids per cluster:\n",
      "Cluster 0: [112, 602, 558, 719, 296, 20, 371]\n",
      "Cluster 1: [630, 150, 30, 433, 334, 545, 707, 766, 835, 96]\n",
      "Cluster 2: [149, 518, 46, 414, 439, 854, 663, 603, 228, 115, 340, 202, 691]\n"
     ]
    }
   ],
   "source": [
    "# 1. Count of users per cluster\n",
    "user_counts = user_entropy['cluster'].value_counts().sort_index()\n",
    "print(\"Users per cluster:\\n\", user_counts)\n",
    "\n",
    "# 2. Get 10 example user_ids per cluster\n",
    "example_users = user_entropy.groupby('cluster')['user_id'].apply(lambda x: x.sample(10, random_state=42)).reset_index(drop=True)\n",
    "print(\"\\n10 example user_ids per cluster:\")\n",
    "for cluster in sorted(user_entropy['cluster'].unique()):\n",
    "    users = example_users[user_entropy['cluster'] == cluster].tolist()\n",
    "    print(f\"Cluster {cluster}: {users}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  num_ratings\n",
      "0        2           62\n",
      "1        3           54\n",
      "2        4           24\n",
      "3       15          104\n",
      "4       17           28\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get user_ids from Cluster 0 (low entropy)\n",
    "cluster_0_users = user_entropy[user_entropy['cluster'] == 0]['user_id']\n",
    "\n",
    "# Step 2: Filter original df_final for only those users\n",
    "cluster_0_ratings = df_final[df_final['user_id'].isin(cluster_0_users)]\n",
    "\n",
    "# Step 3: Count ratings per user\n",
    "ratings_per_user_cluster_0 = cluster_0_ratings.groupby('user_id').size().reset_index(name='num_ratings')\n",
    "\n",
    "# Step 4: Preview the result\n",
    "print(ratings_per_user_cluster_0.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  item_id  rating  decade\n",
      "206         15      405       2  1990.0\n",
      "642         15      749       1  1990.0\n",
      "708         15       25       3  1990.0\n",
      "1208        15      331       3  1990.0\n",
      "1365        15      222       3  1990.0\n",
      "...        ...      ...     ...     ...\n",
      "79919       15      472       3  1990.0\n",
      "80757       15      938       3  1990.0\n",
      "81481       15      322       3  1990.0\n",
      "93458       15      845       2  1990.0\n",
      "94881       15      225       3  1990.0\n",
      "\n",
      "[104 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "user_15_data = df_final[df_final['user_id'] == 15]\n",
    "print(user_15_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/data/cluster_0_users.csv',\n",
       " '/mnt/data/cluster_1_users.csv',\n",
       " '/mnt/data/cluster_2_users.csv')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_final and user_entropy are already available in memory\n",
    "\n",
    "# Step 1: Get user clusters\n",
    "user_clusters = user_entropy[['user_id', 'cluster']]\n",
    "\n",
    "# Step 2: Count number of ratings per user\n",
    "user_rating_counts = df_final.groupby('user_id').size().reset_index(name='num_ratings')\n",
    "\n",
    "# Step 3: Merge counts with cluster labels\n",
    "user_info = pd.merge(user_clusters, user_rating_counts, on='user_id')\n",
    "\n",
    "# Step 4: Split into three DataFrames by cluster\n",
    "cluster_0_df = user_info[user_info['cluster'] == 0][['user_id', 'num_ratings']]\n",
    "cluster_1_df = user_info[user_info['cluster'] == 1][['user_id', 'num_ratings']]\n",
    "cluster_2_df = user_info[user_info['cluster'] == 2][['user_id', 'num_ratings']]\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "cluster_0_df.to_csv(\"../cluster_0_users.csv\", index=False)\n",
    "cluster_1_df.to_csv(\"../cluster_1_users.csv\", index=False)\n",
    "cluster_2_df.to_csv(\"../cluster_2_users.csv\", index=False)\n",
    "\n",
    "\"/mnt/data/cluster_0_users.csv\", \"/mnt/data/cluster_1_users.csv\", \"/mnt/data/cluster_2_users.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating biased datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_users_dataset(num_new_users, users_per_decade, start_user_id, df_final):\n",
    "    # List to store new user data\n",
    "    new_users_data = []\n",
    "    \n",
    "    # Define fixed decades\n",
    "    decades = [1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990]\n",
    "\n",
    "    # ✅ Loop to create new users\n",
    "    for i in range(num_new_users):\n",
    "        user_id = start_user_id + i  # Generate unique user ID\n",
    "        \n",
    "        # Handle case when index exceeds available decades\n",
    "        if i // users_per_decade >= len(decades):\n",
    "            favorite_decade = decades[-1]  # Assign the last available decade if overflow\n",
    "        else:\n",
    "            favorite_decade = decades[i // users_per_decade]  # Assign a favorite decade for the user\n",
    "\n",
    "        # 🎥 Get all movies from the selected favorite decade\n",
    "        movies_from_fav_decade = df_final[df_final['decade'] == favorite_decade]['item_id'].unique()\n",
    "\n",
    "        # 🎯 Assign a rating of 5.0 only for movies in the favorite decade\n",
    "        for item_id in movies_from_fav_decade:\n",
    "            new_users_data.append([user_id, item_id, 5.0, favorite_decade])\n",
    "\n",
    "    # ✅ Convert new users' data into a DataFrame\n",
    "    df_new_users = pd.DataFrame(new_users_data, columns=['user_id', 'item_id', 'rating', 'decade'])\n",
    "\n",
    "    # 🔄 Merge the new users' data with the original dataset\n",
    "    df_merged = pd.concat([df_final, df_new_users], ignore_index=True)\n",
    "\n",
    "    print(f\"✅ New dataset generated for {num_new_users} users with {users_per_decade} users per decade.\")\n",
    "    print(f\"🔹 New dataset size: {df_merged.shape}\")\n",
    "    \n",
    "    return df_merged, df_new_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Initial number of rows in the original dataset: 100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count initial rows in the original dataset\n",
    "initial_rows = df_final.shape[0]\n",
    "print(f\"✅ Initial number of rows in the original dataset: {initial_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New dataset generated for 40 users with 5 users per decade.\n",
      "🔹 New dataset size: (108405, 4)\n",
      "✅ New dataset generated for 80 users with 10 users per decade.\n",
      "🔹 New dataset size: (116810, 4)\n",
      "✅ New dataset generated for 120 users with 15 users per decade.\n",
      "🔹 New dataset size: (125215, 4)\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset for 40 new users (5 users per decade)\n",
    "df_40_40, df_new_40 = create_new_users_dataset(40, 5, 944, df_final)\n",
    "\n",
    "# Generate dataset for 80 new users (10 users per decade)\n",
    "df_40_80, df_new_80 = create_new_users_dataset(80, 10, 944, df_final)\n",
    "\n",
    "# Generate dataset for 120 new users (15 users per decade)\n",
    "df_40_120, df_new_120 = create_new_users_dataset(120, 15, 944, df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 40 new users have 1 and only 1 favorite decade.\n",
      "✅ All 80 new users have 1 and only 1 favorite decade.\n",
      "✅ All 120 new users have 1 and only 1 favorite decade.\n"
     ]
    }
   ],
   "source": [
    "def check_validity(df_new_users, num_users):\n",
    "    # 🔍 Check if any user has been mistakenly assigned more than 1 favorite decade\n",
    "    favorite_decades_per_user = df_new_users.groupby('user_id')['decade'].nunique()\n",
    "    multiple_favorite_decades = favorite_decades_per_user[favorite_decades_per_user > 1]\n",
    "\n",
    "    # ✅ Verify that no user has more than 1 favorite decade\n",
    "    if multiple_favorite_decades.empty:\n",
    "        print(f\"✅ All {num_users} new users have 1 and only 1 favorite decade.\")\n",
    "    else:\n",
    "        print(f\"❌ These users have more than 1 favorite decade: {multiple_favorite_decades.index.tolist()}\")\n",
    "\n",
    "# Check validity for 40 new users\n",
    "check_validity(df_new_40, 40)\n",
    "\n",
    "# Check validity for 80 new users\n",
    "check_validity(df_new_80, 80)\n",
    "\n",
    "# Check validity for 120 new users\n",
    "check_validity(df_new_120, 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ First three columns of each dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Select the first three columns for each dataset\n",
    "columns_to_save = ['user_id', 'item_id', 'rating']\n",
    "\n",
    "# Extract and save the selected columns for each dataset\n",
    "df_40_40[columns_to_save].to_csv(\"df_40.csv\", index=False)\n",
    "df_40_80[columns_to_save].to_csv(\"df_80.csv\", index=False)\n",
    "df_40_120[columns_to_save].to_csv(\"df_120.csv\", index=False)\n",
    "\n",
    "print(\"✅ First three columns of each dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-pptx\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /Users/saramoshtaghi/Library/Python/3.9/lib/python/site-packages (from python-pptx) (11.0.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
      "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting lxml>=3.1.0 (from python-pptx)\n",
      "  Downloading lxml-5.3.1-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.7 kB)\n",
      "Collecting typing-extensions>=4.9.0 (from python-pptx)\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Downloading lxml-5.3.1-cp39-cp39-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
      "Installing collected packages: XlsxWriter, typing-extensions, lxml, python-pptx\n",
      "Successfully installed XlsxWriter-3.2.2 lxml-5.3.1 python-pptx-1.0.2 typing-extensions-4.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-pptx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PowerPoint presentation created successfully: Impact_of_Adding_Users.pptx\n"
     ]
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "\n",
    "\n",
    "# Create a new PowerPoint presentation\n",
    "prs = Presentation()\n",
    "\n",
    "# Define a title and content layout\n",
    "title_slide_layout = prs.slide_layouts[0]\n",
    "content_slide_layout = prs.slide_layouts[1]\n",
    "\n",
    "# 🎯 Slide 1: Introduction\n",
    "slide1 = prs.slides.add_slide(title_slide_layout)\n",
    "title1 = slide1.shapes.title\n",
    "subtitle1 = slide1.placeholders[1]\n",
    "\n",
    "title1.text = \"Impact of Adding New Users on Dataset\"\n",
    "subtitle1.text = \"Analyzing dataset size after adding 40, 80, and 120 new users.\\nEach user rates movies from their favorite decade.\"\n",
    "\n",
    "# 🎯 Slide 2: Scenario 1 – Adding 40 New Users\n",
    "slide2 = prs.slides.add_slide(content_slide_layout)\n",
    "title2, content2 = slide2.shapes.title, slide2.placeholders[1]\n",
    "\n",
    "title2.text = \"Scenario 1: Adding 40 New Users\"\n",
    "content2.text = (\n",
    "    \"✅ Users 944 to 948 rate movies from 1920 (2 movies each).\\n\"\n",
    "    \"✅ Users 949 to 953 rate movies from 1930 (29 movies each).\\n\"\n",
    "    \"✅ Users continue rating movies based on their favorite decade.\\n\"\n",
    "    \"✅ Total New Ratings = 8,405\\n\"\n",
    "    \"🔹 New Dataset Size: 108,405 rows.\"\n",
    ")\n",
    "\n",
    "# 🎯 Slide 3: Scenario 2 – Adding 80 New Users\n",
    "slide3 = prs.slides.add_slide(content_slide_layout)\n",
    "title3, content3 = slide3.shapes.title, slide3.placeholders[1]\n",
    "\n",
    "title3.text = \"Scenario 2: Adding 80 New Users\"\n",
    "content3.text = (\n",
    "    \"✅ 10 users per decade, rating movies from their favorite decade.\\n\"\n",
    "    \"✅ Total New Ratings = 16,810\\n\"\n",
    "    \"🔹 New Dataset Size: 116,810 rows.\"\n",
    ")\n",
    "\n",
    "# 🎯 Slide 4: Scenario 3 – Adding 120 New Users\n",
    "slide4 = prs.slides.add_slide(content_slide_layout)\n",
    "title4, content4 = slide4.shapes.title, slide4.placeholders[1]\n",
    "\n",
    "title4.text = \"Scenario 3: Adding 120 New Users\"\n",
    "content4.text = (\n",
    "    \"✅ 15 users per decade, rating movies from their favorite decade.\\n\"\n",
    "    \"✅ Total New Ratings = 25,215\\n\"\n",
    "    \"🔹 New Dataset Size: 125,215 rows.\"\n",
    ")\n",
    "\n",
    "# 🎯 Slide 5: Final Summary\n",
    "slide5 = prs.slides.add_slide(content_slide_layout)\n",
    "title5, content5 = slide5.shapes.title, slide5.placeholders[1]\n",
    "\n",
    "title5.text = \"Final Summary\"\n",
    "content5.text = (\n",
    "    \"🔹 40 Users: 8,405 new ratings → 108,405 rows\\n\"\n",
    "    \"🔹 80 Users: 16,810 new ratings → 116,810 rows\\n\"\n",
    "    \"🔹 120 Users: 25,215 new ratings → 125,215 rows\\n\"\n",
    "    \"✅ Adding more users increases the dataset size significantly.\"\n",
    ")\n",
    "\n",
    "# Save the PowerPoint presentation\n",
    "prs.save(\"Impact_of_Adding_Users.pptx\")\n",
    "\n",
    "print(\"✅ PowerPoint presentation created successfully: Impact_of_Adding_Users.pptx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
